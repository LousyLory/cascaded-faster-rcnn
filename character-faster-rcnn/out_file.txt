Called with args:
Namespace(cfg_file='./experiments/cfgs/faster_rcnn_end2end.yml', gpu_id=0, imdb_name='map_train', max_iters=10010, pretrained_model='./data/imagenet_models/VGG16.v2.caffemodel', randomize=False, set_cfgs=None, solver='./models/map/VGG16/faster_rcnn_end2end/solver.prototxt')
Using config:
{'DATA_DIR': '/media/ray/maps_project/faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_end2end',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/media/ray/maps_project/faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/media/ray/maps_project/faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 3,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [1000],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 64,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.4,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 3,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [1000],
           'SNAPSHOT_INFIX': '',
           'SNAPSHOT_ITERS': 5000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...
wrote gt roidb to /media/ray/maps_project/faster-rcnn/data/cache/train_gt_roidb.pkl
done
Preparing training data...
done
4134 roidb entries
Output will be saved to `/media/ray/maps_project/faster-rcnn/output/faster_rcnn_end2end/train`
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.
  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1WARNING: Logging before InitGoogleLogging() is written to STDERR
I1106 14:12:56.397341 31365 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/map/VGG16/faster_rcnn_end2end/train.prototxt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 0
snapshot_prefix: "vgg16_faster_rcnn_map"
average_loss: 100
iter_size: 2
I1106 14:12:56.397403 31365 solver.cpp:81] Creating training net from train_net file: models/map/VGG16/faster_rcnn_end2end/train.prototxt
I1106 14:12:56.398190 31365 net.cpp:49] Initializing net from parameters: 
name: "VGG_ILSVRC_16_layers"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "rpn_conv/3x3"
  type: "Convolution"
  bottom: "conv5_3"
  top: "rpn/output"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu/3x3"
  type: "ReLU"
  bottom: "rpn/output"
  top: "rpn/output"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn-data"
  type: "Python"
  bottom: "rpn_cls_score"
  bottom: "gt_boxes"
  bottom: "im_info"
  bottom: "data"
  top: "rpn_labels"
  top: "rpn_bbox_targets"
  top: "rpn_bbox_inside_weights"
  top: "rpn_bbox_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "rpn_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_cls_score_reshape"
  bottom: "rpn_labels"
  top: "rpn_cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "rpn_loss_bbox"
  type: "SmoothL1Loss"
  bottom: "rpn_bbox_pred"
  bottom: "rpn_bbox_targets"
  bottom: "rpn_bbox_inside_weights"
  bottom: "rpn_bbox_outside_weights"
  top: "rpn_loss_bbox"
  loss_weight: 1
  smooth_l1_loss_param {
    sigma: 3
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rpn_rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "roi-data"
  type: "Python"
  bottom: "rpn_rois"
  bottom: "gt_boxes"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "rpn.proposal_target_layer"
    layer: "ProposalTargetLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "pool5"
  roi_pooling_param {
    pooled_h: 7
    pooled_w: 7
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "labels"
  top: "loss_cls"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "loss_bbox"
  loss_weight: 1
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "cls_score"
  bottom: "labels"
  top: "accuracy"
}
I1106 14:12:56.398519 31365 layer_factory.hpp:77] Creating layer input-data
I1106 14:12:56.399371 31365 net.cpp:106] Creating Layer input-data
I1106 14:12:56.399381 31365 net.cpp:411] input-data -> data
I1106 14:12:56.399391 31365 net.cpp:411] input-data -> im_info
I1106 14:12:56.399397 31365 net.cpp:411] input-data -> gt_boxes
I1106 14:12:56.412621 31365 net.cpp:150] Setting up input-data
I1106 14:12:56.412650 31365 net.cpp:157] Top shape: 1 3 1000 1000 (3000000)
I1106 14:12:56.412654 31365 net.cpp:157] Top shape: 1 3 (3)
I1106 14:12:56.412658 31365 net.cpp:157] Top shape: 1 4 (4)
I1106 14:12:56.412660 31365 net.cpp:165] Memory required for data: 12000028
I1106 14:12:56.412664 31365 layer_factory.hpp:77] Creating layer data_input-data_0_split
I1106 14:12:56.412675 31365 net.cpp:106] Creating Layer data_input-data_0_split
I1106 14:12:56.412679 31365 net.cpp:454] data_input-data_0_split <- data
I1106 14:12:56.412685 31365 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_0
I1106 14:12:56.412693 31365 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_1
I1106 14:12:56.412729 31365 net.cpp:150] Setting up data_input-data_0_split
I1106 14:12:56.412737 31365 net.cpp:157] Top shape: 1 3 1000 1000 (3000000)
I1106 14:12:56.412741 31365 net.cpp:157] Top shape: 1 3 1000 1000 (3000000)
I1106 14:12:56.412744 31365 net.cpp:165] Memory required for data: 36000028
I1106 14:12:56.412746 31365 layer_factory.hpp:77] Creating layer im_info_input-data_1_split
I1106 14:12:56.412753 31365 net.cpp:106] Creating Layer im_info_input-data_1_split
I1106 14:12:56.412756 31365 net.cpp:454] im_info_input-data_1_split <- im_info
I1106 14:12:56.412761 31365 net.cpp:411] im_info_input-data_1_split -> im_info_input-data_1_split_0
I1106 14:12:56.412766 31365 net.cpp:411] im_info_input-data_1_split -> im_info_input-data_1_split_1
I1106 14:12:56.412796 31365 net.cpp:150] Setting up im_info_input-data_1_split
I1106 14:12:56.412802 31365 net.cpp:157] Top shape: 1 3 (3)
I1106 14:12:56.412806 31365 net.cpp:157] Top shape: 1 3 (3)
I1106 14:12:56.412807 31365 net.cpp:165] Memory required for data: 36000052
I1106 14:12:56.412811 31365 layer_factory.hpp:77] Creating layer gt_boxes_input-data_2_split
I1106 14:12:56.412814 31365 net.cpp:106] Creating Layer gt_boxes_input-data_2_split
I1106 14:12:56.412817 31365 net.cpp:454] gt_boxes_input-data_2_split <- gt_boxes
I1106 14:12:56.412824 31365 net.cpp:411] gt_boxes_input-data_2_split -> gt_boxes_input-data_2_split_0
I1106 14:12:56.412829 31365 net.cpp:411] gt_boxes_input-data_2_split -> gt_boxes_input-data_2_split_1
I1106 14:12:56.412860 31365 net.cpp:150] Setting up gt_boxes_input-data_2_split
I1106 14:12:56.412866 31365 net.cpp:157] Top shape: 1 4 (4)
I1106 14:12:56.412870 31365 net.cpp:157] Top shape: 1 4 (4)
I1106 14:12:56.412873 31365 net.cpp:165] Memory required for data: 36000084
I1106 14:12:56.412874 31365 layer_factory.hpp:77] Creating layer conv1_1
I1106 14:12:56.412885 31365 net.cpp:106] Creating Layer conv1_1
I1106 14:12:56.412889 31365 net.cpp:454] conv1_1 <- data_input-data_0_split_0
I1106 14:12:56.412894 31365 net.cpp:411] conv1_1 -> conv1_1
I1106 14:12:56.416105 31365 net.cpp:150] Setting up conv1_1
I1106 14:12:56.416131 31365 net.cpp:157] Top shape: 1 64 1000 1000 (64000000)
I1106 14:12:56.416133 31365 net.cpp:165] Memory required for data: 292000084
I1106 14:12:56.416146 31365 layer_factory.hpp:77] Creating layer relu1_1
I1106 14:12:56.416152 31365 net.cpp:106] Creating Layer relu1_1
I1106 14:12:56.416158 31365 net.cpp:454] relu1_1 <- conv1_1
I1106 14:12:56.416162 31365 net.cpp:397] relu1_1 -> conv1_1 (in-place)
I1106 14:12:56.416169 31365 net.cpp:150] Setting up relu1_1
I1106 14:12:56.416173 31365 net.cpp:157] Top shape: 1 64 1000 1000 (64000000)
I1106 14:12:56.416175 31365 net.cpp:165] Memory required for data: 548000084
I1106 14:12:56.416177 31365 layer_factory.hpp:77] Creating layer conv1_2
I1106 14:12:56.416183 31365 net.cpp:106] Creating Layer conv1_2
I1106 14:12:56.416185 31365 net.cpp:454] conv1_2 <- conv1_1
I1106 14:12:56.416193 31365 net.cpp:411] conv1_2 -> conv1_2
I1106 14:12:56.419381 31365 net.cpp:150] Setting up conv1_2
I1106 14:12:56.419407 31365 net.cpp:157] Top shape: 1 64 1000 1000 (64000000)
I1106 14:12:56.419410 31365 net.cpp:165] Memory required for data: 804000084
I1106 14:12:56.419420 31365 layer_factory.hpp:77] Creating layer relu1_2
I1106 14:12:56.419427 31365 net.cpp:106] Creating Layer relu1_2
I1106 14:12:56.419430 31365 net.cpp:454] relu1_2 <- conv1_2
I1106 14:12:56.419435 31365 net.cpp:397] relu1_2 -> conv1_2 (in-place)
I1106 14:12:56.419441 31365 net.cpp:150] Setting up relu1_2
I1106 14:12:56.419445 31365 net.cpp:157] Top shape: 1 64 1000 1000 (64000000)
I1106 14:12:56.419446 31365 net.cpp:165] Memory required for data: 1060000084
I1106 14:12:56.419450 31365 layer_factory.hpp:77] Creating layer pool1
I1106 14:12:56.419461 31365 net.cpp:106] Creating Layer pool1
I1106 14:12:56.419463 31365 net.cpp:454] pool1 <- conv1_2
I1106 14:12:56.419467 31365 net.cpp:411] pool1 -> pool1
I1106 14:12:56.419512 31365 net.cpp:150] Setting up pool1
I1106 14:12:56.419520 31365 net.cpp:157] Top shape: 1 64 500 500 (16000000)
I1106 14:12:56.419523 31365 net.cpp:165] Memory required for data: 1124000084
I1106 14:12:56.419525 31365 layer_factory.hpp:77] Creating layer conv2_1
I1106 14:12:56.419531 31365 net.cpp:106] Creating Layer conv2_1
I1106 14:12:56.419534 31365 net.cpp:454] conv2_1 <- pool1
I1106 14:12:56.419540 31365 net.cpp:411] conv2_1 -> conv2_1
I1106 14:12:56.421133 31365 net.cpp:150] Setting up conv2_1
I1106 14:12:56.421156 31365 net.cpp:157] Top shape: 1 128 500 500 (32000000)
I1106 14:12:56.421159 31365 net.cpp:165] Memory required for data: 1252000084
I1106 14:12:56.421169 31365 layer_factory.hpp:77] Creating layer relu2_1
I1106 14:12:56.421174 31365 net.cpp:106] Creating Layer relu2_1
I1106 14:12:56.421177 31365 net.cpp:454] relu2_1 <- conv2_1
I1106 14:12:56.421183 31365 net.cpp:397] relu2_1 -> conv2_1 (in-place)
I1106 14:12:56.421188 31365 net.cpp:150] Setting up relu2_1
I1106 14:12:56.421205 31365 net.cpp:157] Top shape: 1 128 500 500 (32000000)
I1106 14:12:56.421206 31365 net.cpp:165] Memory required for data: 1380000084
I1106 14:12:56.421210 31365 layer_factory.hpp:77] Creating layer conv2_2
I1106 14:12:56.421216 31365 net.cpp:106] Creating Layer conv2_2
I1106 14:12:56.421217 31365 net.cpp:454] conv2_2 <- conv2_1
I1106 14:12:56.421223 31365 net.cpp:411] conv2_2 -> conv2_2
I1106 14:12:56.422298 31365 net.cpp:150] Setting up conv2_2
I1106 14:12:56.422322 31365 net.cpp:157] Top shape: 1 128 500 500 (32000000)
I1106 14:12:56.422325 31365 net.cpp:165] Memory required for data: 1508000084
I1106 14:12:56.422332 31365 layer_factory.hpp:77] Creating layer relu2_2
I1106 14:12:56.422339 31365 net.cpp:106] Creating Layer relu2_2
I1106 14:12:56.422343 31365 net.cpp:454] relu2_2 <- conv2_2
I1106 14:12:56.422346 31365 net.cpp:397] relu2_2 -> conv2_2 (in-place)
I1106 14:12:56.422351 31365 net.cpp:150] Setting up relu2_2
I1106 14:12:56.422368 31365 net.cpp:157] Top shape: 1 128 500 500 (32000000)
I1106 14:12:56.422369 31365 net.cpp:165] Memory required for data: 1636000084
I1106 14:12:56.422372 31365 layer_factory.hpp:77] Creating layer pool2
I1106 14:12:56.422376 31365 net.cpp:106] Creating Layer pool2
I1106 14:12:56.422411 31365 net.cpp:454] pool2 <- conv2_2
I1106 14:12:56.422430 31365 net.cpp:411] pool2 -> pool2
I1106 14:12:56.422471 31365 net.cpp:150] Setting up pool2
I1106 14:12:56.422478 31365 net.cpp:157] Top shape: 1 128 250 250 (8000000)
I1106 14:12:56.422480 31365 net.cpp:165] Memory required for data: 1668000084
I1106 14:12:56.422483 31365 layer_factory.hpp:77] Creating layer conv3_1
I1106 14:12:56.422492 31365 net.cpp:106] Creating Layer conv3_1
I1106 14:12:56.422494 31365 net.cpp:454] conv3_1 <- pool2
I1106 14:12:56.422500 31365 net.cpp:411] conv3_1 -> conv3_1
I1106 14:12:56.423669 31365 net.cpp:150] Setting up conv3_1
I1106 14:12:56.423694 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.423697 31365 net.cpp:165] Memory required for data: 1732000084
I1106 14:12:56.423720 31365 layer_factory.hpp:77] Creating layer relu3_1
I1106 14:12:56.423727 31365 net.cpp:106] Creating Layer relu3_1
I1106 14:12:56.423729 31365 net.cpp:454] relu3_1 <- conv3_1
I1106 14:12:56.423734 31365 net.cpp:397] relu3_1 -> conv3_1 (in-place)
I1106 14:12:56.423739 31365 net.cpp:150] Setting up relu3_1
I1106 14:12:56.423743 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.423745 31365 net.cpp:165] Memory required for data: 1796000084
I1106 14:12:56.423748 31365 layer_factory.hpp:77] Creating layer conv3_2
I1106 14:12:56.423761 31365 net.cpp:106] Creating Layer conv3_2
I1106 14:12:56.423768 31365 net.cpp:454] conv3_2 <- conv3_1
I1106 14:12:56.423773 31365 net.cpp:411] conv3_2 -> conv3_2
I1106 14:12:56.425509 31365 net.cpp:150] Setting up conv3_2
I1106 14:12:56.425534 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.425537 31365 net.cpp:165] Memory required for data: 1860000084
I1106 14:12:56.425544 31365 layer_factory.hpp:77] Creating layer relu3_2
I1106 14:12:56.425561 31365 net.cpp:106] Creating Layer relu3_2
I1106 14:12:56.425565 31365 net.cpp:454] relu3_2 <- conv3_2
I1106 14:12:56.425570 31365 net.cpp:397] relu3_2 -> conv3_2 (in-place)
I1106 14:12:56.425576 31365 net.cpp:150] Setting up relu3_2
I1106 14:12:56.425580 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.425581 31365 net.cpp:165] Memory required for data: 1924000084
I1106 14:12:56.425583 31365 layer_factory.hpp:77] Creating layer conv3_3
I1106 14:12:56.425591 31365 net.cpp:106] Creating Layer conv3_3
I1106 14:12:56.425595 31365 net.cpp:454] conv3_3 <- conv3_2
I1106 14:12:56.425601 31365 net.cpp:411] conv3_3 -> conv3_3
I1106 14:12:56.427350 31365 net.cpp:150] Setting up conv3_3
I1106 14:12:56.427376 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.427379 31365 net.cpp:165] Memory required for data: 1988000084
I1106 14:12:56.427386 31365 layer_factory.hpp:77] Creating layer relu3_3
I1106 14:12:56.427403 31365 net.cpp:106] Creating Layer relu3_3
I1106 14:12:56.427408 31365 net.cpp:454] relu3_3 <- conv3_3
I1106 14:12:56.427413 31365 net.cpp:397] relu3_3 -> conv3_3 (in-place)
I1106 14:12:56.427418 31365 net.cpp:150] Setting up relu3_3
I1106 14:12:56.427423 31365 net.cpp:157] Top shape: 1 256 250 250 (16000000)
I1106 14:12:56.427424 31365 net.cpp:165] Memory required for data: 2052000084
I1106 14:12:56.427428 31365 layer_factory.hpp:77] Creating layer pool3
I1106 14:12:56.427431 31365 net.cpp:106] Creating Layer pool3
I1106 14:12:56.427434 31365 net.cpp:454] pool3 <- conv3_3
I1106 14:12:56.427439 31365 net.cpp:411] pool3 -> pool3
I1106 14:12:56.427491 31365 net.cpp:150] Setting up pool3
I1106 14:12:56.427500 31365 net.cpp:157] Top shape: 1 256 125 125 (4000000)
I1106 14:12:56.427501 31365 net.cpp:165] Memory required for data: 2068000084
I1106 14:12:56.427505 31365 layer_factory.hpp:77] Creating layer conv4_1
I1106 14:12:56.427511 31365 net.cpp:106] Creating Layer conv4_1
I1106 14:12:56.427515 31365 net.cpp:454] conv4_1 <- pool3
I1106 14:12:56.427520 31365 net.cpp:411] conv4_1 -> conv4_1
I1106 14:12:56.431623 31365 net.cpp:150] Setting up conv4_1
I1106 14:12:56.431648 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.431651 31365 net.cpp:165] Memory required for data: 2100000084
I1106 14:12:56.431658 31365 layer_factory.hpp:77] Creating layer relu4_1
I1106 14:12:56.431674 31365 net.cpp:106] Creating Layer relu4_1
I1106 14:12:56.431677 31365 net.cpp:454] relu4_1 <- conv4_1
I1106 14:12:56.431684 31365 net.cpp:397] relu4_1 -> conv4_1 (in-place)
I1106 14:12:56.431690 31365 net.cpp:150] Setting up relu4_1
I1106 14:12:56.431706 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.431708 31365 net.cpp:165] Memory required for data: 2132000084
I1106 14:12:56.431710 31365 layer_factory.hpp:77] Creating layer conv4_2
I1106 14:12:56.431716 31365 net.cpp:106] Creating Layer conv4_2
I1106 14:12:56.431731 31365 net.cpp:454] conv4_2 <- conv4_1
I1106 14:12:56.431737 31365 net.cpp:411] conv4_2 -> conv4_2
I1106 14:12:56.437165 31365 net.cpp:150] Setting up conv4_2
I1106 14:12:56.437189 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.437192 31365 net.cpp:165] Memory required for data: 2164000084
I1106 14:12:56.437217 31365 layer_factory.hpp:77] Creating layer relu4_2
I1106 14:12:56.437223 31365 net.cpp:106] Creating Layer relu4_2
I1106 14:12:56.437227 31365 net.cpp:454] relu4_2 <- conv4_2
I1106 14:12:56.437232 31365 net.cpp:397] relu4_2 -> conv4_2 (in-place)
I1106 14:12:56.437248 31365 net.cpp:150] Setting up relu4_2
I1106 14:12:56.437252 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.437254 31365 net.cpp:165] Memory required for data: 2196000084
I1106 14:12:56.437256 31365 layer_factory.hpp:77] Creating layer conv4_3
I1106 14:12:56.437275 31365 net.cpp:106] Creating Layer conv4_3
I1106 14:12:56.437278 31365 net.cpp:454] conv4_3 <- conv4_2
I1106 14:12:56.437283 31365 net.cpp:411] conv4_3 -> conv4_3
I1106 14:12:56.442781 31365 net.cpp:150] Setting up conv4_3
I1106 14:12:56.442806 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.442809 31365 net.cpp:165] Memory required for data: 2228000084
I1106 14:12:56.442816 31365 layer_factory.hpp:77] Creating layer relu4_3
I1106 14:12:56.442834 31365 net.cpp:106] Creating Layer relu4_3
I1106 14:12:56.442839 31365 net.cpp:454] relu4_3 <- conv4_3
I1106 14:12:56.442843 31365 net.cpp:397] relu4_3 -> conv4_3 (in-place)
I1106 14:12:56.442849 31365 net.cpp:150] Setting up relu4_3
I1106 14:12:56.442852 31365 net.cpp:157] Top shape: 1 512 125 125 (8000000)
I1106 14:12:56.442867 31365 net.cpp:165] Memory required for data: 2260000084
I1106 14:12:56.442869 31365 layer_factory.hpp:77] Creating layer pool4
I1106 14:12:56.442878 31365 net.cpp:106] Creating Layer pool4
I1106 14:12:56.442881 31365 net.cpp:454] pool4 <- conv4_3
I1106 14:12:56.442885 31365 net.cpp:411] pool4 -> pool4
I1106 14:12:56.442939 31365 net.cpp:150] Setting up pool4
I1106 14:12:56.442946 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.442950 31365 net.cpp:165] Memory required for data: 2268128596
I1106 14:12:56.442951 31365 layer_factory.hpp:77] Creating layer conv5_1
I1106 14:12:56.442960 31365 net.cpp:106] Creating Layer conv5_1
I1106 14:12:56.442963 31365 net.cpp:454] conv5_1 <- pool4
I1106 14:12:56.442968 31365 net.cpp:411] conv5_1 -> conv5_1
I1106 14:12:56.448397 31365 net.cpp:150] Setting up conv5_1
I1106 14:12:56.448422 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.448424 31365 net.cpp:165] Memory required for data: 2276257108
I1106 14:12:56.448431 31365 layer_factory.hpp:77] Creating layer relu5_1
I1106 14:12:56.448451 31365 net.cpp:106] Creating Layer relu5_1
I1106 14:12:56.448453 31365 net.cpp:454] relu5_1 <- conv5_1
I1106 14:12:56.448457 31365 net.cpp:397] relu5_1 -> conv5_1 (in-place)
I1106 14:12:56.448463 31365 net.cpp:150] Setting up relu5_1
I1106 14:12:56.448467 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.448482 31365 net.cpp:165] Memory required for data: 2284385620
I1106 14:12:56.448483 31365 layer_factory.hpp:77] Creating layer conv5_2
I1106 14:12:56.448492 31365 net.cpp:106] Creating Layer conv5_2
I1106 14:12:56.448506 31365 net.cpp:454] conv5_2 <- conv5_1
I1106 14:12:56.448513 31365 net.cpp:411] conv5_2 -> conv5_2
I1106 14:12:56.454000 31365 net.cpp:150] Setting up conv5_2
I1106 14:12:56.454015 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.454028 31365 net.cpp:165] Memory required for data: 2292514132
I1106 14:12:56.454035 31365 layer_factory.hpp:77] Creating layer relu5_2
I1106 14:12:56.454041 31365 net.cpp:106] Creating Layer relu5_2
I1106 14:12:56.454056 31365 net.cpp:454] relu5_2 <- conv5_2
I1106 14:12:56.454062 31365 net.cpp:397] relu5_2 -> conv5_2 (in-place)
I1106 14:12:56.454068 31365 net.cpp:150] Setting up relu5_2
I1106 14:12:56.454071 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.454087 31365 net.cpp:165] Memory required for data: 2300642644
I1106 14:12:56.454088 31365 layer_factory.hpp:77] Creating layer conv5_3
I1106 14:12:56.454115 31365 net.cpp:106] Creating Layer conv5_3
I1106 14:12:56.454118 31365 net.cpp:454] conv5_3 <- conv5_2
I1106 14:12:56.454123 31365 net.cpp:411] conv5_3 -> conv5_3
I1106 14:12:56.459601 31365 net.cpp:150] Setting up conv5_3
I1106 14:12:56.459627 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.459630 31365 net.cpp:165] Memory required for data: 2308771156
I1106 14:12:56.459636 31365 layer_factory.hpp:77] Creating layer relu5_3
I1106 14:12:56.459653 31365 net.cpp:106] Creating Layer relu5_3
I1106 14:12:56.459656 31365 net.cpp:454] relu5_3 <- conv5_3
I1106 14:12:56.459663 31365 net.cpp:397] relu5_3 -> conv5_3 (in-place)
I1106 14:12:56.459669 31365 net.cpp:150] Setting up relu5_3
I1106 14:12:56.459684 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.459687 31365 net.cpp:165] Memory required for data: 2316899668
I1106 14:12:56.459689 31365 layer_factory.hpp:77] Creating layer conv5_3_relu5_3_0_split
I1106 14:12:56.459695 31365 net.cpp:106] Creating Layer conv5_3_relu5_3_0_split
I1106 14:12:56.459697 31365 net.cpp:454] conv5_3_relu5_3_0_split <- conv5_3
I1106 14:12:56.459703 31365 net.cpp:411] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_0
I1106 14:12:56.459709 31365 net.cpp:411] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_1
I1106 14:12:56.459744 31365 net.cpp:150] Setting up conv5_3_relu5_3_0_split
I1106 14:12:56.459753 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.459758 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.459759 31365 net.cpp:165] Memory required for data: 2333156692
I1106 14:12:56.459761 31365 layer_factory.hpp:77] Creating layer rpn_conv/3x3
I1106 14:12:56.459774 31365 net.cpp:106] Creating Layer rpn_conv/3x3
I1106 14:12:56.459776 31365 net.cpp:454] rpn_conv/3x3 <- conv5_3_relu5_3_0_split_0
I1106 14:12:56.459781 31365 net.cpp:411] rpn_conv/3x3 -> rpn/output
I1106 14:12:56.489893 31365 net.cpp:150] Setting up rpn_conv/3x3
I1106 14:12:56.489907 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.489910 31365 net.cpp:165] Memory required for data: 2341285204
I1106 14:12:56.489917 31365 layer_factory.hpp:77] Creating layer rpn_relu/3x3
I1106 14:12:56.489924 31365 net.cpp:106] Creating Layer rpn_relu/3x3
I1106 14:12:56.489928 31365 net.cpp:454] rpn_relu/3x3 <- rpn/output
I1106 14:12:56.489943 31365 net.cpp:397] rpn_relu/3x3 -> rpn/output (in-place)
I1106 14:12:56.489949 31365 net.cpp:150] Setting up rpn_relu/3x3
I1106 14:12:56.489953 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.489954 31365 net.cpp:165] Memory required for data: 2349413716
I1106 14:12:56.489958 31365 layer_factory.hpp:77] Creating layer rpn/output_rpn_relu/3x3_0_split
I1106 14:12:56.489964 31365 net.cpp:106] Creating Layer rpn/output_rpn_relu/3x3_0_split
I1106 14:12:56.489966 31365 net.cpp:454] rpn/output_rpn_relu/3x3_0_split <- rpn/output
I1106 14:12:56.489970 31365 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_0
I1106 14:12:56.489976 31365 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_1
I1106 14:12:56.490013 31365 net.cpp:150] Setting up rpn/output_rpn_relu/3x3_0_split
I1106 14:12:56.490020 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.490025 31365 net.cpp:157] Top shape: 1 512 63 63 (2032128)
I1106 14:12:56.490026 31365 net.cpp:165] Memory required for data: 2365670740
I1106 14:12:56.490028 31365 layer_factory.hpp:77] Creating layer rpn_cls_score
I1106 14:12:56.490038 31365 net.cpp:106] Creating Layer rpn_cls_score
I1106 14:12:56.490041 31365 net.cpp:454] rpn_cls_score <- rpn/output_rpn_relu/3x3_0_split_0
I1106 14:12:56.490049 31365 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1106 14:12:56.490422 31365 net.cpp:150] Setting up rpn_cls_score
I1106 14:12:56.490432 31365 net.cpp:157] Top shape: 1 18 63 63 (71442)
I1106 14:12:56.490433 31365 net.cpp:165] Memory required for data: 2365956508
I1106 14:12:56.490439 31365 layer_factory.hpp:77] Creating layer rpn_cls_score_rpn_cls_score_0_split
I1106 14:12:56.490444 31365 net.cpp:106] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I1106 14:12:56.490447 31365 net.cpp:454] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I1106 14:12:56.490468 31365 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I1106 14:12:56.490480 31365 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I1106 14:12:56.490515 31365 net.cpp:150] Setting up rpn_cls_score_rpn_cls_score_0_split
I1106 14:12:56.490523 31365 net.cpp:157] Top shape: 1 18 63 63 (71442)
I1106 14:12:56.490526 31365 net.cpp:157] Top shape: 1 18 63 63 (71442)
I1106 14:12:56.490528 31365 net.cpp:165] Memory required for data: 2366528044
I1106 14:12:56.490531 31365 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1106 14:12:56.490542 31365 net.cpp:106] Creating Layer rpn_bbox_pred
I1106 14:12:56.490547 31365 net.cpp:454] rpn_bbox_pred <- rpn/output_rpn_relu/3x3_0_split_1
I1106 14:12:56.490553 31365 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1106 14:12:56.491016 31365 net.cpp:150] Setting up rpn_bbox_pred
I1106 14:12:56.491027 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.491030 31365 net.cpp:165] Memory required for data: 2367099580
I1106 14:12:56.491035 31365 layer_factory.hpp:77] Creating layer rpn_bbox_pred_rpn_bbox_pred_0_split
I1106 14:12:56.491040 31365 net.cpp:106] Creating Layer rpn_bbox_pred_rpn_bbox_pred_0_split
I1106 14:12:56.491044 31365 net.cpp:454] rpn_bbox_pred_rpn_bbox_pred_0_split <- rpn_bbox_pred
I1106 14:12:56.491048 31365 net.cpp:411] rpn_bbox_pred_rpn_bbox_pred_0_split -> rpn_bbox_pred_rpn_bbox_pred_0_split_0
I1106 14:12:56.491055 31365 net.cpp:411] rpn_bbox_pred_rpn_bbox_pred_0_split -> rpn_bbox_pred_rpn_bbox_pred_0_split_1
I1106 14:12:56.491129 31365 net.cpp:150] Setting up rpn_bbox_pred_rpn_bbox_pred_0_split
I1106 14:12:56.491134 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.491137 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.491139 31365 net.cpp:165] Memory required for data: 2368242652
I1106 14:12:56.491142 31365 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I1106 14:12:56.491150 31365 net.cpp:106] Creating Layer rpn_cls_score_reshape
I1106 14:12:56.491153 31365 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I1106 14:12:56.491160 31365 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I1106 14:12:56.491185 31365 net.cpp:150] Setting up rpn_cls_score_reshape
I1106 14:12:56.491192 31365 net.cpp:157] Top shape: 1 2 567 63 (71442)
I1106 14:12:56.491195 31365 net.cpp:165] Memory required for data: 2368528420
I1106 14:12:56.491197 31365 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape_rpn_cls_score_reshape_0_split
I1106 14:12:56.491202 31365 net.cpp:106] Creating Layer rpn_cls_score_reshape_rpn_cls_score_reshape_0_split
I1106 14:12:56.491205 31365 net.cpp:454] rpn_cls_score_reshape_rpn_cls_score_reshape_0_split <- rpn_cls_score_reshape
I1106 14:12:56.491211 31365 net.cpp:411] rpn_cls_score_reshape_rpn_cls_score_reshape_0_split -> rpn_cls_score_reshape_rpn_cls_score_reshape_0_split_0
I1106 14:12:56.491219 31365 net.cpp:411] rpn_cls_score_reshape_rpn_cls_score_reshape_0_split -> rpn_cls_score_reshape_rpn_cls_score_reshape_0_split_1
I1106 14:12:56.491250 31365 net.cpp:150] Setting up rpn_cls_score_reshape_rpn_cls_score_reshape_0_split
I1106 14:12:56.491256 31365 net.cpp:157] Top shape: 1 2 567 63 (71442)
I1106 14:12:56.491259 31365 net.cpp:157] Top shape: 1 2 567 63 (71442)
I1106 14:12:56.491262 31365 net.cpp:165] Memory required for data: 2369099956
I1106 14:12:56.491264 31365 layer_factory.hpp:77] Creating layer rpn-data
I1106 14:12:56.491930 31365 net.cpp:106] Creating Layer rpn-data
I1106 14:12:56.491943 31365 net.cpp:454] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I1106 14:12:56.491950 31365 net.cpp:454] rpn-data <- gt_boxes_input-data_2_split_0
I1106 14:12:56.491953 31365 net.cpp:454] rpn-data <- im_info_input-data_1_split_0
I1106 14:12:56.491956 31365 net.cpp:454] rpn-data <- data_input-data_0_split_1
I1106 14:12:56.491962 31365 net.cpp:411] rpn-data -> rpn_labels
I1106 14:12:56.491982 31365 net.cpp:411] rpn-data -> rpn_bbox_targets
I1106 14:12:56.491991 31365 net.cpp:411] rpn-data -> rpn_bbox_inside_weights
I1106 14:12:56.491997 31365 net.cpp:411] rpn-data -> rpn_bbox_outside_weights
I1106 14:12:56.493487 31365 net.cpp:150] Setting up rpn-data
I1106 14:12:56.493515 31365 net.cpp:157] Top shape: 1 1 567 63 (35721)
I1106 14:12:56.493520 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.493522 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.493525 31365 net.cpp:157] Top shape: 1 36 63 63 (142884)
I1106 14:12:56.493540 31365 net.cpp:165] Memory required for data: 2370957448
I1106 14:12:56.493543 31365 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1106 14:12:56.493551 31365 net.cpp:106] Creating Layer rpn_loss_cls
I1106 14:12:56.493554 31365 net.cpp:454] rpn_loss_cls <- rpn_cls_score_reshape_rpn_cls_score_reshape_0_split_0
I1106 14:12:56.493559 31365 net.cpp:454] rpn_loss_cls <- rpn_labels
I1106 14:12:56.493564 31365 net.cpp:411] rpn_loss_cls -> rpn_cls_loss
I1106 14:12:56.493574 31365 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1106 14:12:56.493718 31365 net.cpp:150] Setting up rpn_loss_cls
I1106 14:12:56.493726 31365 net.cpp:157] Top shape: (1)
I1106 14:12:56.493728 31365 net.cpp:160]     with loss weight 1
I1106 14:12:56.493739 31365 net.cpp:165] Memory required for data: 2370957452
I1106 14:12:56.493742 31365 layer_factory.hpp:77] Creating layer rpn_loss_bbox
I1106 14:12:56.493749 31365 net.cpp:106] Creating Layer rpn_loss_bbox
I1106 14:12:56.493752 31365 net.cpp:454] rpn_loss_bbox <- rpn_bbox_pred_rpn_bbox_pred_0_split_0
I1106 14:12:56.493757 31365 net.cpp:454] rpn_loss_bbox <- rpn_bbox_targets
I1106 14:12:56.493760 31365 net.cpp:454] rpn_loss_bbox <- rpn_bbox_inside_weights
I1106 14:12:56.493764 31365 net.cpp:454] rpn_loss_bbox <- rpn_bbox_outside_weights
I1106 14:12:56.493768 31365 net.cpp:411] rpn_loss_bbox -> rpn_loss_bbox
I1106 14:12:56.495437 31365 net.cpp:150] Setting up rpn_loss_bbox
I1106 14:12:56.495450 31365 net.cpp:157] Top shape: (1)
I1106 14:12:56.495453 31365 net.cpp:160]     with loss weight 1
I1106 14:12:56.495458 31365 net.cpp:165] Memory required for data: 2370957456
I1106 14:12:56.495462 31365 layer_factory.hpp:77] Creating layer rpn_cls_prob
I1106 14:12:56.495471 31365 net.cpp:106] Creating Layer rpn_cls_prob
I1106 14:12:56.495486 31365 net.cpp:454] rpn_cls_prob <- rpn_cls_score_reshape_rpn_cls_score_reshape_0_split_1
I1106 14:12:56.495491 31365 net.cpp:411] rpn_cls_prob -> rpn_cls_prob
I1106 14:12:56.495555 31365 net.cpp:150] Setting up rpn_cls_prob
I1106 14:12:56.495564 31365 net.cpp:157] Top shape: 1 2 567 63 (71442)
I1106 14:12:56.495566 31365 net.cpp:165] Memory required for data: 2371243224
I1106 14:12:56.495569 31365 layer_factory.hpp:77] Creating layer rpn_cls_prob_reshape
I1106 14:12:56.495575 31365 net.cpp:106] Creating Layer rpn_cls_prob_reshape
I1106 14:12:56.495579 31365 net.cpp:454] rpn_cls_prob_reshape <- rpn_cls_prob
I1106 14:12:56.495584 31365 net.cpp:411] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I1106 14:12:56.495609 31365 net.cpp:150] Setting up rpn_cls_prob_reshape
I1106 14:12:56.495615 31365 net.cpp:157] Top shape: 1 18 63 63 (71442)
I1106 14:12:56.495617 31365 net.cpp:165] Memory required for data: 2371528992
I1106 14:12:56.495620 31365 layer_factory.hpp:77] Creating layer proposal
I1106 14:12:56.497107 31365 net.cpp:106] Creating Layer proposal
I1106 14:12:56.497119 31365 net.cpp:454] proposal <- rpn_cls_prob_reshape
I1106 14:12:56.497125 31365 net.cpp:454] proposal <- rpn_bbox_pred_rpn_bbox_pred_0_split_1
I1106 14:12:56.497129 31365 net.cpp:454] proposal <- im_info_input-data_1_split_1
I1106 14:12:56.497134 31365 net.cpp:411] proposal -> rpn_rois
I1106 14:12:56.497771 31365 net.cpp:150] Setting up proposal
I1106 14:12:56.497797 31365 net.cpp:157] Top shape: 1 5 (5)
I1106 14:12:56.497799 31365 net.cpp:165] Memory required for data: 2371529012
I1106 14:12:56.497802 31365 layer_factory.hpp:77] Creating layer roi-data
I1106 14:12:56.497993 31365 net.cpp:106] Creating Layer roi-data
I1106 14:12:56.498005 31365 net.cpp:454] roi-data <- rpn_rois
I1106 14:12:56.498011 31365 net.cpp:454] roi-data <- gt_boxes_input-data_2_split_1
I1106 14:12:56.498018 31365 net.cpp:411] roi-data -> rois
I1106 14:12:56.498025 31365 net.cpp:411] roi-data -> labels
I1106 14:12:56.498035 31365 net.cpp:411] roi-data -> bbox_targets
I1106 14:12:56.498041 31365 net.cpp:411] roi-data -> bbox_inside_weights
I1106 14:12:56.498047 31365 net.cpp:411] roi-data -> bbox_outside_weights
I1106 14:12:56.498518 31365 net.cpp:150] Setting up roi-data
I1106 14:12:56.498543 31365 net.cpp:157] Top shape: 1 5 (5)
I1106 14:12:56.498548 31365 net.cpp:157] Top shape: 1 1 (1)
I1106 14:12:56.498549 31365 net.cpp:157] Top shape: 1 8 (8)
I1106 14:12:56.498553 31365 net.cpp:157] Top shape: 1 8 (8)
I1106 14:12:56.498555 31365 net.cpp:157] Top shape: 1 8 (8)
I1106 14:12:56.498569 31365 net.cpp:165] Memory required for data: 2371529132
I1106 14:12:56.498572 31365 layer_factory.hpp:77] Creating layer labels_roi-data_1_split
I1106 14:12:56.498579 31365 net.cpp:106] Creating Layer labels_roi-data_1_split
I1106 14:12:56.498581 31365 net.cpp:454] labels_roi-data_1_split <- labels
I1106 14:12:56.498587 31365 net.cpp:411] labels_roi-data_1_split -> labels_roi-data_1_split_0
I1106 14:12:56.498606 31365 net.cpp:411] labels_roi-data_1_split -> labels_roi-data_1_split_1
I1106 14:12:56.498639 31365 net.cpp:150] Setting up labels_roi-data_1_split
I1106 14:12:56.498646 31365 net.cpp:157] Top shape: 1 1 (1)
I1106 14:12:56.498649 31365 net.cpp:157] Top shape: 1 1 (1)
I1106 14:12:56.498651 31365 net.cpp:165] Memory required for data: 2371529140
I1106 14:12:56.498654 31365 layer_factory.hpp:77] Creating layer roi_pool5
I1106 14:12:56.498661 31365 net.cpp:106] Creating Layer roi_pool5
I1106 14:12:56.498663 31365 net.cpp:454] roi_pool5 <- conv5_3_relu5_3_0_split_1
I1106 14:12:56.498667 31365 net.cpp:454] roi_pool5 <- rois
I1106 14:12:56.498672 31365 net.cpp:411] roi_pool5 -> pool5
I1106 14:12:56.498678 31365 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I1106 14:12:56.498714 31365 net.cpp:150] Setting up roi_pool5
I1106 14:12:56.498721 31365 net.cpp:157] Top shape: 1 512 7 7 (25088)
I1106 14:12:56.498723 31365 net.cpp:165] Memory required for data: 2371629492
I1106 14:12:56.498726 31365 layer_factory.hpp:77] Creating layer fc6
I1106 14:12:56.498733 31365 net.cpp:106] Creating Layer fc6
I1106 14:12:56.498734 31365 net.cpp:454] fc6 <- pool5
I1106 14:12:56.498745 31365 net.cpp:411] fc6 -> fc6
I1106 14:12:56.770318 31365 net.cpp:150] Setting up fc6
I1106 14:12:56.770367 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.770370 31365 net.cpp:165] Memory required for data: 2371645876
I1106 14:12:56.770428 31365 layer_factory.hpp:77] Creating layer relu6
I1106 14:12:56.770439 31365 net.cpp:106] Creating Layer relu6
I1106 14:12:56.770443 31365 net.cpp:454] relu6 <- fc6
I1106 14:12:56.770450 31365 net.cpp:397] relu6 -> fc6 (in-place)
I1106 14:12:56.770459 31365 net.cpp:150] Setting up relu6
I1106 14:12:56.770463 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.770465 31365 net.cpp:165] Memory required for data: 2371662260
I1106 14:12:56.770479 31365 layer_factory.hpp:77] Creating layer drop6
I1106 14:12:56.770488 31365 net.cpp:106] Creating Layer drop6
I1106 14:12:56.770490 31365 net.cpp:454] drop6 <- fc6
I1106 14:12:56.770494 31365 net.cpp:397] drop6 -> fc6 (in-place)
I1106 14:12:56.770522 31365 net.cpp:150] Setting up drop6
I1106 14:12:56.770526 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.770529 31365 net.cpp:165] Memory required for data: 2371678644
I1106 14:12:56.770531 31365 layer_factory.hpp:77] Creating layer fc7
I1106 14:12:56.770536 31365 net.cpp:106] Creating Layer fc7
I1106 14:12:56.770539 31365 net.cpp:454] fc7 <- fc6
I1106 14:12:56.770558 31365 net.cpp:411] fc7 -> fc7
I1106 14:12:56.815142 31365 net.cpp:150] Setting up fc7
I1106 14:12:56.815193 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.815196 31365 net.cpp:165] Memory required for data: 2371695028
I1106 14:12:56.815207 31365 layer_factory.hpp:77] Creating layer relu7
I1106 14:12:56.815217 31365 net.cpp:106] Creating Layer relu7
I1106 14:12:56.815222 31365 net.cpp:454] relu7 <- fc7
I1106 14:12:56.815229 31365 net.cpp:397] relu7 -> fc7 (in-place)
I1106 14:12:56.815238 31365 net.cpp:150] Setting up relu7
I1106 14:12:56.815254 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.815256 31365 net.cpp:165] Memory required for data: 2371711412
I1106 14:12:56.815258 31365 layer_factory.hpp:77] Creating layer drop7
I1106 14:12:56.815264 31365 net.cpp:106] Creating Layer drop7
I1106 14:12:56.815268 31365 net.cpp:454] drop7 <- fc7
I1106 14:12:56.815270 31365 net.cpp:397] drop7 -> fc7 (in-place)
I1106 14:12:56.815296 31365 net.cpp:150] Setting up drop7
I1106 14:12:56.815304 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.815318 31365 net.cpp:165] Memory required for data: 2371727796
I1106 14:12:56.815320 31365 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I1106 14:12:56.815326 31365 net.cpp:106] Creating Layer fc7_drop7_0_split
I1106 14:12:56.815330 31365 net.cpp:454] fc7_drop7_0_split <- fc7
I1106 14:12:56.815336 31365 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_0
I1106 14:12:56.815342 31365 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_1
I1106 14:12:56.815395 31365 net.cpp:150] Setting up fc7_drop7_0_split
I1106 14:12:56.815402 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.815407 31365 net.cpp:157] Top shape: 1 4096 (4096)
I1106 14:12:56.815408 31365 net.cpp:165] Memory required for data: 2371760564
I1106 14:12:56.815410 31365 layer_factory.hpp:77] Creating layer cls_score
I1106 14:12:56.815418 31365 net.cpp:106] Creating Layer cls_score
I1106 14:12:56.815421 31365 net.cpp:454] cls_score <- fc7_drop7_0_split_0
I1106 14:12:56.815428 31365 net.cpp:411] cls_score -> cls_score
I1106 14:12:56.815649 31365 net.cpp:150] Setting up cls_score
I1106 14:12:56.815656 31365 net.cpp:157] Top shape: 1 2 (2)
I1106 14:12:56.815659 31365 net.cpp:165] Memory required for data: 2371760572
I1106 14:12:56.815663 31365 layer_factory.hpp:77] Creating layer cls_score_cls_score_0_split
I1106 14:12:56.815668 31365 net.cpp:106] Creating Layer cls_score_cls_score_0_split
I1106 14:12:56.815683 31365 net.cpp:454] cls_score_cls_score_0_split <- cls_score
I1106 14:12:56.815690 31365 net.cpp:411] cls_score_cls_score_0_split -> cls_score_cls_score_0_split_0
I1106 14:12:56.815696 31365 net.cpp:411] cls_score_cls_score_0_split -> cls_score_cls_score_0_split_1
I1106 14:12:56.815726 31365 net.cpp:150] Setting up cls_score_cls_score_0_split
I1106 14:12:56.815732 31365 net.cpp:157] Top shape: 1 2 (2)
I1106 14:12:56.815735 31365 net.cpp:157] Top shape: 1 2 (2)
I1106 14:12:56.815737 31365 net.cpp:165] Memory required for data: 2371760588
I1106 14:12:56.815740 31365 layer_factory.hpp:77] Creating layer bbox_pred
I1106 14:12:56.815749 31365 net.cpp:106] Creating Layer bbox_pred
I1106 14:12:56.815752 31365 net.cpp:454] bbox_pred <- fc7_drop7_0_split_1
I1106 14:12:56.815757 31365 net.cpp:411] bbox_pred -> bbox_pred
I1106 14:12:56.817019 31365 net.cpp:150] Setting up bbox_pred
I1106 14:12:56.817044 31365 net.cpp:157] Top shape: 1 8 (8)
I1106 14:12:56.817047 31365 net.cpp:165] Memory required for data: 2371760620
I1106 14:12:56.817054 31365 layer_factory.hpp:77] Creating layer loss_cls
I1106 14:12:56.817062 31365 net.cpp:106] Creating Layer loss_cls
I1106 14:12:56.817066 31365 net.cpp:454] loss_cls <- cls_score_cls_score_0_split_0
I1106 14:12:56.817071 31365 net.cpp:454] loss_cls <- labels_roi-data_1_split_0
I1106 14:12:56.817076 31365 net.cpp:411] loss_cls -> loss_cls
I1106 14:12:56.817085 31365 layer_factory.hpp:77] Creating layer loss_cls
I1106 14:12:56.817176 31365 net.cpp:150] Setting up loss_cls
I1106 14:12:56.817183 31365 net.cpp:157] Top shape: (1)
I1106 14:12:56.817186 31365 net.cpp:160]     with loss weight 1
I1106 14:12:56.817196 31365 net.cpp:165] Memory required for data: 2371760624
I1106 14:12:56.817199 31365 layer_factory.hpp:77] Creating layer loss_bbox
I1106 14:12:56.817224 31365 net.cpp:106] Creating Layer loss_bbox
I1106 14:12:56.817230 31365 net.cpp:454] loss_bbox <- bbox_pred
I1106 14:12:56.817235 31365 net.cpp:454] loss_bbox <- bbox_targets
I1106 14:12:56.817239 31365 net.cpp:454] loss_bbox <- bbox_inside_weights
I1106 14:12:56.817242 31365 net.cpp:454] loss_bbox <- bbox_outside_weights
I1106 14:12:56.817247 31365 net.cpp:411] loss_bbox -> loss_bbox
I1106 14:12:56.817334 31365 net.cpp:150] Setting up loss_bbox
I1106 14:12:56.817342 31365 net.cpp:157] Top shape: (1)
I1106 14:12:56.817343 31365 net.cpp:160]     with loss weight 1
I1106 14:12:56.817348 31365 net.cpp:165] Memory required for data: 2371760628
I1106 14:12:56.817351 31365 layer_factory.hpp:77] Creating layer accuracy
I1106 14:12:56.817358 31365 net.cpp:106] Creating Layer accuracy
I1106 14:12:56.817360 31365 net.cpp:454] accuracy <- cls_score_cls_score_0_split_1
I1106 14:12:56.817364 31365 net.cpp:454] accuracy <- labels_roi-data_1_split_1
I1106 14:12:56.817368 31365 net.cpp:411] accuracy -> accuracy
I1106 14:12:56.817375 31365 net.cpp:150] Setting up accuracy
I1106 14:12:56.817378 31365 net.cpp:157] Top shape: (1)
I1106 14:12:56.817381 31365 net.cpp:165] Memory required for data: 2371760632
I1106 14:12:56.817384 31365 net.cpp:228] accuracy does not need backward computation.
I1106 14:12:56.817387 31365 net.cpp:226] loss_bbox needs backward computation.
I1106 14:12:56.817390 31365 net.cpp:226] loss_cls needs backward computation.
I1106 14:12:56.817394 31365 net.cpp:226] bbox_pred needs backward computation.
I1106 14:12:56.817397 31365 net.cpp:226] cls_score_cls_score_0_split needs backward computation.
I1106 14:12:56.817400 31365 net.cpp:226] cls_score needs backward computation.
I1106 14:12:56.817404 31365 net.cpp:226] fc7_drop7_0_split needs backward computation.
I1106 14:12:56.817409 31365 net.cpp:226] drop7 needs backward computation.
I1106 14:12:56.817411 31365 net.cpp:226] relu7 needs backward computation.
I1106 14:12:56.817414 31365 net.cpp:226] fc7 needs backward computation.
I1106 14:12:56.817416 31365 net.cpp:226] drop6 needs backward computation.
I1106 14:12:56.817418 31365 net.cpp:226] relu6 needs backward computation.
I1106 14:12:56.817421 31365 net.cpp:226] fc6 needs backward computation.
I1106 14:12:56.817425 31365 net.cpp:226] roi_pool5 needs backward computation.
I1106 14:12:56.817427 31365 net.cpp:228] labels_roi-data_1_split does not need backward computation.
I1106 14:12:56.817430 31365 net.cpp:226] roi-data needs backward computation.
I1106 14:12:56.817435 31365 net.cpp:226] proposal needs backward computation.
I1106 14:12:56.817438 31365 net.cpp:226] rpn_cls_prob_reshape needs backward computation.
I1106 14:12:56.817441 31365 net.cpp:226] rpn_cls_prob needs backward computation.
I1106 14:12:56.817445 31365 net.cpp:226] rpn_loss_bbox needs backward computation.
I1106 14:12:56.817448 31365 net.cpp:226] rpn_loss_cls needs backward computation.
I1106 14:12:56.817453 31365 net.cpp:226] rpn-data needs backward computation.
I1106 14:12:56.817458 31365 net.cpp:226] rpn_cls_score_reshape_rpn_cls_score_reshape_0_split needs backward computation.
I1106 14:12:56.817461 31365 net.cpp:226] rpn_cls_score_reshape needs backward computation.
I1106 14:12:56.817464 31365 net.cpp:226] rpn_bbox_pred_rpn_bbox_pred_0_split needs backward computation.
I1106 14:12:56.817467 31365 net.cpp:226] rpn_bbox_pred needs backward computation.
I1106 14:12:56.817471 31365 net.cpp:226] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I1106 14:12:56.817474 31365 net.cpp:226] rpn_cls_score needs backward computation.
I1106 14:12:56.817477 31365 net.cpp:226] rpn/output_rpn_relu/3x3_0_split needs backward computation.
I1106 14:12:56.817481 31365 net.cpp:226] rpn_relu/3x3 needs backward computation.
I1106 14:12:56.817483 31365 net.cpp:226] rpn_conv/3x3 needs backward computation.
I1106 14:12:56.817489 31365 net.cpp:226] conv5_3_relu5_3_0_split needs backward computation.
I1106 14:12:56.817492 31365 net.cpp:226] relu5_3 needs backward computation.
I1106 14:12:56.817494 31365 net.cpp:226] conv5_3 needs backward computation.
I1106 14:12:56.817497 31365 net.cpp:226] relu5_2 needs backward computation.
I1106 14:12:56.817500 31365 net.cpp:226] conv5_2 needs backward computation.
I1106 14:12:56.817503 31365 net.cpp:226] relu5_1 needs backward computation.
I1106 14:12:56.817505 31365 net.cpp:226] conv5_1 needs backward computation.
I1106 14:12:56.817508 31365 net.cpp:226] pool4 needs backward computation.
I1106 14:12:56.817512 31365 net.cpp:226] relu4_3 needs backward computation.
I1106 14:12:56.817513 31365 net.cpp:226] conv4_3 needs backward computation.
I1106 14:12:56.817517 31365 net.cpp:226] relu4_2 needs backward computation.
I1106 14:12:56.817520 31365 net.cpp:226] conv4_2 needs backward computation.
I1106 14:12:56.817523 31365 net.cpp:226] relu4_1 needs backward computation.
I1106 14:12:56.817526 31365 net.cpp:226] conv4_1 needs backward computation.
I1106 14:12:56.817528 31365 net.cpp:226] pool3 needs backward computation.
I1106 14:12:56.817531 31365 net.cpp:226] relu3_3 needs backward computation.
I1106 14:12:56.817534 31365 net.cpp:226] conv3_3 needs backward computation.
I1106 14:12:56.817536 31365 net.cpp:226] relu3_2 needs backward computation.
I1106 14:12:56.817539 31365 net.cpp:226] conv3_2 needs backward computation.
I1106 14:12:56.817543 31365 net.cpp:226] relu3_1 needs backward computation.
I1106 14:12:56.817544 31365 net.cpp:226] conv3_1 needs backward computation.
I1106 14:12:56.817548 31365 net.cpp:228] pool2 does not need backward computation.
I1106 14:12:56.817551 31365 net.cpp:228] relu2_2 does not need backward computation.
I1106 14:12:56.817553 31365 net.cpp:228] conv2_2 does not need backward computation.
I1106 14:12:56.817556 31365 net.cpp:228] relu2_1 does not need backward computation.
I1106 14:12:56.817559 31365 net.cpp:228] conv2_1 does not need backward computation.
I1106 14:12:56.817562 31365 net.cpp:228] pool1 does not need backward computation.
I1106 14:12:56.817565 31365 net.cpp:228] relu1_2 does not need backward computation.
I1106 14:12:56.817570 31365 net.cpp:228] conv1_2 does not need backward computation.
I1106 14:12:56.817574 31365 net.cpp:228] relu1_1 does not need backward computation.
I1106 14:12:56.817575 31365 net.cpp:228] conv1_1 does not need backward computation.
I1106 14:12:56.817580 31365 net.cpp:228] gt_boxes_input-data_2_split does not need backward computation.
I1106 14:12:56.817584 31365 net.cpp:228] im_info_input-data_1_split does not need backward computation.
I1106 14:12:56.817589 31365 net.cpp:228] data_input-data_0_split does not need backward computation.
I1106 14:12:56.817592 31365 net.cpp:228] input-data does not need backward computation.
I1106 14:12:56.817595 31365 net.cpp:270] This network produces output accuracy
I1106 14:12:56.817597 31365 net.cpp:270] This network produces output loss_bbox
I1106 14:12:56.817600 31365 net.cpp:270] This network produces output loss_cls
I1106 14:12:56.817603 31365 net.cpp:270] This network produces output rpn_cls_loss
I1106 14:12:56.817606 31365 net.cpp:270] This network produces output rpn_loss_bbox
I1106 14:12:56.817658 31365 net.cpp:283] Network initialization done.
I1106 14:12:56.817859 31365 solver.cpp:60] Solver scaffolding done.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432430
I1106 14:12:57.188850 31365 net.cpp:816] Ignoring source layer pool5
I1106 14:12:57.289440 31365 net.cpp:816] Ignoring source layer fc8
I1106 14:12:57.289482 31365 net.cpp:816] Ignoring source layer prob
I1106 14:13:00.735524 31365 solver.cpp:229] Iteration 0, loss = 2.67734
I1106 14:13:00.735610 31365 solver.cpp:245]     Train net output #0: accuracy = 0.40625
I1106 14:13:00.735625 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.465524 (* 1 = 0.465524 loss)
I1106 14:13:00.735630 31365 solver.cpp:245]     Train net output #2: loss_cls = 1.17681 (* 1 = 1.17681 loss)
I1106 14:13:00.735635 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.782553 (* 1 = 0.782553 loss)
I1106 14:13:00.735641 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.319246 (* 1 = 0.319246 loss)
I1106 14:13:00.735661 31365 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1106 14:13:39.097695 31365 solver.cpp:229] Iteration 20, loss = 2.3152
I1106 14:13:39.097769 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 14:13:39.097784 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.902829 (* 1 = 0.902829 loss)
I1106 14:13:39.097790 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.387504 (* 1 = 0.387504 loss)
I1106 14:13:39.097795 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.476817 (* 1 = 0.476817 loss)
I1106 14:13:39.097800 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.201146 (* 1 = 0.201146 loss)
I1106 14:13:39.097806 31365 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I1106 14:14:17.792578 31365 solver.cpp:229] Iteration 40, loss = 1.68863
I1106 14:14:17.792662 31365 solver.cpp:245]     Train net output #0: accuracy = 0.828125
I1106 14:14:17.792678 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.796442 (* 1 = 0.796442 loss)
I1106 14:14:17.792685 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.410599 (* 1 = 0.410599 loss)
I1106 14:14:17.792690 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.221479 (* 1 = 0.221479 loss)
I1106 14:14:17.792695 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.238848 (* 1 = 0.238848 loss)
I1106 14:14:17.792716 31365 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I1106 14:14:57.388861 31365 solver.cpp:229] Iteration 60, loss = 1.7191
I1106 14:14:57.388947 31365 solver.cpp:245]     Train net output #0: accuracy = 0.796875
I1106 14:14:57.388962 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.880891 (* 1 = 0.880891 loss)
I1106 14:14:57.388968 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.388428 (* 1 = 0.388428 loss)
I1106 14:14:57.388972 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.214599 (* 1 = 0.214599 loss)
I1106 14:14:57.388978 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.285006 (* 1 = 0.285006 loss)
I1106 14:14:57.388984 31365 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I1106 14:15:48.047873 31365 solver.cpp:229] Iteration 80, loss = 1.58555
I1106 14:15:48.047951 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:15:48.047979 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.699337 (* 1 = 0.699337 loss)
I1106 14:15:48.047986 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.373701 (* 1 = 0.373701 loss)
I1106 14:15:48.047991 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.155006 (* 1 = 0.155006 loss)
I1106 14:15:48.047996 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0744871 (* 1 = 0.0744871 loss)
I1106 14:15:48.048015 31365 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I1106 14:16:40.183431 31365 solver.cpp:229] Iteration 100, loss = 1.43856
I1106 14:16:40.183516 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:16:40.183531 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.788786 (* 1 = 0.788786 loss)
I1106 14:16:40.183537 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.38055 (* 1 = 0.38055 loss)
I1106 14:16:40.183542 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.277946 (* 1 = 0.277946 loss)
I1106 14:16:40.183547 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.145659 (* 1 = 0.145659 loss)
I1106 14:16:40.183555 31365 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1106 14:17:32.339084 31365 solver.cpp:229] Iteration 120, loss = 1.97405
I1106 14:17:32.339162 31365 solver.cpp:245]     Train net output #0: accuracy = 0.765625
I1106 14:17:32.339177 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.896136 (* 1 = 0.896136 loss)
I1106 14:17:32.339184 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.638618 (* 1 = 0.638618 loss)
I1106 14:17:32.339188 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.208493 (* 1 = 0.208493 loss)
I1106 14:17:32.339193 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.237372 (* 1 = 0.237372 loss)
I1106 14:17:32.339200 31365 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I1106 14:18:24.422772 31365 solver.cpp:229] Iteration 140, loss = 1.28727
I1106 14:18:24.422850 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:18:24.422865 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.785786 (* 1 = 0.785786 loss)
I1106 14:18:24.422870 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.343251 (* 1 = 0.343251 loss)
I1106 14:18:24.422876 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0904957 (* 1 = 0.0904957 loss)
I1106 14:18:24.422881 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0837598 (* 1 = 0.0837598 loss)
I1106 14:18:24.422888 31365 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I1106 14:19:16.421272 31365 solver.cpp:229] Iteration 160, loss = 1.11444
I1106 14:19:16.421344 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:19:16.421357 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.753914 (* 1 = 0.753914 loss)
I1106 14:19:16.421365 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.316763 (* 1 = 0.316763 loss)
I1106 14:19:16.421370 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0924419 (* 1 = 0.0924419 loss)
I1106 14:19:16.421375 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.121202 (* 1 = 0.121202 loss)
I1106 14:19:16.421382 31365 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I1106 14:20:08.432060 31365 solver.cpp:229] Iteration 180, loss = 1.19671
I1106 14:20:08.432116 31365 solver.cpp:245]     Train net output #0: accuracy = 0.765625
I1106 14:20:08.432133 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.708918 (* 1 = 0.708918 loss)
I1106 14:20:08.432142 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.397171 (* 1 = 0.397171 loss)
I1106 14:20:08.432147 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.107453 (* 1 = 0.107453 loss)
I1106 14:20:08.432159 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0913373 (* 1 = 0.0913373 loss)
I1106 14:20:08.432168 31365 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I1106 14:21:00.573107 31365 solver.cpp:229] Iteration 200, loss = 1.26262
I1106 14:21:00.573182 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 14:21:00.573196 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.799535 (* 1 = 0.799535 loss)
I1106 14:21:00.573201 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.158278 (* 1 = 0.158278 loss)
I1106 14:21:00.573207 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0565995 (* 1 = 0.0565995 loss)
I1106 14:21:00.573212 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00896952 (* 1 = 0.00896952 loss)
I1106 14:21:00.573220 31365 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1106 14:21:52.690191 31365 solver.cpp:229] Iteration 220, loss = 0.955009
I1106 14:21:52.690265 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:21:52.690292 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.611865 (* 1 = 0.611865 loss)
I1106 14:21:52.690299 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.15168 (* 1 = 0.15168 loss)
I1106 14:21:52.690304 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0503791 (* 1 = 0.0503791 loss)
I1106 14:21:52.690309 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.13256 (* 1 = 0.13256 loss)
I1106 14:21:52.690315 31365 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I1106 14:22:44.743024 31365 solver.cpp:229] Iteration 240, loss = 0.905456
I1106 14:22:44.743096 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:22:44.743110 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.484482 (* 1 = 0.484482 loss)
I1106 14:22:44.743118 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.211176 (* 1 = 0.211176 loss)
I1106 14:22:44.743122 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.108912 (* 1 = 0.108912 loss)
I1106 14:22:44.743127 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.136393 (* 1 = 0.136393 loss)
I1106 14:22:44.743134 31365 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I1106 14:23:36.775380 31365 solver.cpp:229] Iteration 260, loss = 0.954387
I1106 14:23:36.775456 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 14:23:36.775470 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.534957 (* 1 = 0.534957 loss)
I1106 14:23:36.775476 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.252553 (* 1 = 0.252553 loss)
I1106 14:23:36.775481 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.106956 (* 1 = 0.106956 loss)
I1106 14:23:36.775487 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0418811 (* 1 = 0.0418811 loss)
I1106 14:23:36.775493 31365 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I1106 14:24:28.778586 31365 solver.cpp:229] Iteration 280, loss = 1.19501
I1106 14:24:28.778664 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 14:24:28.778678 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.429301 (* 1 = 0.429301 loss)
I1106 14:24:28.778684 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.34022 (* 1 = 0.34022 loss)
I1106 14:24:28.778689 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0321189 (* 1 = 0.0321189 loss)
I1106 14:24:28.778694 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0670583 (* 1 = 0.0670583 loss)
I1106 14:24:28.778700 31365 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I1106 14:25:20.800681 31365 solver.cpp:229] Iteration 300, loss = 0.871521
I1106 14:25:20.800761 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 14:25:20.800776 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.560291 (* 1 = 0.560291 loss)
I1106 14:25:20.800781 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.151603 (* 1 = 0.151603 loss)
I1106 14:25:20.800786 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.115294 (* 1 = 0.115294 loss)
I1106 14:25:20.800791 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0629701 (* 1 = 0.0629701 loss)
I1106 14:25:20.800798 31365 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1106 14:26:12.819458 31365 solver.cpp:229] Iteration 320, loss = 0.856832
I1106 14:26:12.819536 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 14:26:12.819550 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.468607 (* 1 = 0.468607 loss)
I1106 14:26:12.819555 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.21371 (* 1 = 0.21371 loss)
I1106 14:26:12.819561 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.157283 (* 1 = 0.157283 loss)
I1106 14:26:12.819566 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.069455 (* 1 = 0.069455 loss)
I1106 14:26:12.819572 31365 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I1106 14:27:04.881595 31365 solver.cpp:229] Iteration 340, loss = 1.06549
I1106 14:27:04.881666 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 14:27:04.881680 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.695647 (* 1 = 0.695647 loss)
I1106 14:27:04.881686 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.302122 (* 1 = 0.302122 loss)
I1106 14:27:04.881691 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.133398 (* 1 = 0.133398 loss)
I1106 14:27:04.881696 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.104123 (* 1 = 0.104123 loss)
I1106 14:27:04.881703 31365 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I1106 14:27:56.933596 31365 solver.cpp:229] Iteration 360, loss = 0.867348
I1106 14:27:56.933676 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:27:56.933688 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.451352 (* 1 = 0.451352 loss)
I1106 14:27:56.933694 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.16577 (* 1 = 0.16577 loss)
I1106 14:27:56.933699 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0990592 (* 1 = 0.0990592 loss)
I1106 14:27:56.933704 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.138537 (* 1 = 0.138537 loss)
I1106 14:27:56.933712 31365 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I1106 14:28:48.968062 31365 solver.cpp:229] Iteration 380, loss = 1.04763
I1106 14:28:48.968137 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:28:48.968165 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.40825 (* 1 = 0.40825 loss)
I1106 14:28:48.968171 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.269185 (* 1 = 0.269185 loss)
I1106 14:28:48.968176 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.109654 (* 1 = 0.109654 loss)
I1106 14:28:48.968183 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0400226 (* 1 = 0.0400226 loss)
I1106 14:28:48.968189 31365 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I1106 14:29:41.003850 31365 solver.cpp:229] Iteration 400, loss = 0.702093
I1106 14:29:41.003937 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 14:29:41.003952 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.338146 (* 1 = 0.338146 loss)
I1106 14:29:41.003957 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.092254 (* 1 = 0.092254 loss)
I1106 14:29:41.003962 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0258707 (* 1 = 0.0258707 loss)
I1106 14:29:41.003968 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0217647 (* 1 = 0.0217647 loss)
I1106 14:29:41.003974 31365 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1106 14:30:33.034160 31365 solver.cpp:229] Iteration 420, loss = 0.94673
I1106 14:30:33.034240 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 14:30:33.034255 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.361552 (* 1 = 0.361552 loss)
I1106 14:30:33.034260 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.292014 (* 1 = 0.292014 loss)
I1106 14:30:33.034265 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0470088 (* 1 = 0.0470088 loss)
I1106 14:30:33.034271 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0578877 (* 1 = 0.0578877 loss)
I1106 14:30:33.034277 31365 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I1106 14:31:25.064322 31365 solver.cpp:229] Iteration 440, loss = 0.845391
I1106 14:31:25.064401 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:31:25.064415 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.406034 (* 1 = 0.406034 loss)
I1106 14:31:25.064420 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.199347 (* 1 = 0.199347 loss)
I1106 14:31:25.064426 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0932823 (* 1 = 0.0932823 loss)
I1106 14:31:25.064430 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.194637 (* 1 = 0.194637 loss)
I1106 14:31:25.064437 31365 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I1106 14:32:17.112172 31365 solver.cpp:229] Iteration 460, loss = 0.715288
I1106 14:32:17.112247 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:32:17.112262 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.461132 (* 1 = 0.461132 loss)
I1106 14:32:17.112267 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.178502 (* 1 = 0.178502 loss)
I1106 14:32:17.112272 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0838156 (* 1 = 0.0838156 loss)
I1106 14:32:17.112277 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0783892 (* 1 = 0.0783892 loss)
I1106 14:32:17.112284 31365 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I1106 14:33:09.111620 31365 solver.cpp:229] Iteration 480, loss = 0.587985
I1106 14:33:09.111698 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 14:33:09.111712 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.418647 (* 1 = 0.418647 loss)
I1106 14:33:09.111718 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.110362 (* 1 = 0.110362 loss)
I1106 14:33:09.111723 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0372826 (* 1 = 0.0372826 loss)
I1106 14:33:09.111728 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0180308 (* 1 = 0.0180308 loss)
I1106 14:33:09.111735 31365 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I1106 14:34:01.165127 31365 solver.cpp:229] Iteration 500, loss = 0.676444
I1106 14:34:01.165204 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 14:34:01.165230 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.503176 (* 1 = 0.503176 loss)
I1106 14:34:01.165237 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.211416 (* 1 = 0.211416 loss)
I1106 14:34:01.165242 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0646074 (* 1 = 0.0646074 loss)
I1106 14:34:01.165247 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0678126 (* 1 = 0.0678126 loss)
I1106 14:34:01.165254 31365 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1106 14:34:53.297207 31365 solver.cpp:229] Iteration 520, loss = 0.727246
I1106 14:34:53.297281 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 14:34:53.297296 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.339101 (* 1 = 0.339101 loss)
I1106 14:34:53.297302 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.101461 (* 1 = 0.101461 loss)
I1106 14:34:53.297307 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0343764 (* 1 = 0.0343764 loss)
I1106 14:34:53.297312 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0421396 (* 1 = 0.0421396 loss)
I1106 14:34:53.297319 31365 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I1106 14:35:45.436123 31365 solver.cpp:229] Iteration 540, loss = 0.988632
I1106 14:35:45.436199 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 14:35:45.436213 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.45381 (* 1 = 0.45381 loss)
I1106 14:35:45.436219 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.26098 (* 1 = 0.26098 loss)
I1106 14:35:45.436224 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0385063 (* 1 = 0.0385063 loss)
I1106 14:35:45.436229 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.066453 (* 1 = 0.066453 loss)
I1106 14:35:45.436236 31365 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I1106 14:36:37.590236 31365 solver.cpp:229] Iteration 560, loss = 0.593995
I1106 14:36:37.590315 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:36:37.590329 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.358068 (* 1 = 0.358068 loss)
I1106 14:36:37.590335 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.238516 (* 1 = 0.238516 loss)
I1106 14:36:37.590340 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0461376 (* 1 = 0.0461376 loss)
I1106 14:36:37.590345 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0607978 (* 1 = 0.0607978 loss)
I1106 14:36:37.590353 31365 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I1106 14:37:29.724833 31365 solver.cpp:229] Iteration 580, loss = 0.906871
I1106 14:37:29.724910 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 14:37:29.724925 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.435293 (* 1 = 0.435293 loss)
I1106 14:37:29.724931 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.234848 (* 1 = 0.234848 loss)
I1106 14:37:29.724936 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0428843 (* 1 = 0.0428843 loss)
I1106 14:37:29.724942 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0651989 (* 1 = 0.0651989 loss)
I1106 14:37:29.724948 31365 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I1106 14:38:21.868837 31365 solver.cpp:229] Iteration 600, loss = 0.799003
I1106 14:38:21.868911 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 14:38:21.868924 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.259496 (* 1 = 0.259496 loss)
I1106 14:38:21.868930 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.214232 (* 1 = 0.214232 loss)
I1106 14:38:21.868935 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0503101 (* 1 = 0.0503101 loss)
I1106 14:38:21.868940 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.067982 (* 1 = 0.067982 loss)
I1106 14:38:21.868947 31365 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1106 14:39:14.067718 31365 solver.cpp:229] Iteration 620, loss = 0.884802
I1106 14:39:14.067816 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:39:14.067842 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.426358 (* 1 = 0.426358 loss)
I1106 14:39:14.067849 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.220881 (* 1 = 0.220881 loss)
I1106 14:39:14.067854 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.156712 (* 1 = 0.156712 loss)
I1106 14:39:14.067859 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.139272 (* 1 = 0.139272 loss)
I1106 14:39:14.067867 31365 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I1106 14:40:06.287624 31365 solver.cpp:229] Iteration 640, loss = 0.770204
I1106 14:40:06.287706 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 14:40:06.287721 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.427644 (* 1 = 0.427644 loss)
I1106 14:40:06.287727 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.115219 (* 1 = 0.115219 loss)
I1106 14:40:06.287732 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0332451 (* 1 = 0.0332451 loss)
I1106 14:40:06.287737 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0861346 (* 1 = 0.0861346 loss)
I1106 14:40:06.287744 31365 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I1106 14:40:58.451252 31365 solver.cpp:229] Iteration 660, loss = 0.550402
I1106 14:40:58.451339 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 14:40:58.451354 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.302498 (* 1 = 0.302498 loss)
I1106 14:40:58.451360 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.258398 (* 1 = 0.258398 loss)
I1106 14:40:58.451365 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0399392 (* 1 = 0.0399392 loss)
I1106 14:40:58.451370 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.102506 (* 1 = 0.102506 loss)
I1106 14:40:58.451390 31365 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I1106 14:41:50.634958 31365 solver.cpp:229] Iteration 680, loss = 0.585079
I1106 14:41:50.635040 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 14:41:50.635056 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.280093 (* 1 = 0.280093 loss)
I1106 14:41:50.635061 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0940267 (* 1 = 0.0940267 loss)
I1106 14:41:50.635067 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0237401 (* 1 = 0.0237401 loss)
I1106 14:41:50.635071 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0397192 (* 1 = 0.0397192 loss)
I1106 14:41:50.635079 31365 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I1106 14:42:42.830626 31365 solver.cpp:229] Iteration 700, loss = 0.659213
I1106 14:42:42.830713 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 14:42:42.830729 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.343113 (* 1 = 0.343113 loss)
I1106 14:42:42.830734 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.210374 (* 1 = 0.210374 loss)
I1106 14:42:42.830739 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0696978 (* 1 = 0.0696978 loss)
I1106 14:42:42.830744 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0593929 (* 1 = 0.0593929 loss)
I1106 14:42:42.830751 31365 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1106 14:43:35.000107 31365 solver.cpp:229] Iteration 720, loss = 0.497755
I1106 14:43:35.000191 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:43:35.000207 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.312129 (* 1 = 0.312129 loss)
I1106 14:43:35.000212 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.17311 (* 1 = 0.17311 loss)
I1106 14:43:35.000218 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0212342 (* 1 = 0.0212342 loss)
I1106 14:43:35.000223 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0250417 (* 1 = 0.0250417 loss)
I1106 14:43:35.000243 31365 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I1106 14:44:27.193019 31365 solver.cpp:229] Iteration 740, loss = 0.637211
I1106 14:44:27.193101 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 14:44:27.193116 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.267962 (* 1 = 0.267962 loss)
I1106 14:44:27.193122 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.182281 (* 1 = 0.182281 loss)
I1106 14:44:27.193128 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0414447 (* 1 = 0.0414447 loss)
I1106 14:44:27.193133 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0362097 (* 1 = 0.0362097 loss)
I1106 14:44:27.193140 31365 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I1106 14:45:19.371130 31365 solver.cpp:229] Iteration 760, loss = 0.618679
I1106 14:45:19.371218 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 14:45:19.371235 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.37406 (* 1 = 0.37406 loss)
I1106 14:45:19.371240 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.140486 (* 1 = 0.140486 loss)
I1106 14:45:19.371246 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.038606 (* 1 = 0.038606 loss)
I1106 14:45:19.371251 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.101231 (* 1 = 0.101231 loss)
I1106 14:45:19.371273 31365 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I1106 14:46:11.600518 31365 solver.cpp:229] Iteration 780, loss = 0.85841
I1106 14:46:11.600601 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 14:46:11.600617 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.410567 (* 1 = 0.410567 loss)
I1106 14:46:11.600623 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.357545 (* 1 = 0.357545 loss)
I1106 14:46:11.600628 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0469232 (* 1 = 0.0469232 loss)
I1106 14:46:11.600633 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.055459 (* 1 = 0.055459 loss)
I1106 14:46:11.600641 31365 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I1106 14:47:03.783048 31365 solver.cpp:229] Iteration 800, loss = 0.433281
I1106 14:47:03.783134 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 14:47:03.783149 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.314926 (* 1 = 0.314926 loss)
I1106 14:47:03.783156 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0928116 (* 1 = 0.0928116 loss)
I1106 14:47:03.783160 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0103449 (* 1 = 0.0103449 loss)
I1106 14:47:03.783165 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0249484 (* 1 = 0.0249484 loss)
I1106 14:47:03.783172 31365 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1106 14:47:55.938506 31365 solver.cpp:229] Iteration 820, loss = 0.769026
I1106 14:47:55.938592 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:47:55.938608 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.325286 (* 1 = 0.325286 loss)
I1106 14:47:55.938613 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.239022 (* 1 = 0.239022 loss)
I1106 14:47:55.938619 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0315476 (* 1 = 0.0315476 loss)
I1106 14:47:55.938624 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0193631 (* 1 = 0.0193631 loss)
I1106 14:47:55.938632 31365 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I1106 14:48:48.117509 31365 solver.cpp:229] Iteration 840, loss = 0.866228
I1106 14:48:48.117594 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 14:48:48.117609 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.335025 (* 1 = 0.335025 loss)
I1106 14:48:48.117614 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.210905 (* 1 = 0.210905 loss)
I1106 14:48:48.117620 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0561008 (* 1 = 0.0561008 loss)
I1106 14:48:48.117625 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0307924 (* 1 = 0.0307924 loss)
I1106 14:48:48.117632 31365 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I1106 14:49:40.293448 31365 solver.cpp:229] Iteration 860, loss = 0.68319
I1106 14:49:40.293534 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:49:40.293550 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.230432 (* 1 = 0.230432 loss)
I1106 14:49:40.293555 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.173565 (* 1 = 0.173565 loss)
I1106 14:49:40.293560 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0258684 (* 1 = 0.0258684 loss)
I1106 14:49:40.293566 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0422357 (* 1 = 0.0422357 loss)
I1106 14:49:40.293573 31365 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I1106 14:50:32.491716 31365 solver.cpp:229] Iteration 880, loss = 0.633925
I1106 14:50:32.491801 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 14:50:32.491816 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.315902 (* 1 = 0.315902 loss)
I1106 14:50:32.491822 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.22891 (* 1 = 0.22891 loss)
I1106 14:50:32.491827 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0330683 (* 1 = 0.0330683 loss)
I1106 14:50:32.491832 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0737259 (* 1 = 0.0737259 loss)
I1106 14:50:32.491852 31365 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I1106 14:51:24.669495 31365 solver.cpp:229] Iteration 900, loss = 0.582709
I1106 14:51:24.669582 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:51:24.669598 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.397112 (* 1 = 0.397112 loss)
I1106 14:51:24.669605 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.16488 (* 1 = 0.16488 loss)
I1106 14:51:24.669610 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0612287 (* 1 = 0.0612287 loss)
I1106 14:51:24.669615 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0640341 (* 1 = 0.0640341 loss)
I1106 14:51:24.669622 31365 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1106 14:52:16.884219 31365 solver.cpp:229] Iteration 920, loss = 0.794836
I1106 14:52:16.884294 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 14:52:16.884307 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.49491 (* 1 = 0.49491 loss)
I1106 14:52:16.884313 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.193904 (* 1 = 0.193904 loss)
I1106 14:52:16.884318 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0421441 (* 1 = 0.0421441 loss)
I1106 14:52:16.884323 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0240881 (* 1 = 0.0240881 loss)
I1106 14:52:16.884330 31365 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I1106 14:53:09.047963 31365 solver.cpp:229] Iteration 940, loss = 0.840228
I1106 14:53:09.048039 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:53:09.048053 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.436194 (* 1 = 0.436194 loss)
I1106 14:53:09.048058 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.338415 (* 1 = 0.338415 loss)
I1106 14:53:09.048064 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0760887 (* 1 = 0.0760887 loss)
I1106 14:53:09.048069 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.129867 (* 1 = 0.129867 loss)
I1106 14:53:09.048075 31365 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I1106 14:54:01.141029 31365 solver.cpp:229] Iteration 960, loss = 0.832148
I1106 14:54:01.141104 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 14:54:01.141119 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.506953 (* 1 = 0.506953 loss)
I1106 14:54:01.141126 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.274899 (* 1 = 0.274899 loss)
I1106 14:54:01.141131 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0606596 (* 1 = 0.0606596 loss)
I1106 14:54:01.141137 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0447005 (* 1 = 0.0447005 loss)
I1106 14:54:01.141144 31365 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I1106 14:54:53.276206 31365 solver.cpp:229] Iteration 980, loss = 1.16334
I1106 14:54:53.276285 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 14:54:53.276299 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.364322 (* 1 = 0.364322 loss)
I1106 14:54:53.276305 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.308778 (* 1 = 0.308778 loss)
I1106 14:54:53.276310 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0407996 (* 1 = 0.0407996 loss)
I1106 14:54:53.276316 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.114757 (* 1 = 0.114757 loss)
I1106 14:54:53.276322 31365 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I1106 14:55:45.362869 31365 solver.cpp:229] Iteration 1000, loss = 0.529268
I1106 14:55:45.362949 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 14:55:45.362962 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.229377 (* 1 = 0.229377 loss)
I1106 14:55:45.362968 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.232424 (* 1 = 0.232424 loss)
I1106 14:55:45.362973 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0360376 (* 1 = 0.0360376 loss)
I1106 14:55:45.362978 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0597705 (* 1 = 0.0597705 loss)
I1106 14:55:45.362985 31365 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1106 14:56:37.449534 31365 solver.cpp:229] Iteration 1020, loss = 0.54414
I1106 14:56:37.449615 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 14:56:37.449630 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.290402 (* 1 = 0.290402 loss)
I1106 14:56:37.449635 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0706773 (* 1 = 0.0706773 loss)
I1106 14:56:37.449640 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.106106 (* 1 = 0.106106 loss)
I1106 14:56:37.449646 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0413818 (* 1 = 0.0413818 loss)
I1106 14:56:37.449652 31365 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I1106 14:57:29.551841 31365 solver.cpp:229] Iteration 1040, loss = 0.6384
I1106 14:57:29.551918 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 14:57:29.551933 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.396984 (* 1 = 0.396984 loss)
I1106 14:57:29.551939 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.320428 (* 1 = 0.320428 loss)
I1106 14:57:29.551944 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.035191 (* 1 = 0.035191 loss)
I1106 14:57:29.551949 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0171935 (* 1 = 0.0171935 loss)
I1106 14:57:29.551954 31365 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I1106 14:58:21.656414 31365 solver.cpp:229] Iteration 1060, loss = 0.595809
I1106 14:58:21.656492 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 14:58:21.656505 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.248561 (* 1 = 0.248561 loss)
I1106 14:58:21.656512 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.246172 (* 1 = 0.246172 loss)
I1106 14:58:21.656517 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0331105 (* 1 = 0.0331105 loss)
I1106 14:58:21.656522 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0289818 (* 1 = 0.0289818 loss)
I1106 14:58:21.656529 31365 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I1106 14:59:13.745769 31365 solver.cpp:229] Iteration 1080, loss = 0.611829
I1106 14:59:13.745846 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 14:59:13.745860 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.254037 (* 1 = 0.254037 loss)
I1106 14:59:13.745867 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.231395 (* 1 = 0.231395 loss)
I1106 14:59:13.745872 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0620766 (* 1 = 0.0620766 loss)
I1106 14:59:13.745877 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0717644 (* 1 = 0.0717644 loss)
I1106 14:59:13.745883 31365 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I1106 15:00:05.806087 31365 solver.cpp:229] Iteration 1100, loss = 0.627826
I1106 15:00:05.806169 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:00:05.806182 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.421223 (* 1 = 0.421223 loss)
I1106 15:00:05.806190 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.220625 (* 1 = 0.220625 loss)
I1106 15:00:05.806195 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0355721 (* 1 = 0.0355721 loss)
I1106 15:00:05.806200 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0507535 (* 1 = 0.0507535 loss)
I1106 15:00:05.806206 31365 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1106 15:00:57.917449 31365 solver.cpp:229] Iteration 1120, loss = 0.574252
I1106 15:00:57.917529 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:00:57.917543 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.231639 (* 1 = 0.231639 loss)
I1106 15:00:57.917551 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0767921 (* 1 = 0.0767921 loss)
I1106 15:00:57.917556 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0533466 (* 1 = 0.0533466 loss)
I1106 15:00:57.917560 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0520742 (* 1 = 0.0520742 loss)
I1106 15:00:57.917567 31365 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I1106 15:01:50.003090 31365 solver.cpp:229] Iteration 1140, loss = 0.649536
I1106 15:01:50.003171 31365 solver.cpp:245]     Train net output #0: accuracy = 0.828125
I1106 15:01:50.003185 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.302341 (* 1 = 0.302341 loss)
I1106 15:01:50.003192 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.378306 (* 1 = 0.378306 loss)
I1106 15:01:50.003197 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0380881 (* 1 = 0.0380881 loss)
I1106 15:01:50.003202 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.045008 (* 1 = 0.045008 loss)
I1106 15:01:50.003209 31365 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I1106 15:02:42.159353 31365 solver.cpp:229] Iteration 1160, loss = 0.644953
I1106 15:02:42.159427 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:02:42.159442 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.331173 (* 1 = 0.331173 loss)
I1106 15:02:42.159448 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.287666 (* 1 = 0.287666 loss)
I1106 15:02:42.159454 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0197222 (* 1 = 0.0197222 loss)
I1106 15:02:42.159459 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0444891 (* 1 = 0.0444891 loss)
I1106 15:02:42.159466 31365 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I1106 15:03:34.323165 31365 solver.cpp:229] Iteration 1180, loss = 0.562899
I1106 15:03:34.323246 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:03:34.323261 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.276442 (* 1 = 0.276442 loss)
I1106 15:03:34.323266 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.149309 (* 1 = 0.149309 loss)
I1106 15:03:34.323271 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.061937 (* 1 = 0.061937 loss)
I1106 15:03:34.323276 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.1249 (* 1 = 0.1249 loss)
I1106 15:03:34.323297 31365 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I1106 15:04:26.494566 31365 solver.cpp:229] Iteration 1200, loss = 0.561409
I1106 15:04:26.494642 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:04:26.494654 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.350095 (* 1 = 0.350095 loss)
I1106 15:04:26.494663 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.057875 (* 1 = 0.057875 loss)
I1106 15:04:26.494668 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.188461 (* 1 = 0.188461 loss)
I1106 15:04:26.494673 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.115315 (* 1 = 0.115315 loss)
I1106 15:04:26.494680 31365 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1106 15:05:18.669606 31365 solver.cpp:229] Iteration 1220, loss = 0.448487
I1106 15:05:18.669682 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:05:18.669695 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.330164 (* 1 = 0.330164 loss)
I1106 15:05:18.669701 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0946723 (* 1 = 0.0946723 loss)
I1106 15:05:18.669706 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0147416 (* 1 = 0.0147416 loss)
I1106 15:05:18.669711 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0300348 (* 1 = 0.0300348 loss)
I1106 15:05:18.669719 31365 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I1106 15:06:10.808755 31365 solver.cpp:229] Iteration 1240, loss = 0.533784
I1106 15:06:10.808825 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 15:06:10.808836 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.26092 (* 1 = 0.26092 loss)
I1106 15:06:10.808842 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.260649 (* 1 = 0.260649 loss)
I1106 15:06:10.808847 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0515988 (* 1 = 0.0515988 loss)
I1106 15:06:10.808852 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0636635 (* 1 = 0.0636635 loss)
I1106 15:06:10.808859 31365 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I1106 15:07:02.943790 31365 solver.cpp:229] Iteration 1260, loss = 0.464479
I1106 15:07:02.943857 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:07:02.943869 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.348849 (* 1 = 0.348849 loss)
I1106 15:07:02.943876 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.146424 (* 1 = 0.146424 loss)
I1106 15:07:02.943881 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0156275 (* 1 = 0.0156275 loss)
I1106 15:07:02.943886 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0492083 (* 1 = 0.0492083 loss)
I1106 15:07:02.943892 31365 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I1106 15:07:55.066864 31365 solver.cpp:229] Iteration 1280, loss = 0.525835
I1106 15:07:55.066934 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:07:55.066947 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.310742 (* 1 = 0.310742 loss)
I1106 15:07:55.066956 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0953446 (* 1 = 0.0953446 loss)
I1106 15:07:55.066961 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0233006 (* 1 = 0.0233006 loss)
I1106 15:07:55.066965 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0530095 (* 1 = 0.0530095 loss)
I1106 15:07:55.066972 31365 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I1106 15:08:47.228226 31365 solver.cpp:229] Iteration 1300, loss = 0.483827
I1106 15:08:47.228299 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:08:47.228312 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.323322 (* 1 = 0.323322 loss)
I1106 15:08:47.228320 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.153532 (* 1 = 0.153532 loss)
I1106 15:08:47.228325 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0202883 (* 1 = 0.0202883 loss)
I1106 15:08:47.228330 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0231033 (* 1 = 0.0231033 loss)
I1106 15:08:47.228338 31365 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1106 15:09:39.405597 31365 solver.cpp:229] Iteration 1320, loss = 0.855224
I1106 15:09:39.405686 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:09:39.405699 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.422585 (* 1 = 0.422585 loss)
I1106 15:09:39.405705 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.146548 (* 1 = 0.146548 loss)
I1106 15:09:39.405710 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0138853 (* 1 = 0.0138853 loss)
I1106 15:09:39.405715 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.051988 (* 1 = 0.051988 loss)
I1106 15:09:39.405722 31365 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I1106 15:10:31.561960 31365 solver.cpp:229] Iteration 1340, loss = 0.530213
I1106 15:10:31.562033 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:10:31.562047 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.360143 (* 1 = 0.360143 loss)
I1106 15:10:31.562052 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.172938 (* 1 = 0.172938 loss)
I1106 15:10:31.562057 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0154987 (* 1 = 0.0154987 loss)
I1106 15:10:31.562062 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.010719 (* 1 = 0.010719 loss)
I1106 15:10:31.562069 31365 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I1106 15:11:23.734484 31365 solver.cpp:229] Iteration 1360, loss = 0.555036
I1106 15:11:23.734561 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:11:23.734575 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.303308 (* 1 = 0.303308 loss)
I1106 15:11:23.734580 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.181201 (* 1 = 0.181201 loss)
I1106 15:11:23.734585 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0214849 (* 1 = 0.0214849 loss)
I1106 15:11:23.734591 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0350147 (* 1 = 0.0350147 loss)
I1106 15:11:23.734597 31365 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I1106 15:12:15.880408 31365 solver.cpp:229] Iteration 1380, loss = 0.736201
I1106 15:12:15.880476 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:12:15.880488 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.32134 (* 1 = 0.32134 loss)
I1106 15:12:15.880494 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.214586 (* 1 = 0.214586 loss)
I1106 15:12:15.880499 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0540914 (* 1 = 0.0540914 loss)
I1106 15:12:15.880504 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0850049 (* 1 = 0.0850049 loss)
I1106 15:12:15.880511 31365 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I1106 15:13:07.973994 31365 solver.cpp:229] Iteration 1400, loss = 0.739833
I1106 15:13:07.974062 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 15:13:07.974074 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.361271 (* 1 = 0.361271 loss)
I1106 15:13:07.974082 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.344286 (* 1 = 0.344286 loss)
I1106 15:13:07.974087 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.127096 (* 1 = 0.127096 loss)
I1106 15:13:07.974093 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0738173 (* 1 = 0.0738173 loss)
I1106 15:13:07.974099 31365 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1106 15:14:00.130398 31365 solver.cpp:229] Iteration 1420, loss = 0.620898
I1106 15:14:00.130465 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:14:00.130478 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.300419 (* 1 = 0.300419 loss)
I1106 15:14:00.130484 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116599 (* 1 = 0.116599 loss)
I1106 15:14:00.130489 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0276485 (* 1 = 0.0276485 loss)
I1106 15:14:00.130494 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0705306 (* 1 = 0.0705306 loss)
I1106 15:14:00.130501 31365 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I1106 15:14:52.261595 31365 solver.cpp:229] Iteration 1440, loss = 0.542664
I1106 15:14:52.261662 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:14:52.261674 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.282252 (* 1 = 0.282252 loss)
I1106 15:14:52.261679 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.191886 (* 1 = 0.191886 loss)
I1106 15:14:52.261685 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.026943 (* 1 = 0.026943 loss)
I1106 15:14:52.261690 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0674369 (* 1 = 0.0674369 loss)
I1106 15:14:52.261696 31365 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I1106 15:15:44.364032 31365 solver.cpp:229] Iteration 1460, loss = 0.746207
I1106 15:15:44.364099 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:15:44.364111 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.267506 (* 1 = 0.267506 loss)
I1106 15:15:44.364117 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.163621 (* 1 = 0.163621 loss)
I1106 15:15:44.364122 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0351725 (* 1 = 0.0351725 loss)
I1106 15:15:44.364127 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0248971 (* 1 = 0.0248971 loss)
I1106 15:15:44.364133 31365 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I1106 15:16:36.494854 31365 solver.cpp:229] Iteration 1480, loss = 0.495741
I1106 15:16:36.494930 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:16:36.494942 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.356138 (* 1 = 0.356138 loss)
I1106 15:16:36.494948 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0523108 (* 1 = 0.0523108 loss)
I1106 15:16:36.494953 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0518286 (* 1 = 0.0518286 loss)
I1106 15:16:36.494958 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0469237 (* 1 = 0.0469237 loss)
I1106 15:16:36.494966 31365 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I1106 15:17:28.647864 31365 solver.cpp:229] Iteration 1500, loss = 0.481746
I1106 15:17:28.647938 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:17:28.647951 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.308531 (* 1 = 0.308531 loss)
I1106 15:17:28.647958 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.173474 (* 1 = 0.173474 loss)
I1106 15:17:28.647964 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.042404 (* 1 = 0.042404 loss)
I1106 15:17:28.647969 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0451378 (* 1 = 0.0451378 loss)
I1106 15:17:28.647975 31365 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1106 15:18:20.790887 31365 solver.cpp:229] Iteration 1520, loss = 0.657201
I1106 15:18:20.790959 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:18:20.790972 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.460185 (* 1 = 0.460185 loss)
I1106 15:18:20.790979 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.247035 (* 1 = 0.247035 loss)
I1106 15:18:20.790984 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0532588 (* 1 = 0.0532588 loss)
I1106 15:18:20.790989 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0419774 (* 1 = 0.0419774 loss)
I1106 15:18:20.790997 31365 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I1106 15:19:12.911013 31365 solver.cpp:229] Iteration 1540, loss = 0.593518
I1106 15:19:12.911088 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:19:12.911101 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.343037 (* 1 = 0.343037 loss)
I1106 15:19:12.911106 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.178254 (* 1 = 0.178254 loss)
I1106 15:19:12.911113 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.057176 (* 1 = 0.057176 loss)
I1106 15:19:12.911118 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0550077 (* 1 = 0.0550077 loss)
I1106 15:19:12.911123 31365 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I1106 15:20:05.087873 31365 solver.cpp:229] Iteration 1560, loss = 0.413231
I1106 15:20:05.087945 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:20:05.087958 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.286441 (* 1 = 0.286441 loss)
I1106 15:20:05.087967 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.122168 (* 1 = 0.122168 loss)
I1106 15:20:05.087972 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0132874 (* 1 = 0.0132874 loss)
I1106 15:20:05.087977 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.046581 (* 1 = 0.046581 loss)
I1106 15:20:05.087985 31365 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I1106 15:20:57.156111 31365 solver.cpp:229] Iteration 1580, loss = 0.466106
I1106 15:20:57.156177 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:20:57.156189 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.237627 (* 1 = 0.237627 loss)
I1106 15:20:57.156195 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.188119 (* 1 = 0.188119 loss)
I1106 15:20:57.156200 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0299159 (* 1 = 0.0299159 loss)
I1106 15:20:57.156205 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0316122 (* 1 = 0.0316122 loss)
I1106 15:20:57.156213 31365 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I1106 15:21:49.231685 31365 solver.cpp:229] Iteration 1600, loss = 0.488655
I1106 15:21:49.231755 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:21:49.231765 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.414497 (* 1 = 0.414497 loss)
I1106 15:21:49.231770 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.135402 (* 1 = 0.135402 loss)
I1106 15:21:49.231776 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00688534 (* 1 = 0.00688534 loss)
I1106 15:21:49.231781 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0127658 (* 1 = 0.0127658 loss)
I1106 15:21:49.231788 31365 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1106 15:22:41.299599 31365 solver.cpp:229] Iteration 1620, loss = 0.767638
I1106 15:22:41.299649 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:22:41.299664 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.233939 (* 1 = 0.233939 loss)
I1106 15:22:41.299672 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.143236 (* 1 = 0.143236 loss)
I1106 15:22:41.299680 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0511097 (* 1 = 0.0511097 loss)
I1106 15:22:41.299688 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0374989 (* 1 = 0.0374989 loss)
I1106 15:22:41.299696 31365 sgd_solver.cpp:106] Iteration 1620, lr = 0.001
I1106 15:23:33.442558 31365 solver.cpp:229] Iteration 1640, loss = 0.43388
I1106 15:23:33.442623 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:23:33.442636 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.280107 (* 1 = 0.280107 loss)
I1106 15:23:33.442641 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0845776 (* 1 = 0.0845776 loss)
I1106 15:23:33.442646 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0331052 (* 1 = 0.0331052 loss)
I1106 15:23:33.442651 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0470894 (* 1 = 0.0470894 loss)
I1106 15:23:33.442657 31365 sgd_solver.cpp:106] Iteration 1640, lr = 0.001
I1106 15:24:25.555631 31365 solver.cpp:229] Iteration 1660, loss = 0.62643
I1106 15:24:25.555701 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 15:24:25.555713 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.382173 (* 1 = 0.382173 loss)
I1106 15:24:25.555718 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.308659 (* 1 = 0.308659 loss)
I1106 15:24:25.555723 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.040993 (* 1 = 0.040993 loss)
I1106 15:24:25.555729 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.104185 (* 1 = 0.104185 loss)
I1106 15:24:25.555735 31365 sgd_solver.cpp:106] Iteration 1660, lr = 0.001
I1106 15:25:17.669186 31365 solver.cpp:229] Iteration 1680, loss = 0.506298
I1106 15:25:17.669261 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:25:17.669275 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.419873 (* 1 = 0.419873 loss)
I1106 15:25:17.669281 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0925752 (* 1 = 0.0925752 loss)
I1106 15:25:17.669287 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00996866 (* 1 = 0.00996866 loss)
I1106 15:25:17.669292 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0134612 (* 1 = 0.0134612 loss)
I1106 15:25:17.669299 31365 sgd_solver.cpp:106] Iteration 1680, lr = 0.001
I1106 15:26:09.784456 31365 solver.cpp:229] Iteration 1700, loss = 0.616539
I1106 15:26:09.784533 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:26:09.784546 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.350692 (* 1 = 0.350692 loss)
I1106 15:26:09.784552 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.241214 (* 1 = 0.241214 loss)
I1106 15:26:09.784559 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0544258 (* 1 = 0.0544258 loss)
I1106 15:26:09.784564 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0772296 (* 1 = 0.0772296 loss)
I1106 15:26:09.784570 31365 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1106 15:27:01.888314 31365 solver.cpp:229] Iteration 1720, loss = 0.582428
I1106 15:27:01.888391 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:27:01.888404 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.336136 (* 1 = 0.336136 loss)
I1106 15:27:01.888409 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.137081 (* 1 = 0.137081 loss)
I1106 15:27:01.888415 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0394218 (* 1 = 0.0394218 loss)
I1106 15:27:01.888420 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.119718 (* 1 = 0.119718 loss)
I1106 15:27:01.888427 31365 sgd_solver.cpp:106] Iteration 1720, lr = 0.001
I1106 15:27:54.030572 31365 solver.cpp:229] Iteration 1740, loss = 0.53234
I1106 15:27:54.030648 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:27:54.030663 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.224413 (* 1 = 0.224413 loss)
I1106 15:27:54.030668 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.150448 (* 1 = 0.150448 loss)
I1106 15:27:54.030673 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0327056 (* 1 = 0.0327056 loss)
I1106 15:27:54.030678 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0458988 (* 1 = 0.0458988 loss)
I1106 15:27:54.030688 31365 sgd_solver.cpp:106] Iteration 1740, lr = 0.001
I1106 15:28:46.153780 31365 solver.cpp:229] Iteration 1760, loss = 0.457032
I1106 15:28:46.153857 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:28:46.153872 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.160035 (* 1 = 0.160035 loss)
I1106 15:28:46.153877 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.146207 (* 1 = 0.146207 loss)
I1106 15:28:46.153882 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.042069 (* 1 = 0.042069 loss)
I1106 15:28:46.153887 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0291624 (* 1 = 0.0291624 loss)
I1106 15:28:46.153894 31365 sgd_solver.cpp:106] Iteration 1760, lr = 0.001
I1106 15:29:38.302279 31365 solver.cpp:229] Iteration 1780, loss = 0.48991
I1106 15:29:38.302356 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:29:38.302369 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.256783 (* 1 = 0.256783 loss)
I1106 15:29:38.302376 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.172331 (* 1 = 0.172331 loss)
I1106 15:29:38.302397 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0332832 (* 1 = 0.0332832 loss)
I1106 15:29:38.302402 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0155888 (* 1 = 0.0155888 loss)
I1106 15:29:38.302410 31365 sgd_solver.cpp:106] Iteration 1780, lr = 0.001
I1106 15:30:30.419945 31365 solver.cpp:229] Iteration 1800, loss = 0.549093
I1106 15:30:30.420018 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:30:30.420032 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.217888 (* 1 = 0.217888 loss)
I1106 15:30:30.420037 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.205748 (* 1 = 0.205748 loss)
I1106 15:30:30.420042 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0145578 (* 1 = 0.0145578 loss)
I1106 15:30:30.420047 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0391732 (* 1 = 0.0391732 loss)
I1106 15:30:30.420053 31365 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1106 15:31:22.581388 31365 solver.cpp:229] Iteration 1820, loss = 0.615899
I1106 15:31:22.581459 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:31:22.581471 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.266166 (* 1 = 0.266166 loss)
I1106 15:31:22.581476 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.111811 (* 1 = 0.111811 loss)
I1106 15:31:22.581482 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0749924 (* 1 = 0.0749924 loss)
I1106 15:31:22.581487 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0549084 (* 1 = 0.0549084 loss)
I1106 15:31:22.581493 31365 sgd_solver.cpp:106] Iteration 1820, lr = 0.001
I1106 15:32:14.697213 31365 solver.cpp:229] Iteration 1840, loss = 0.425483
I1106 15:32:14.697281 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:32:14.697293 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.243917 (* 1 = 0.243917 loss)
I1106 15:32:14.697300 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.101595 (* 1 = 0.101595 loss)
I1106 15:32:14.697305 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0573468 (* 1 = 0.0573468 loss)
I1106 15:32:14.697310 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0183725 (* 1 = 0.0183725 loss)
I1106 15:32:14.697316 31365 sgd_solver.cpp:106] Iteration 1840, lr = 0.001
I1106 15:33:06.846768 31365 solver.cpp:229] Iteration 1860, loss = 0.356387
I1106 15:33:06.846837 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:33:06.846848 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.199286 (* 1 = 0.199286 loss)
I1106 15:33:06.846858 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0909294 (* 1 = 0.0909294 loss)
I1106 15:33:06.846863 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0337637 (* 1 = 0.0337637 loss)
I1106 15:33:06.846868 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0200265 (* 1 = 0.0200265 loss)
I1106 15:33:06.846873 31365 sgd_solver.cpp:106] Iteration 1860, lr = 0.001
I1106 15:33:58.988440 31365 solver.cpp:229] Iteration 1880, loss = 0.754843
I1106 15:33:58.988509 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:33:58.988521 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.233231 (* 1 = 0.233231 loss)
I1106 15:33:58.988529 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0733569 (* 1 = 0.0733569 loss)
I1106 15:33:58.988534 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00600098 (* 1 = 0.00600098 loss)
I1106 15:33:58.988539 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0125431 (* 1 = 0.0125431 loss)
I1106 15:33:58.988546 31365 sgd_solver.cpp:106] Iteration 1880, lr = 0.001
I1106 15:34:51.062328 31365 solver.cpp:229] Iteration 1900, loss = 0.713902
I1106 15:34:51.062404 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:34:51.062418 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.524074 (* 1 = 0.524074 loss)
I1106 15:34:51.062423 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.166947 (* 1 = 0.166947 loss)
I1106 15:34:51.062429 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.12339 (* 1 = 0.12339 loss)
I1106 15:34:51.062434 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.05656 (* 1 = 0.05656 loss)
I1106 15:34:51.062453 31365 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1106 15:35:43.137200 31365 solver.cpp:229] Iteration 1920, loss = 0.358316
I1106 15:35:43.137269 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:35:43.137281 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.213059 (* 1 = 0.213059 loss)
I1106 15:35:43.137287 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.126139 (* 1 = 0.126139 loss)
I1106 15:35:43.137292 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0207103 (* 1 = 0.0207103 loss)
I1106 15:35:43.137297 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.017225 (* 1 = 0.017225 loss)
I1106 15:35:43.137303 31365 sgd_solver.cpp:106] Iteration 1920, lr = 0.001
I1106 15:36:35.228823 31365 solver.cpp:229] Iteration 1940, loss = 0.512066
I1106 15:36:35.228890 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:36:35.228904 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.331446 (* 1 = 0.331446 loss)
I1106 15:36:35.228909 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.170348 (* 1 = 0.170348 loss)
I1106 15:36:35.228915 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0234996 (* 1 = 0.0234996 loss)
I1106 15:36:35.228920 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0930126 (* 1 = 0.0930126 loss)
I1106 15:36:35.228927 31365 sgd_solver.cpp:106] Iteration 1940, lr = 0.001
I1106 15:37:27.334967 31365 solver.cpp:229] Iteration 1960, loss = 0.507004
I1106 15:37:27.335036 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:37:27.335048 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.225823 (* 1 = 0.225823 loss)
I1106 15:37:27.335057 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.235989 (* 1 = 0.235989 loss)
I1106 15:37:27.335062 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0200409 (* 1 = 0.0200409 loss)
I1106 15:37:27.335067 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0149973 (* 1 = 0.0149973 loss)
I1106 15:37:27.335073 31365 sgd_solver.cpp:106] Iteration 1960, lr = 0.001
I1106 15:38:19.450928 31365 solver.cpp:229] Iteration 1980, loss = 0.530985
I1106 15:38:19.450999 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:38:19.451011 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.248816 (* 1 = 0.248816 loss)
I1106 15:38:19.451016 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.199477 (* 1 = 0.199477 loss)
I1106 15:38:19.451022 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0203144 (* 1 = 0.0203144 loss)
I1106 15:38:19.451027 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0658285 (* 1 = 0.0658285 loss)
I1106 15:38:19.451035 31365 sgd_solver.cpp:106] Iteration 1980, lr = 0.001
I1106 15:39:11.533795 31365 solver.cpp:229] Iteration 2000, loss = 0.828324
I1106 15:39:11.533862 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:39:11.533874 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.420512 (* 1 = 0.420512 loss)
I1106 15:39:11.533880 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.253312 (* 1 = 0.253312 loss)
I1106 15:39:11.533885 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.107072 (* 1 = 0.107072 loss)
I1106 15:39:11.533890 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.303681 (* 1 = 0.303681 loss)
I1106 15:39:11.533896 31365 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1106 15:40:03.634908 31365 solver.cpp:229] Iteration 2020, loss = 0.40236
I1106 15:40:03.634977 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:40:03.634989 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.208384 (* 1 = 0.208384 loss)
I1106 15:40:03.634999 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0698604 (* 1 = 0.0698604 loss)
I1106 15:40:03.635004 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00437256 (* 1 = 0.00437256 loss)
I1106 15:40:03.635010 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0226764 (* 1 = 0.0226764 loss)
I1106 15:40:03.635015 31365 sgd_solver.cpp:106] Iteration 2020, lr = 0.001
I1106 15:40:55.776759 31365 solver.cpp:229] Iteration 2040, loss = 0.436525
I1106 15:40:55.776826 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:40:55.776839 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.307588 (* 1 = 0.307588 loss)
I1106 15:40:55.776847 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0660961 (* 1 = 0.0660961 loss)
I1106 15:40:55.776852 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0138401 (* 1 = 0.0138401 loss)
I1106 15:40:55.776857 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.052924 (* 1 = 0.052924 loss)
I1106 15:40:55.776863 31365 sgd_solver.cpp:106] Iteration 2040, lr = 0.001
I1106 15:41:47.903218 31365 solver.cpp:229] Iteration 2060, loss = 0.586959
I1106 15:41:47.903287 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:41:47.903300 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.331397 (* 1 = 0.331397 loss)
I1106 15:41:47.903307 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.191146 (* 1 = 0.191146 loss)
I1106 15:41:47.903312 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0257808 (* 1 = 0.0257808 loss)
I1106 15:41:47.903317 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0633036 (* 1 = 0.0633036 loss)
I1106 15:41:47.903324 31365 sgd_solver.cpp:106] Iteration 2060, lr = 0.001
I1106 15:42:40.001682 31365 solver.cpp:229] Iteration 2080, loss = 0.437074
I1106 15:42:40.001754 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:42:40.001766 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.264002 (* 1 = 0.264002 loss)
I1106 15:42:40.001775 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.24299 (* 1 = 0.24299 loss)
I1106 15:42:40.001781 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0139289 (* 1 = 0.0139289 loss)
I1106 15:42:40.001786 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0231951 (* 1 = 0.0231951 loss)
I1106 15:42:40.001793 31365 sgd_solver.cpp:106] Iteration 2080, lr = 0.001
I1106 15:43:32.073050 31365 solver.cpp:229] Iteration 2100, loss = 0.389781
I1106 15:43:32.073117 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:43:32.073128 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.195477 (* 1 = 0.195477 loss)
I1106 15:43:32.073138 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.156593 (* 1 = 0.156593 loss)
I1106 15:43:32.073143 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00649401 (* 1 = 0.00649401 loss)
I1106 15:43:32.073148 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0149909 (* 1 = 0.0149909 loss)
I1106 15:43:32.073155 31365 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1106 15:44:24.192451 31365 solver.cpp:229] Iteration 2120, loss = 0.393392
I1106 15:44:24.192524 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:44:24.192538 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.156432 (* 1 = 0.156432 loss)
I1106 15:44:24.192544 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0699986 (* 1 = 0.0699986 loss)
I1106 15:44:24.192549 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0240619 (* 1 = 0.0240619 loss)
I1106 15:44:24.192554 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0171896 (* 1 = 0.0171896 loss)
I1106 15:44:24.192561 31365 sgd_solver.cpp:106] Iteration 2120, lr = 0.001
I1106 15:45:16.325373 31365 solver.cpp:229] Iteration 2140, loss = 0.413806
I1106 15:45:16.325448 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:45:16.325462 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.236086 (* 1 = 0.236086 loss)
I1106 15:45:16.325467 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.148359 (* 1 = 0.148359 loss)
I1106 15:45:16.325474 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0168212 (* 1 = 0.0168212 loss)
I1106 15:45:16.325479 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0369074 (* 1 = 0.0369074 loss)
I1106 15:45:16.325484 31365 sgd_solver.cpp:106] Iteration 2140, lr = 0.001
I1106 15:46:08.493383 31365 solver.cpp:229] Iteration 2160, loss = 0.465492
I1106 15:46:08.493458 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:46:08.493472 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.260434 (* 1 = 0.260434 loss)
I1106 15:46:08.493479 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.209097 (* 1 = 0.209097 loss)
I1106 15:46:08.493484 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0715739 (* 1 = 0.0715739 loss)
I1106 15:46:08.493489 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0176302 (* 1 = 0.0176302 loss)
I1106 15:46:08.493495 31365 sgd_solver.cpp:106] Iteration 2160, lr = 0.001
I1106 15:47:00.665196 31365 solver.cpp:229] Iteration 2180, loss = 0.525316
I1106 15:47:00.665271 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 15:47:00.665284 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.188895 (* 1 = 0.188895 loss)
I1106 15:47:00.665292 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0550612 (* 1 = 0.0550612 loss)
I1106 15:47:00.665297 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.013755 (* 1 = 0.013755 loss)
I1106 15:47:00.665302 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.066099 (* 1 = 0.066099 loss)
I1106 15:47:00.665308 31365 sgd_solver.cpp:106] Iteration 2180, lr = 0.001
I1106 15:47:52.761574 31365 solver.cpp:229] Iteration 2200, loss = 0.415602
I1106 15:47:52.761647 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 15:47:52.761660 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.217139 (* 1 = 0.217139 loss)
I1106 15:47:52.761667 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0940398 (* 1 = 0.0940398 loss)
I1106 15:47:52.761672 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0418891 (* 1 = 0.0418891 loss)
I1106 15:47:52.761677 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0279144 (* 1 = 0.0279144 loss)
I1106 15:47:52.761684 31365 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1106 15:48:44.892673 31365 solver.cpp:229] Iteration 2220, loss = 0.616471
I1106 15:48:44.892746 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:48:44.892760 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.207659 (* 1 = 0.207659 loss)
I1106 15:48:44.892765 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.232391 (* 1 = 0.232391 loss)
I1106 15:48:44.892771 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00884543 (* 1 = 0.00884543 loss)
I1106 15:48:44.892776 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0166663 (* 1 = 0.0166663 loss)
I1106 15:48:44.892783 31365 sgd_solver.cpp:106] Iteration 2220, lr = 0.001
I1106 15:49:37.067909 31365 solver.cpp:229] Iteration 2240, loss = 0.421993
I1106 15:49:37.067983 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:49:37.067997 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.299443 (* 1 = 0.299443 loss)
I1106 15:49:37.068002 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.160722 (* 1 = 0.160722 loss)
I1106 15:49:37.068008 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0122736 (* 1 = 0.0122736 loss)
I1106 15:49:37.068013 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0119904 (* 1 = 0.0119904 loss)
I1106 15:49:37.068020 31365 sgd_solver.cpp:106] Iteration 2240, lr = 0.001
I1106 15:50:29.233228 31365 solver.cpp:229] Iteration 2260, loss = 0.476218
I1106 15:50:29.233304 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:50:29.233317 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.180016 (* 1 = 0.180016 loss)
I1106 15:50:29.233323 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.18419 (* 1 = 0.18419 loss)
I1106 15:50:29.233328 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0224327 (* 1 = 0.0224327 loss)
I1106 15:50:29.233335 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0233008 (* 1 = 0.0233008 loss)
I1106 15:50:29.233341 31365 sgd_solver.cpp:106] Iteration 2260, lr = 0.001
I1106 15:51:21.375130 31365 solver.cpp:229] Iteration 2280, loss = 0.559125
I1106 15:51:21.375203 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 15:51:21.375217 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.347268 (* 1 = 0.347268 loss)
I1106 15:51:21.375223 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.174487 (* 1 = 0.174487 loss)
I1106 15:51:21.375228 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0532692 (* 1 = 0.0532692 loss)
I1106 15:51:21.375234 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0270247 (* 1 = 0.0270247 loss)
I1106 15:51:21.375241 31365 sgd_solver.cpp:106] Iteration 2280, lr = 0.001
I1106 15:52:13.567497 31365 solver.cpp:229] Iteration 2300, loss = 0.615951
I1106 15:52:13.567572 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:52:13.567586 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.256843 (* 1 = 0.256843 loss)
I1106 15:52:13.567592 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.209974 (* 1 = 0.209974 loss)
I1106 15:52:13.567597 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.072909 (* 1 = 0.072909 loss)
I1106 15:52:13.567602 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0792564 (* 1 = 0.0792564 loss)
I1106 15:52:13.567610 31365 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1106 15:53:05.725512 31365 solver.cpp:229] Iteration 2320, loss = 0.48532
I1106 15:53:05.725584 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 15:53:05.725597 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.244423 (* 1 = 0.244423 loss)
I1106 15:53:05.725603 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.163242 (* 1 = 0.163242 loss)
I1106 15:53:05.725608 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0220271 (* 1 = 0.0220271 loss)
I1106 15:53:05.725615 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0509296 (* 1 = 0.0509296 loss)
I1106 15:53:05.725621 31365 sgd_solver.cpp:106] Iteration 2320, lr = 0.001
I1106 15:53:57.855978 31365 solver.cpp:229] Iteration 2340, loss = 0.630597
I1106 15:53:57.856047 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 15:53:57.856060 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.387263 (* 1 = 0.387263 loss)
I1106 15:53:57.856067 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.205872 (* 1 = 0.205872 loss)
I1106 15:53:57.856073 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0152347 (* 1 = 0.0152347 loss)
I1106 15:53:57.856078 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0387772 (* 1 = 0.0387772 loss)
I1106 15:53:57.856084 31365 sgd_solver.cpp:106] Iteration 2340, lr = 0.001
I1106 15:54:49.935235 31365 solver.cpp:229] Iteration 2360, loss = 0.437027
I1106 15:54:49.935304 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:54:49.935317 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.193321 (* 1 = 0.193321 loss)
I1106 15:54:49.935325 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0953914 (* 1 = 0.0953914 loss)
I1106 15:54:49.935330 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.019912 (* 1 = 0.019912 loss)
I1106 15:54:49.935335 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0389621 (* 1 = 0.0389621 loss)
I1106 15:54:49.935341 31365 sgd_solver.cpp:106] Iteration 2360, lr = 0.001
I1106 15:55:42.018174 31365 solver.cpp:229] Iteration 2380, loss = 0.556565
I1106 15:55:42.018244 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 15:55:42.018255 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.206911 (* 1 = 0.206911 loss)
I1106 15:55:42.018261 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.333775 (* 1 = 0.333775 loss)
I1106 15:55:42.018266 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0319279 (* 1 = 0.0319279 loss)
I1106 15:55:42.018271 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.049851 (* 1 = 0.049851 loss)
I1106 15:55:42.018277 31365 sgd_solver.cpp:106] Iteration 2380, lr = 0.001
I1106 15:56:34.133757 31365 solver.cpp:229] Iteration 2400, loss = 0.379569
I1106 15:56:34.133832 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:56:34.133846 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.200747 (* 1 = 0.200747 loss)
I1106 15:56:34.133852 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.142105 (* 1 = 0.142105 loss)
I1106 15:56:34.133858 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0190146 (* 1 = 0.0190146 loss)
I1106 15:56:34.133863 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0417331 (* 1 = 0.0417331 loss)
I1106 15:56:34.133870 31365 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1106 15:57:26.256399 31365 solver.cpp:229] Iteration 2420, loss = 0.362875
I1106 15:57:26.256475 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:57:26.256489 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.318987 (* 1 = 0.318987 loss)
I1106 15:57:26.256495 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.120795 (* 1 = 0.120795 loss)
I1106 15:57:26.256500 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0156729 (* 1 = 0.0156729 loss)
I1106 15:57:26.256505 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00767136 (* 1 = 0.00767136 loss)
I1106 15:57:26.256513 31365 sgd_solver.cpp:106] Iteration 2420, lr = 0.001
I1106 15:58:18.415336 31365 solver.cpp:229] Iteration 2440, loss = 0.414809
I1106 15:58:18.415410 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 15:58:18.415424 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.240042 (* 1 = 0.240042 loss)
I1106 15:58:18.415431 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.133033 (* 1 = 0.133033 loss)
I1106 15:58:18.415436 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0288251 (* 1 = 0.0288251 loss)
I1106 15:58:18.415441 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0435626 (* 1 = 0.0435626 loss)
I1106 15:58:18.415447 31365 sgd_solver.cpp:106] Iteration 2440, lr = 0.001
I1106 15:59:10.550016 31365 solver.cpp:229] Iteration 2460, loss = 0.384951
I1106 15:59:10.550091 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 15:59:10.550103 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.225572 (* 1 = 0.225572 loss)
I1106 15:59:10.550112 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.150131 (* 1 = 0.150131 loss)
I1106 15:59:10.550117 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0205027 (* 1 = 0.0205027 loss)
I1106 15:59:10.550122 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0150277 (* 1 = 0.0150277 loss)
I1106 15:59:10.550128 31365 sgd_solver.cpp:106] Iteration 2460, lr = 0.001
I1106 16:00:02.661839 31365 solver.cpp:229] Iteration 2480, loss = 0.632079
I1106 16:00:02.661914 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:00:02.661928 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.239094 (* 1 = 0.239094 loss)
I1106 16:00:02.661933 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.198484 (* 1 = 0.198484 loss)
I1106 16:00:02.661939 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0226753 (* 1 = 0.0226753 loss)
I1106 16:00:02.661944 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0231681 (* 1 = 0.0231681 loss)
I1106 16:00:02.661952 31365 sgd_solver.cpp:106] Iteration 2480, lr = 0.001
I1106 16:00:54.761153 31365 solver.cpp:229] Iteration 2500, loss = 0.594064
I1106 16:00:54.761227 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:00:54.761241 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.231148 (* 1 = 0.231148 loss)
I1106 16:00:54.761247 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.263915 (* 1 = 0.263915 loss)
I1106 16:00:54.761252 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0264406 (* 1 = 0.0264406 loss)
I1106 16:00:54.761257 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0260266 (* 1 = 0.0260266 loss)
I1106 16:00:54.761265 31365 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1106 16:01:46.899780 31365 solver.cpp:229] Iteration 2520, loss = 0.584357
I1106 16:01:46.899854 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 16:01:46.899868 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.354364 (* 1 = 0.354364 loss)
I1106 16:01:46.899875 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0449781 (* 1 = 0.0449781 loss)
I1106 16:01:46.899880 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0927923 (* 1 = 0.0927923 loss)
I1106 16:01:46.899885 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.068485 (* 1 = 0.068485 loss)
I1106 16:01:46.899893 31365 sgd_solver.cpp:106] Iteration 2520, lr = 0.001
I1106 16:02:39.046043 31365 solver.cpp:229] Iteration 2540, loss = 0.365592
I1106 16:02:39.046114 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 16:02:39.046126 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.176164 (* 1 = 0.176164 loss)
I1106 16:02:39.046135 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0231663 (* 1 = 0.0231663 loss)
I1106 16:02:39.046140 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0254747 (* 1 = 0.0254747 loss)
I1106 16:02:39.046145 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.038186 (* 1 = 0.038186 loss)
I1106 16:02:39.046152 31365 sgd_solver.cpp:106] Iteration 2540, lr = 0.001
I1106 16:03:31.131268 31365 solver.cpp:229] Iteration 2560, loss = 0.475537
I1106 16:03:31.131335 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:03:31.131348 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.168273 (* 1 = 0.168273 loss)
I1106 16:03:31.131356 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.147811 (* 1 = 0.147811 loss)
I1106 16:03:31.131361 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0365268 (* 1 = 0.0365268 loss)
I1106 16:03:31.131366 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0673415 (* 1 = 0.0673415 loss)
I1106 16:03:31.131373 31365 sgd_solver.cpp:106] Iteration 2560, lr = 0.001
I1106 16:04:23.279203 31365 solver.cpp:229] Iteration 2580, loss = 0.534509
I1106 16:04:23.279273 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 16:04:23.279284 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.247766 (* 1 = 0.247766 loss)
I1106 16:04:23.279294 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.296156 (* 1 = 0.296156 loss)
I1106 16:04:23.279299 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0260706 (* 1 = 0.0260706 loss)
I1106 16:04:23.279304 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0527419 (* 1 = 0.0527419 loss)
I1106 16:04:23.279309 31365 sgd_solver.cpp:106] Iteration 2580, lr = 0.001
I1106 16:05:15.401166 31365 solver.cpp:229] Iteration 2600, loss = 0.548953
I1106 16:05:15.401239 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:05:15.401253 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.255081 (* 1 = 0.255081 loss)
I1106 16:05:15.401258 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.231844 (* 1 = 0.231844 loss)
I1106 16:05:15.401264 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0664305 (* 1 = 0.0664305 loss)
I1106 16:05:15.401269 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0218017 (* 1 = 0.0218017 loss)
I1106 16:05:15.401276 31365 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I1106 16:06:07.490159 31365 solver.cpp:229] Iteration 2620, loss = 0.56089
I1106 16:06:07.490232 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:06:07.490247 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.218885 (* 1 = 0.218885 loss)
I1106 16:06:07.490252 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.183859 (* 1 = 0.183859 loss)
I1106 16:06:07.490257 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.031389 (* 1 = 0.031389 loss)
I1106 16:06:07.490262 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0345636 (* 1 = 0.0345636 loss)
I1106 16:06:07.490269 31365 sgd_solver.cpp:106] Iteration 2620, lr = 0.001
I1106 16:06:59.629022 31365 solver.cpp:229] Iteration 2640, loss = 0.671914
I1106 16:06:59.629092 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:06:59.629104 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.275657 (* 1 = 0.275657 loss)
I1106 16:06:59.629109 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.215155 (* 1 = 0.215155 loss)
I1106 16:06:59.629115 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.019481 (* 1 = 0.019481 loss)
I1106 16:06:59.629120 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0200529 (* 1 = 0.0200529 loss)
I1106 16:06:59.629127 31365 sgd_solver.cpp:106] Iteration 2640, lr = 0.001
I1106 16:07:51.748137 31365 solver.cpp:229] Iteration 2660, loss = 0.41174
I1106 16:07:51.748210 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:07:51.748224 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.142857 (* 1 = 0.142857 loss)
I1106 16:07:51.748230 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.164641 (* 1 = 0.164641 loss)
I1106 16:07:51.748236 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0147762 (* 1 = 0.0147762 loss)
I1106 16:07:51.748241 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0148065 (* 1 = 0.0148065 loss)
I1106 16:07:51.748248 31365 sgd_solver.cpp:106] Iteration 2660, lr = 0.001
I1106 16:08:43.897173 31365 solver.cpp:229] Iteration 2680, loss = 0.386545
I1106 16:08:43.897248 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:08:43.897263 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.216469 (* 1 = 0.216469 loss)
I1106 16:08:43.897269 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0555132 (* 1 = 0.0555132 loss)
I1106 16:08:43.897274 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100068 (* 1 = 0.0100068 loss)
I1106 16:08:43.897279 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0298522 (* 1 = 0.0298522 loss)
I1106 16:08:43.897285 31365 sgd_solver.cpp:106] Iteration 2680, lr = 0.001
I1106 16:09:36.015964 31365 solver.cpp:229] Iteration 2700, loss = 0.365045
I1106 16:09:36.016038 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:09:36.016052 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.209718 (* 1 = 0.209718 loss)
I1106 16:09:36.016058 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.180325 (* 1 = 0.180325 loss)
I1106 16:09:36.016063 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00877145 (* 1 = 0.00877145 loss)
I1106 16:09:36.016069 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00915138 (* 1 = 0.00915138 loss)
I1106 16:09:36.016077 31365 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I1106 16:10:28.199049 31365 solver.cpp:229] Iteration 2720, loss = 0.669814
I1106 16:10:28.199098 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 16:10:28.199112 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.450956 (* 1 = 0.450956 loss)
I1106 16:10:28.199120 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.399442 (* 1 = 0.399442 loss)
I1106 16:10:28.199126 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0250743 (* 1 = 0.0250743 loss)
I1106 16:10:28.199133 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0836121 (* 1 = 0.0836121 loss)
I1106 16:10:28.199141 31365 sgd_solver.cpp:106] Iteration 2720, lr = 0.001
I1106 16:11:20.300568 31365 solver.cpp:229] Iteration 2740, loss = 0.383317
I1106 16:11:20.300634 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:11:20.300647 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.220104 (* 1 = 0.220104 loss)
I1106 16:11:20.300652 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.110022 (* 1 = 0.110022 loss)
I1106 16:11:20.300657 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0210998 (* 1 = 0.0210998 loss)
I1106 16:11:20.300662 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0220679 (* 1 = 0.0220679 loss)
I1106 16:11:20.300668 31365 sgd_solver.cpp:106] Iteration 2740, lr = 0.001
I1106 16:12:12.401983 31365 solver.cpp:229] Iteration 2760, loss = 0.478656
I1106 16:12:12.402052 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 16:12:12.402065 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.241009 (* 1 = 0.241009 loss)
I1106 16:12:12.402070 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.222736 (* 1 = 0.222736 loss)
I1106 16:12:12.402076 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0568205 (* 1 = 0.0568205 loss)
I1106 16:12:12.402081 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0362004 (* 1 = 0.0362004 loss)
I1106 16:12:12.402087 31365 sgd_solver.cpp:106] Iteration 2760, lr = 0.001
I1106 16:13:04.554991 31365 solver.cpp:229] Iteration 2780, loss = 0.519212
I1106 16:13:04.555059 31365 solver.cpp:245]     Train net output #0: accuracy = 0.84375
I1106 16:13:04.555073 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.221215 (* 1 = 0.221215 loss)
I1106 16:13:04.555078 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.320265 (* 1 = 0.320265 loss)
I1106 16:13:04.555083 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0346689 (* 1 = 0.0346689 loss)
I1106 16:13:04.555088 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0242998 (* 1 = 0.0242998 loss)
I1106 16:13:04.555094 31365 sgd_solver.cpp:106] Iteration 2780, lr = 0.001
I1106 16:13:56.668191 31365 solver.cpp:229] Iteration 2800, loss = 0.371154
I1106 16:13:56.668260 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:13:56.668273 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.19688 (* 1 = 0.19688 loss)
I1106 16:13:56.668278 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.137266 (* 1 = 0.137266 loss)
I1106 16:13:56.668284 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00685608 (* 1 = 0.00685608 loss)
I1106 16:13:56.668289 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0131347 (* 1 = 0.0131347 loss)
I1106 16:13:56.668295 31365 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I1106 16:14:48.777153 31365 solver.cpp:229] Iteration 2820, loss = 0.530123
I1106 16:14:48.777226 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:14:48.777241 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.321978 (* 1 = 0.321978 loss)
I1106 16:14:48.777247 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0728814 (* 1 = 0.0728814 loss)
I1106 16:14:48.777252 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0361752 (* 1 = 0.0361752 loss)
I1106 16:14:48.777258 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0542572 (* 1 = 0.0542572 loss)
I1106 16:14:48.777264 31365 sgd_solver.cpp:106] Iteration 2820, lr = 0.001
I1106 16:15:40.960261 31365 solver.cpp:229] Iteration 2840, loss = 0.557776
I1106 16:15:40.960335 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:15:40.960347 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.206861 (* 1 = 0.206861 loss)
I1106 16:15:40.960356 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.208643 (* 1 = 0.208643 loss)
I1106 16:15:40.960361 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.342971 (* 1 = 0.342971 loss)
I1106 16:15:40.960366 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0936207 (* 1 = 0.0936207 loss)
I1106 16:15:40.960372 31365 sgd_solver.cpp:106] Iteration 2840, lr = 0.001
I1106 16:16:33.128947 31365 solver.cpp:229] Iteration 2860, loss = 0.442946
I1106 16:16:33.129019 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:16:33.129032 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.267508 (* 1 = 0.267508 loss)
I1106 16:16:33.129041 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0958513 (* 1 = 0.0958513 loss)
I1106 16:16:33.129046 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0320251 (* 1 = 0.0320251 loss)
I1106 16:16:33.129051 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0207554 (* 1 = 0.0207554 loss)
I1106 16:16:33.129058 31365 sgd_solver.cpp:106] Iteration 2860, lr = 0.001
I1106 16:17:25.267902 31365 solver.cpp:229] Iteration 2880, loss = 0.437049
I1106 16:17:25.267976 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:17:25.267988 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.27881 (* 1 = 0.27881 loss)
I1106 16:17:25.267995 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.213896 (* 1 = 0.213896 loss)
I1106 16:17:25.268002 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0549256 (* 1 = 0.0549256 loss)
I1106 16:17:25.268007 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0280372 (* 1 = 0.0280372 loss)
I1106 16:17:25.268013 31365 sgd_solver.cpp:106] Iteration 2880, lr = 0.001
I1106 16:18:17.400908 31365 solver.cpp:229] Iteration 2900, loss = 0.536958
I1106 16:18:17.400979 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 16:18:17.400991 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.288038 (* 1 = 0.288038 loss)
I1106 16:18:17.401000 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.300801 (* 1 = 0.300801 loss)
I1106 16:18:17.401005 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0360027 (* 1 = 0.0360027 loss)
I1106 16:18:17.401010 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0403067 (* 1 = 0.0403067 loss)
I1106 16:18:17.401018 31365 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I1106 16:19:09.584081 31365 solver.cpp:229] Iteration 2920, loss = 0.575696
I1106 16:19:09.584148 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:19:09.584161 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.293919 (* 1 = 0.293919 loss)
I1106 16:19:09.584167 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.129385 (* 1 = 0.129385 loss)
I1106 16:19:09.584172 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0855953 (* 1 = 0.0855953 loss)
I1106 16:19:09.584177 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.115086 (* 1 = 0.115086 loss)
I1106 16:19:09.584184 31365 sgd_solver.cpp:106] Iteration 2920, lr = 0.001
I1106 16:20:01.712595 31365 solver.cpp:229] Iteration 2940, loss = 0.548505
I1106 16:20:01.712663 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:20:01.712676 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.283924 (* 1 = 0.283924 loss)
I1106 16:20:01.712683 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.102941 (* 1 = 0.102941 loss)
I1106 16:20:01.712689 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0391648 (* 1 = 0.0391648 loss)
I1106 16:20:01.712694 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.028024 (* 1 = 0.028024 loss)
I1106 16:20:01.712700 31365 sgd_solver.cpp:106] Iteration 2940, lr = 0.001
I1106 16:20:53.834784 31365 solver.cpp:229] Iteration 2960, loss = 0.315003
I1106 16:20:53.834852 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:20:53.834864 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.159817 (* 1 = 0.159817 loss)
I1106 16:20:53.834873 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0967453 (* 1 = 0.0967453 loss)
I1106 16:20:53.834878 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0387714 (* 1 = 0.0387714 loss)
I1106 16:20:53.834883 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0314919 (* 1 = 0.0314919 loss)
I1106 16:20:53.834889 31365 sgd_solver.cpp:106] Iteration 2960, lr = 0.001
I1106 16:21:45.950350 31365 solver.cpp:229] Iteration 2980, loss = 0.790751
I1106 16:21:45.950430 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:21:45.950444 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.231126 (* 1 = 0.231126 loss)
I1106 16:21:45.950450 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.138806 (* 1 = 0.138806 loss)
I1106 16:21:45.950456 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0105011 (* 1 = 0.0105011 loss)
I1106 16:21:45.950461 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0319875 (* 1 = 0.0319875 loss)
I1106 16:21:45.950469 31365 sgd_solver.cpp:106] Iteration 2980, lr = 0.001
I1106 16:22:38.060236 31365 solver.cpp:229] Iteration 3000, loss = 0.527001
I1106 16:22:38.060304 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:22:38.060317 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.177987 (* 1 = 0.177987 loss)
I1106 16:22:38.060322 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.101504 (* 1 = 0.101504 loss)
I1106 16:22:38.060328 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0171622 (* 1 = 0.0171622 loss)
I1106 16:22:38.060333 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0276755 (* 1 = 0.0276755 loss)
I1106 16:22:38.060338 31365 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I1106 16:23:30.182024 31365 solver.cpp:229] Iteration 3020, loss = 0.513599
I1106 16:23:30.182096 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:23:30.182109 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.28334 (* 1 = 0.28334 loss)
I1106 16:23:30.182116 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.166591 (* 1 = 0.166591 loss)
I1106 16:23:30.182122 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.089322 (* 1 = 0.089322 loss)
I1106 16:23:30.182127 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.115219 (* 1 = 0.115219 loss)
I1106 16:23:30.182133 31365 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I1106 16:24:22.344588 31365 solver.cpp:229] Iteration 3040, loss = 0.640753
I1106 16:24:22.344660 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:24:22.344672 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.496811 (* 1 = 0.496811 loss)
I1106 16:24:22.344679 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.189266 (* 1 = 0.189266 loss)
I1106 16:24:22.344684 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.163768 (* 1 = 0.163768 loss)
I1106 16:24:22.344689 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.103844 (* 1 = 0.103844 loss)
I1106 16:24:22.344696 31365 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I1106 16:25:14.542065 31365 solver.cpp:229] Iteration 3060, loss = 0.484793
I1106 16:25:14.542140 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:25:14.542155 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.197931 (* 1 = 0.197931 loss)
I1106 16:25:14.542160 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.175101 (* 1 = 0.175101 loss)
I1106 16:25:14.542165 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00506198 (* 1 = 0.00506198 loss)
I1106 16:25:14.542171 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0173964 (* 1 = 0.0173964 loss)
I1106 16:25:14.542177 31365 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I1106 16:26:06.704403 31365 solver.cpp:229] Iteration 3080, loss = 0.460195
I1106 16:26:06.704476 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:26:06.704490 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.294985 (* 1 = 0.294985 loss)
I1106 16:26:06.704496 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0736717 (* 1 = 0.0736717 loss)
I1106 16:26:06.704501 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00731739 (* 1 = 0.00731739 loss)
I1106 16:26:06.704506 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0165481 (* 1 = 0.0165481 loss)
I1106 16:26:06.704514 31365 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I1106 16:26:58.876880 31365 solver.cpp:229] Iteration 3100, loss = 0.351148
I1106 16:26:58.876955 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:26:58.876969 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.204427 (* 1 = 0.204427 loss)
I1106 16:26:58.876976 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0767108 (* 1 = 0.0767108 loss)
I1106 16:26:58.876981 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00956346 (* 1 = 0.00956346 loss)
I1106 16:26:58.876986 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0627795 (* 1 = 0.0627795 loss)
I1106 16:26:58.876992 31365 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I1106 16:27:51.019165 31365 solver.cpp:229] Iteration 3120, loss = 0.39603
I1106 16:27:51.019237 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:27:51.019250 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.382432 (* 1 = 0.382432 loss)
I1106 16:27:51.019258 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0948345 (* 1 = 0.0948345 loss)
I1106 16:27:51.019263 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0217874 (* 1 = 0.0217874 loss)
I1106 16:27:51.019268 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0114817 (* 1 = 0.0114817 loss)
I1106 16:27:51.019275 31365 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I1106 16:28:43.147184 31365 solver.cpp:229] Iteration 3140, loss = 0.320498
I1106 16:28:43.147253 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:28:43.147264 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.198336 (* 1 = 0.198336 loss)
I1106 16:28:43.147270 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0782401 (* 1 = 0.0782401 loss)
I1106 16:28:43.147275 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0080351 (* 1 = 0.0080351 loss)
I1106 16:28:43.147280 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0105958 (* 1 = 0.0105958 loss)
I1106 16:28:43.147287 31365 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I1106 16:29:35.271255 31365 solver.cpp:229] Iteration 3160, loss = 0.451207
I1106 16:29:35.271325 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:29:35.271337 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.295741 (* 1 = 0.295741 loss)
I1106 16:29:35.271343 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.112452 (* 1 = 0.112452 loss)
I1106 16:29:35.271348 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0215548 (* 1 = 0.0215548 loss)
I1106 16:29:35.271353 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00910917 (* 1 = 0.00910917 loss)
I1106 16:29:35.271359 31365 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I1106 16:30:27.425050 31365 solver.cpp:229] Iteration 3180, loss = 0.535964
I1106 16:30:27.425122 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:30:27.425134 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.240705 (* 1 = 0.240705 loss)
I1106 16:30:27.425143 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.119995 (* 1 = 0.119995 loss)
I1106 16:30:27.425148 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0304668 (* 1 = 0.0304668 loss)
I1106 16:30:27.425153 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0384255 (* 1 = 0.0384255 loss)
I1106 16:30:27.425159 31365 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I1106 16:31:19.645025 31365 solver.cpp:229] Iteration 3200, loss = 0.598731
I1106 16:31:19.645097 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:31:19.645109 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.35259 (* 1 = 0.35259 loss)
I1106 16:31:19.645117 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.120501 (* 1 = 0.120501 loss)
I1106 16:31:19.645123 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0235167 (* 1 = 0.0235167 loss)
I1106 16:31:19.645128 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0215873 (* 1 = 0.0215873 loss)
I1106 16:31:19.645133 31365 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I1106 16:32:11.798002 31365 solver.cpp:229] Iteration 3220, loss = 0.41435
I1106 16:32:11.798074 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:32:11.798102 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.214758 (* 1 = 0.214758 loss)
I1106 16:32:11.798108 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116356 (* 1 = 0.116356 loss)
I1106 16:32:11.798113 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0555346 (* 1 = 0.0555346 loss)
I1106 16:32:11.798118 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.100788 (* 1 = 0.100788 loss)
I1106 16:32:11.798126 31365 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I1106 16:33:03.946187 31365 solver.cpp:229] Iteration 3240, loss = 0.412556
I1106 16:33:03.946260 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:33:03.946274 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.199925 (* 1 = 0.199925 loss)
I1106 16:33:03.946280 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.153302 (* 1 = 0.153302 loss)
I1106 16:33:03.946285 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0304894 (* 1 = 0.0304894 loss)
I1106 16:33:03.946290 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0337479 (* 1 = 0.0337479 loss)
I1106 16:33:03.946298 31365 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I1106 16:33:56.122146 31365 solver.cpp:229] Iteration 3260, loss = 0.443839
I1106 16:33:56.122220 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:33:56.122231 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.222776 (* 1 = 0.222776 loss)
I1106 16:33:56.122241 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.128144 (* 1 = 0.128144 loss)
I1106 16:33:56.122246 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00133 (* 1 = 0.00133 loss)
I1106 16:33:56.122251 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00816791 (* 1 = 0.00816791 loss)
I1106 16:33:56.122257 31365 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I1106 16:34:48.298401 31365 solver.cpp:229] Iteration 3280, loss = 0.435016
I1106 16:34:48.298477 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:34:48.298491 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.148899 (* 1 = 0.148899 loss)
I1106 16:34:48.298496 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.194276 (* 1 = 0.194276 loss)
I1106 16:34:48.298501 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00380975 (* 1 = 0.00380975 loss)
I1106 16:34:48.298506 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00424072 (* 1 = 0.00424072 loss)
I1106 16:34:48.298513 31365 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I1106 16:35:40.512331 31365 solver.cpp:229] Iteration 3300, loss = 0.520716
I1106 16:35:40.512398 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:35:40.512410 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.308551 (* 1 = 0.308551 loss)
I1106 16:35:40.512416 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.156111 (* 1 = 0.156111 loss)
I1106 16:35:40.512421 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0567783 (* 1 = 0.0567783 loss)
I1106 16:35:40.512426 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0476689 (* 1 = 0.0476689 loss)
I1106 16:35:40.512434 31365 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I1106 16:36:32.582083 31365 solver.cpp:229] Iteration 3320, loss = 0.393436
I1106 16:36:32.582151 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:36:32.582176 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.182227 (* 1 = 0.182227 loss)
I1106 16:36:32.582182 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.054519 (* 1 = 0.054519 loss)
I1106 16:36:32.582187 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0086591 (* 1 = 0.0086591 loss)
I1106 16:36:32.582192 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0110486 (* 1 = 0.0110486 loss)
I1106 16:36:32.582198 31365 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I1106 16:37:24.649019 31365 solver.cpp:229] Iteration 3340, loss = 0.365017
I1106 16:37:24.649088 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:37:24.649101 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.214588 (* 1 = 0.214588 loss)
I1106 16:37:24.649108 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.179699 (* 1 = 0.179699 loss)
I1106 16:37:24.649114 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00974229 (* 1 = 0.00974229 loss)
I1106 16:37:24.649119 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0342329 (* 1 = 0.0342329 loss)
I1106 16:37:24.649127 31365 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I1106 16:38:16.748483 31365 solver.cpp:229] Iteration 3360, loss = 0.350415
I1106 16:38:16.748550 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:38:16.748562 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.269917 (* 1 = 0.269917 loss)
I1106 16:38:16.748569 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0528353 (* 1 = 0.0528353 loss)
I1106 16:38:16.748574 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00533079 (* 1 = 0.00533079 loss)
I1106 16:38:16.748579 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00862275 (* 1 = 0.00862275 loss)
I1106 16:38:16.748585 31365 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I1106 16:39:08.923440 31365 solver.cpp:229] Iteration 3380, loss = 0.582426
I1106 16:39:08.923526 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:39:08.923542 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.427195 (* 1 = 0.427195 loss)
I1106 16:39:08.923547 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.206229 (* 1 = 0.206229 loss)
I1106 16:39:08.923552 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0306849 (* 1 = 0.0306849 loss)
I1106 16:39:08.923558 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.10133 (* 1 = 0.10133 loss)
I1106 16:39:08.923565 31365 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I1106 16:40:01.073352 31365 solver.cpp:229] Iteration 3400, loss = 0.42348
I1106 16:40:01.073423 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:40:01.073451 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.181662 (* 1 = 0.181662 loss)
I1106 16:40:01.073457 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.253722 (* 1 = 0.253722 loss)
I1106 16:40:01.073462 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0160127 (* 1 = 0.0160127 loss)
I1106 16:40:01.073467 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0273273 (* 1 = 0.0273273 loss)
I1106 16:40:01.073474 31365 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I1106 16:40:53.215380 31365 solver.cpp:229] Iteration 3420, loss = 0.533247
I1106 16:40:53.215451 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:40:53.215463 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.329507 (* 1 = 0.329507 loss)
I1106 16:40:53.215468 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.195226 (* 1 = 0.195226 loss)
I1106 16:40:53.215473 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0318461 (* 1 = 0.0318461 loss)
I1106 16:40:53.215478 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0822196 (* 1 = 0.0822196 loss)
I1106 16:40:53.215486 31365 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I1106 16:41:45.329514 31365 solver.cpp:229] Iteration 3440, loss = 0.42354
I1106 16:41:45.329582 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 16:41:45.329594 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157309 (* 1 = 0.157309 loss)
I1106 16:41:45.329601 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.204694 (* 1 = 0.204694 loss)
I1106 16:41:45.329605 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0105232 (* 1 = 0.0105232 loss)
I1106 16:41:45.329610 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0302136 (* 1 = 0.0302136 loss)
I1106 16:41:45.329617 31365 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I1106 16:42:37.375983 31365 solver.cpp:229] Iteration 3460, loss = 0.269611
I1106 16:42:37.376052 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:42:37.376065 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157448 (* 1 = 0.157448 loss)
I1106 16:42:37.376070 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0586493 (* 1 = 0.0586493 loss)
I1106 16:42:37.376076 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100121 (* 1 = 0.0100121 loss)
I1106 16:42:37.376080 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0228053 (* 1 = 0.0228053 loss)
I1106 16:42:37.376087 31365 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I1106 16:43:29.508401 31365 solver.cpp:229] Iteration 3480, loss = 0.398603
I1106 16:43:29.508473 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:43:29.508486 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.220079 (* 1 = 0.220079 loss)
I1106 16:43:29.508491 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.125373 (* 1 = 0.125373 loss)
I1106 16:43:29.508497 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0141267 (* 1 = 0.0141267 loss)
I1106 16:43:29.508502 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0843119 (* 1 = 0.0843119 loss)
I1106 16:43:29.508508 31365 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I1106 16:44:21.541889 31365 solver.cpp:229] Iteration 3500, loss = 0.391074
I1106 16:44:21.541957 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:44:21.541982 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.169583 (* 1 = 0.169583 loss)
I1106 16:44:21.541987 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118142 (* 1 = 0.118142 loss)
I1106 16:44:21.541992 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00449547 (* 1 = 0.00449547 loss)
I1106 16:44:21.541997 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0243392 (* 1 = 0.0243392 loss)
I1106 16:44:21.542004 31365 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I1106 16:45:13.578706 31365 solver.cpp:229] Iteration 3520, loss = 0.455413
I1106 16:45:13.578775 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:45:13.578788 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.128465 (* 1 = 0.128465 loss)
I1106 16:45:13.578797 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.112668 (* 1 = 0.112668 loss)
I1106 16:45:13.578802 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0189606 (* 1 = 0.0189606 loss)
I1106 16:45:13.578807 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0198726 (* 1 = 0.0198726 loss)
I1106 16:45:13.578814 31365 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I1106 16:46:05.657121 31365 solver.cpp:229] Iteration 3540, loss = 0.380375
I1106 16:46:05.657191 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:46:05.657203 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.188901 (* 1 = 0.188901 loss)
I1106 16:46:05.657208 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127285 (* 1 = 0.127285 loss)
I1106 16:46:05.657213 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00994923 (* 1 = 0.00994923 loss)
I1106 16:46:05.657218 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.055967 (* 1 = 0.055967 loss)
I1106 16:46:05.657225 31365 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I1106 16:46:57.698120 31365 solver.cpp:229] Iteration 3560, loss = 0.337167
I1106 16:46:57.698189 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:46:57.698201 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.190239 (* 1 = 0.190239 loss)
I1106 16:46:57.698206 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.154823 (* 1 = 0.154823 loss)
I1106 16:46:57.698212 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0102718 (* 1 = 0.0102718 loss)
I1106 16:46:57.698217 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0079171 (* 1 = 0.0079171 loss)
I1106 16:46:57.698225 31365 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I1106 16:47:49.768126 31365 solver.cpp:229] Iteration 3580, loss = 0.34621
I1106 16:47:49.768196 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:47:49.768208 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.171104 (* 1 = 0.171104 loss)
I1106 16:47:49.768216 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.159251 (* 1 = 0.159251 loss)
I1106 16:47:49.768223 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0624969 (* 1 = 0.0624969 loss)
I1106 16:47:49.768227 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0220742 (* 1 = 0.0220742 loss)
I1106 16:47:49.768234 31365 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I1106 16:48:41.871078 31365 solver.cpp:229] Iteration 3600, loss = 0.582966
I1106 16:48:41.871143 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:48:41.871170 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.298848 (* 1 = 0.298848 loss)
I1106 16:48:41.871178 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.276804 (* 1 = 0.276804 loss)
I1106 16:48:41.871183 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100057 (* 1 = 0.0100057 loss)
I1106 16:48:41.871188 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0711372 (* 1 = 0.0711372 loss)
I1106 16:48:41.871196 31365 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I1106 16:49:33.966500 31365 solver.cpp:229] Iteration 3620, loss = 0.436287
I1106 16:49:33.966570 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:49:33.966583 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.204999 (* 1 = 0.204999 loss)
I1106 16:49:33.966590 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.145675 (* 1 = 0.145675 loss)
I1106 16:49:33.966596 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0112202 (* 1 = 0.0112202 loss)
I1106 16:49:33.966601 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00843088 (* 1 = 0.00843088 loss)
I1106 16:49:33.966609 31365 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I1106 16:50:26.038645 31365 solver.cpp:229] Iteration 3640, loss = 0.351353
I1106 16:50:26.038717 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:50:26.038729 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.160052 (* 1 = 0.160052 loss)
I1106 16:50:26.038738 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0666144 (* 1 = 0.0666144 loss)
I1106 16:50:26.038743 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0118598 (* 1 = 0.0118598 loss)
I1106 16:50:26.038748 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0428037 (* 1 = 0.0428037 loss)
I1106 16:50:26.038753 31365 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I1106 16:51:18.153280 31365 solver.cpp:229] Iteration 3660, loss = 0.251989
I1106 16:51:18.153354 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:51:18.153367 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.162557 (* 1 = 0.162557 loss)
I1106 16:51:18.153373 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.115658 (* 1 = 0.115658 loss)
I1106 16:51:18.153378 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0223137 (* 1 = 0.0223137 loss)
I1106 16:51:18.153383 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0173283 (* 1 = 0.0173283 loss)
I1106 16:51:18.153390 31365 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I1106 16:52:10.358968 31365 solver.cpp:229] Iteration 3680, loss = 0.341285
I1106 16:52:10.359045 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 16:52:10.359060 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.267593 (* 1 = 0.267593 loss)
I1106 16:52:10.359066 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0398717 (* 1 = 0.0398717 loss)
I1106 16:52:10.359071 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0196343 (* 1 = 0.0196343 loss)
I1106 16:52:10.359076 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0353167 (* 1 = 0.0353167 loss)
I1106 16:52:10.359083 31365 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I1106 16:53:02.581691 31365 solver.cpp:229] Iteration 3700, loss = 0.438026
I1106 16:53:02.581764 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 16:53:02.581792 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.21312 (* 1 = 0.21312 loss)
I1106 16:53:02.581799 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.11574 (* 1 = 0.11574 loss)
I1106 16:53:02.581805 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0227436 (* 1 = 0.0227436 loss)
I1106 16:53:02.581810 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0596323 (* 1 = 0.0596323 loss)
I1106 16:53:02.581817 31365 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I1106 16:53:54.807168 31365 solver.cpp:229] Iteration 3720, loss = 0.369863
I1106 16:53:54.807245 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 16:53:54.807258 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.191111 (* 1 = 0.191111 loss)
I1106 16:53:54.807263 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.204115 (* 1 = 0.204115 loss)
I1106 16:53:54.807270 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00801156 (* 1 = 0.00801156 loss)
I1106 16:53:54.807274 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.016584 (* 1 = 0.016584 loss)
I1106 16:53:54.807282 31365 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I1106 16:54:46.981420 31365 solver.cpp:229] Iteration 3740, loss = 0.679959
I1106 16:54:46.981495 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:54:46.981508 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.24217 (* 1 = 0.24217 loss)
I1106 16:54:46.981514 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.117943 (* 1 = 0.117943 loss)
I1106 16:54:46.981520 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0562703 (* 1 = 0.0562703 loss)
I1106 16:54:46.981525 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.118328 (* 1 = 0.118328 loss)
I1106 16:54:46.981533 31365 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I1106 16:55:39.144067 31365 solver.cpp:229] Iteration 3760, loss = 0.406578
I1106 16:55:39.144142 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:55:39.144156 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.19955 (* 1 = 0.19955 loss)
I1106 16:55:39.144161 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.152553 (* 1 = 0.152553 loss)
I1106 16:55:39.144167 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00851216 (* 1 = 0.00851216 loss)
I1106 16:55:39.144172 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.024987 (* 1 = 0.024987 loss)
I1106 16:55:39.144179 31365 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I1106 16:56:31.289125 31365 solver.cpp:229] Iteration 3780, loss = 0.776059
I1106 16:56:31.289197 31365 solver.cpp:245]     Train net output #0: accuracy = 0.8125
I1106 16:56:31.289225 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.557902 (* 1 = 0.557902 loss)
I1106 16:56:31.289232 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.405501 (* 1 = 0.405501 loss)
I1106 16:56:31.289237 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0152102 (* 1 = 0.0152102 loss)
I1106 16:56:31.289242 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0624906 (* 1 = 0.0624906 loss)
I1106 16:56:31.289249 31365 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I1106 16:57:23.451570 31365 solver.cpp:229] Iteration 3800, loss = 0.401138
I1106 16:57:23.451644 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 16:57:23.451658 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.177616 (* 1 = 0.177616 loss)
I1106 16:57:23.451663 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.220183 (* 1 = 0.220183 loss)
I1106 16:57:23.451668 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0146633 (* 1 = 0.0146633 loss)
I1106 16:57:23.451673 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0128346 (* 1 = 0.0128346 loss)
I1106 16:57:23.451680 31365 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I1106 16:58:15.607130 31365 solver.cpp:229] Iteration 3820, loss = 0.341636
I1106 16:58:15.607203 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 16:58:15.607218 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178707 (* 1 = 0.178707 loss)
I1106 16:58:15.607224 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.102039 (* 1 = 0.102039 loss)
I1106 16:58:15.607229 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0191302 (* 1 = 0.0191302 loss)
I1106 16:58:15.607234 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00307453 (* 1 = 0.00307453 loss)
I1106 16:58:15.607242 31365 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I1106 16:59:07.760020 31365 solver.cpp:229] Iteration 3840, loss = 0.449842
I1106 16:59:07.760095 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 16:59:07.760108 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.231203 (* 1 = 0.231203 loss)
I1106 16:59:07.760114 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.144395 (* 1 = 0.144395 loss)
I1106 16:59:07.760119 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0081229 (* 1 = 0.0081229 loss)
I1106 16:59:07.760125 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00806847 (* 1 = 0.00806847 loss)
I1106 16:59:07.760133 31365 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I1106 16:59:59.917718 31365 solver.cpp:229] Iteration 3860, loss = 0.36135
I1106 16:59:59.917793 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 16:59:59.917807 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178714 (* 1 = 0.178714 loss)
I1106 16:59:59.917814 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0888897 (* 1 = 0.0888897 loss)
I1106 16:59:59.917819 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00300714 (* 1 = 0.00300714 loss)
I1106 16:59:59.917824 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0149198 (* 1 = 0.0149198 loss)
I1106 16:59:59.917830 31365 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I1106 17:00:52.066707 31365 solver.cpp:229] Iteration 3880, loss = 0.391204
I1106 17:00:52.066792 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:00:52.066818 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.267447 (* 1 = 0.267447 loss)
I1106 17:00:52.066824 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.126767 (* 1 = 0.126767 loss)
I1106 17:00:52.066829 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0254852 (* 1 = 0.0254852 loss)
I1106 17:00:52.066834 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0141088 (* 1 = 0.0141088 loss)
I1106 17:00:52.066841 31365 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I1106 17:01:44.229871 31365 solver.cpp:229] Iteration 3900, loss = 0.436867
I1106 17:01:44.229945 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:01:44.229959 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.217123 (* 1 = 0.217123 loss)
I1106 17:01:44.229966 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.105495 (* 1 = 0.105495 loss)
I1106 17:01:44.229971 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0152372 (* 1 = 0.0152372 loss)
I1106 17:01:44.229977 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0300451 (* 1 = 0.0300451 loss)
I1106 17:01:44.229984 31365 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I1106 17:02:36.399045 31365 solver.cpp:229] Iteration 3920, loss = 0.350604
I1106 17:02:36.399119 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:02:36.399133 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.175507 (* 1 = 0.175507 loss)
I1106 17:02:36.399139 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.213849 (* 1 = 0.213849 loss)
I1106 17:02:36.399144 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00800032 (* 1 = 0.00800032 loss)
I1106 17:02:36.399150 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0438648 (* 1 = 0.0438648 loss)
I1106 17:02:36.399158 31365 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I1106 17:03:28.573926 31365 solver.cpp:229] Iteration 3940, loss = 0.48463
I1106 17:03:28.574002 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:03:28.574015 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.275937 (* 1 = 0.275937 loss)
I1106 17:03:28.574021 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.220855 (* 1 = 0.220855 loss)
I1106 17:03:28.574026 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0388071 (* 1 = 0.0388071 loss)
I1106 17:03:28.574033 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.032427 (* 1 = 0.032427 loss)
I1106 17:03:28.574039 31365 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I1106 17:04:20.740988 31365 solver.cpp:229] Iteration 3960, loss = 0.293656
I1106 17:04:20.741062 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:04:20.741076 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.131171 (* 1 = 0.131171 loss)
I1106 17:04:20.741083 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0975819 (* 1 = 0.0975819 loss)
I1106 17:04:20.741089 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.018483 (* 1 = 0.018483 loss)
I1106 17:04:20.741094 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0238269 (* 1 = 0.0238269 loss)
I1106 17:04:20.741101 31365 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I1106 17:05:12.855962 31365 solver.cpp:229] Iteration 3980, loss = 0.767315
I1106 17:05:12.856036 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:05:12.856065 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.222769 (* 1 = 0.222769 loss)
I1106 17:05:12.856070 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.213065 (* 1 = 0.213065 loss)
I1106 17:05:12.856076 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.305112 (* 1 = 0.305112 loss)
I1106 17:05:12.856081 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0755229 (* 1 = 0.0755229 loss)
I1106 17:05:12.856087 31365 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I1106 17:06:05.024125 31365 solver.cpp:229] Iteration 4000, loss = 0.368062
I1106 17:06:05.024200 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:06:05.024214 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.124293 (* 1 = 0.124293 loss)
I1106 17:06:05.024220 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.123191 (* 1 = 0.123191 loss)
I1106 17:06:05.024225 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.018327 (* 1 = 0.018327 loss)
I1106 17:06:05.024231 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0094842 (* 1 = 0.0094842 loss)
I1106 17:06:05.024238 31365 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I1106 17:06:57.137310 31365 solver.cpp:229] Iteration 4020, loss = 0.426731
I1106 17:06:57.137384 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:06:57.137398 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.201927 (* 1 = 0.201927 loss)
I1106 17:06:57.137404 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.145 (* 1 = 0.145 loss)
I1106 17:06:57.137409 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.024123 (* 1 = 0.024123 loss)
I1106 17:06:57.137414 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0644843 (* 1 = 0.0644843 loss)
I1106 17:06:57.137421 31365 sgd_solver.cpp:106] Iteration 4020, lr = 0.001
I1106 17:07:49.276301 31365 solver.cpp:229] Iteration 4040, loss = 0.478171
I1106 17:07:49.276377 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:07:49.276391 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.243477 (* 1 = 0.243477 loss)
I1106 17:07:49.276398 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.117978 (* 1 = 0.117978 loss)
I1106 17:07:49.276404 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0369657 (* 1 = 0.0369657 loss)
I1106 17:07:49.276409 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.111818 (* 1 = 0.111818 loss)
I1106 17:07:49.276417 31365 sgd_solver.cpp:106] Iteration 4040, lr = 0.001
I1106 17:08:41.410091 31365 solver.cpp:229] Iteration 4060, loss = 0.450427
I1106 17:08:41.410163 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:08:41.410189 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.337441 (* 1 = 0.337441 loss)
I1106 17:08:41.410198 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.119765 (* 1 = 0.119765 loss)
I1106 17:08:41.410203 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0390082 (* 1 = 0.0390082 loss)
I1106 17:08:41.410208 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0157881 (* 1 = 0.0157881 loss)
I1106 17:08:41.410215 31365 sgd_solver.cpp:106] Iteration 4060, lr = 0.001
I1106 17:09:33.609145 31365 solver.cpp:229] Iteration 4080, loss = 1.00865
I1106 17:09:33.609220 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:09:33.609235 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.214288 (* 1 = 0.214288 loss)
I1106 17:09:33.609241 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.236393 (* 1 = 0.236393 loss)
I1106 17:09:33.609246 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.53798 (* 1 = 0.53798 loss)
I1106 17:09:33.609251 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.21939 (* 1 = 0.21939 loss)
I1106 17:09:33.609257 31365 sgd_solver.cpp:106] Iteration 4080, lr = 0.001
I1106 17:10:25.736064 31365 solver.cpp:229] Iteration 4100, loss = 0.468437
I1106 17:10:25.736140 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:10:25.736153 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.394849 (* 1 = 0.394849 loss)
I1106 17:10:25.736160 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.122369 (* 1 = 0.122369 loss)
I1106 17:10:25.736166 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0074443 (* 1 = 0.0074443 loss)
I1106 17:10:25.736171 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00977973 (* 1 = 0.00977973 loss)
I1106 17:10:25.736178 31365 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I1106 17:11:17.898061 31365 solver.cpp:229] Iteration 4120, loss = 0.278121
I1106 17:11:17.898140 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:11:17.898154 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.232151 (* 1 = 0.232151 loss)
I1106 17:11:17.898160 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127647 (* 1 = 0.127647 loss)
I1106 17:11:17.898165 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0391569 (* 1 = 0.0391569 loss)
I1106 17:11:17.898171 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0137306 (* 1 = 0.0137306 loss)
I1106 17:11:17.898177 31365 sgd_solver.cpp:106] Iteration 4120, lr = 0.001
I1106 17:12:10.074112 31365 solver.cpp:229] Iteration 4140, loss = 0.477322
I1106 17:12:10.074187 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 17:12:10.074201 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.202784 (* 1 = 0.202784 loss)
I1106 17:12:10.074208 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.267303 (* 1 = 0.267303 loss)
I1106 17:12:10.074214 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0290564 (* 1 = 0.0290564 loss)
I1106 17:12:10.074219 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0391719 (* 1 = 0.0391719 loss)
I1106 17:12:10.074226 31365 sgd_solver.cpp:106] Iteration 4140, lr = 0.001
I1106 17:13:02.232882 31365 solver.cpp:229] Iteration 4160, loss = 0.392556
I1106 17:13:02.232947 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:13:02.232971 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.280358 (* 1 = 0.280358 loss)
I1106 17:13:02.232977 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.174872 (* 1 = 0.174872 loss)
I1106 17:13:02.232983 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0108439 (* 1 = 0.0108439 loss)
I1106 17:13:02.232988 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0143107 (* 1 = 0.0143107 loss)
I1106 17:13:02.232995 31365 sgd_solver.cpp:106] Iteration 4160, lr = 0.001
I1106 17:13:54.377996 31365 solver.cpp:229] Iteration 4180, loss = 0.457513
I1106 17:13:54.378072 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:13:54.378085 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.253876 (* 1 = 0.253876 loss)
I1106 17:13:54.378092 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.107782 (* 1 = 0.107782 loss)
I1106 17:13:54.378098 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00354293 (* 1 = 0.00354293 loss)
I1106 17:13:54.378103 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.015527 (* 1 = 0.015527 loss)
I1106 17:13:54.378109 31365 sgd_solver.cpp:106] Iteration 4180, lr = 0.001
I1106 17:14:46.511431 31365 solver.cpp:229] Iteration 4200, loss = 0.311629
I1106 17:14:46.511505 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:14:46.511519 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.237593 (* 1 = 0.237593 loss)
I1106 17:14:46.511525 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0689297 (* 1 = 0.0689297 loss)
I1106 17:14:46.511530 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00622149 (* 1 = 0.00622149 loss)
I1106 17:14:46.511535 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00512957 (* 1 = 0.00512957 loss)
I1106 17:14:46.511543 31365 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I1106 17:15:38.704902 31365 solver.cpp:229] Iteration 4220, loss = 0.415537
I1106 17:15:38.704978 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:15:38.704993 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.256289 (* 1 = 0.256289 loss)
I1106 17:15:38.704998 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.15019 (* 1 = 0.15019 loss)
I1106 17:15:38.705003 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0253847 (* 1 = 0.0253847 loss)
I1106 17:15:38.705008 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0161266 (* 1 = 0.0161266 loss)
I1106 17:15:38.705015 31365 sgd_solver.cpp:106] Iteration 4220, lr = 0.001
I1106 17:16:30.883414 31365 solver.cpp:229] Iteration 4240, loss = 0.473631
I1106 17:16:30.883487 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:16:30.883499 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.333096 (* 1 = 0.333096 loss)
I1106 17:16:30.883508 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.135131 (* 1 = 0.135131 loss)
I1106 17:16:30.883513 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0174815 (* 1 = 0.0174815 loss)
I1106 17:16:30.883518 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.054436 (* 1 = 0.054436 loss)
I1106 17:16:30.883525 31365 sgd_solver.cpp:106] Iteration 4240, lr = 0.001
I1106 17:17:23.094204 31365 solver.cpp:229] Iteration 4260, loss = 0.428338
I1106 17:17:23.094279 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:17:23.094290 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.203966 (* 1 = 0.203966 loss)
I1106 17:17:23.094300 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.214782 (* 1 = 0.214782 loss)
I1106 17:17:23.094305 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00974339 (* 1 = 0.00974339 loss)
I1106 17:17:23.094310 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0495823 (* 1 = 0.0495823 loss)
I1106 17:17:23.094317 31365 sgd_solver.cpp:106] Iteration 4260, lr = 0.001
I1106 17:18:15.284631 31365 solver.cpp:229] Iteration 4280, loss = 0.483886
I1106 17:18:15.284708 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:18:15.284723 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.321126 (* 1 = 0.321126 loss)
I1106 17:18:15.284729 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.15127 (* 1 = 0.15127 loss)
I1106 17:18:15.284734 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0179569 (* 1 = 0.0179569 loss)
I1106 17:18:15.284739 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0943297 (* 1 = 0.0943297 loss)
I1106 17:18:15.284745 31365 sgd_solver.cpp:106] Iteration 4280, lr = 0.001
I1106 17:19:07.465952 31365 solver.cpp:229] Iteration 4300, loss = 0.572212
I1106 17:19:07.466028 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:19:07.466042 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.320879 (* 1 = 0.320879 loss)
I1106 17:19:07.466050 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.255044 (* 1 = 0.255044 loss)
I1106 17:19:07.466055 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0104525 (* 1 = 0.0104525 loss)
I1106 17:19:07.466060 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0241961 (* 1 = 0.0241961 loss)
I1106 17:19:07.466068 31365 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I1106 17:19:59.639601 31365 solver.cpp:229] Iteration 4320, loss = 0.624209
I1106 17:19:59.639675 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:19:59.639688 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.378117 (* 1 = 0.378117 loss)
I1106 17:19:59.639695 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127362 (* 1 = 0.127362 loss)
I1106 17:19:59.639700 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0179385 (* 1 = 0.0179385 loss)
I1106 17:19:59.639705 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0317878 (* 1 = 0.0317878 loss)
I1106 17:19:59.639713 31365 sgd_solver.cpp:106] Iteration 4320, lr = 0.001
I1106 17:20:51.804258 31365 solver.cpp:229] Iteration 4340, loss = 0.404195
I1106 17:20:51.804332 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 17:20:51.804345 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.284129 (* 1 = 0.284129 loss)
I1106 17:20:51.804353 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0888145 (* 1 = 0.0888145 loss)
I1106 17:20:51.804358 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0416357 (* 1 = 0.0416357 loss)
I1106 17:20:51.804364 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0151687 (* 1 = 0.0151687 loss)
I1106 17:20:51.804371 31365 sgd_solver.cpp:106] Iteration 4340, lr = 0.001
I1106 17:21:43.963759 31365 solver.cpp:229] Iteration 4360, loss = 0.420239
I1106 17:21:43.963832 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:21:43.963845 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.202776 (* 1 = 0.202776 loss)
I1106 17:21:43.963853 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.158575 (* 1 = 0.158575 loss)
I1106 17:21:43.963858 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00715714 (* 1 = 0.00715714 loss)
I1106 17:21:43.963863 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0251171 (* 1 = 0.0251171 loss)
I1106 17:21:43.963871 31365 sgd_solver.cpp:106] Iteration 4360, lr = 0.001
I1106 17:22:36.152552 31365 solver.cpp:229] Iteration 4380, loss = 0.525654
I1106 17:22:36.152623 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:22:36.152635 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.289179 (* 1 = 0.289179 loss)
I1106 17:22:36.152644 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.159917 (* 1 = 0.159917 loss)
I1106 17:22:36.152649 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.100379 (* 1 = 0.100379 loss)
I1106 17:22:36.152654 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0581577 (* 1 = 0.0581577 loss)
I1106 17:22:36.152660 31365 sgd_solver.cpp:106] Iteration 4380, lr = 0.001
I1106 17:23:28.361451 31365 solver.cpp:229] Iteration 4400, loss = 0.440968
I1106 17:23:28.361524 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:23:28.361537 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.347435 (* 1 = 0.347435 loss)
I1106 17:23:28.361546 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.128218 (* 1 = 0.128218 loss)
I1106 17:23:28.361551 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0374318 (* 1 = 0.0374318 loss)
I1106 17:23:28.361555 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0269436 (* 1 = 0.0269436 loss)
I1106 17:23:28.361562 31365 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I1106 17:24:20.507899 31365 solver.cpp:229] Iteration 4420, loss = 0.513029
I1106 17:24:20.507972 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:24:20.507983 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.315388 (* 1 = 0.315388 loss)
I1106 17:24:20.507992 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.187141 (* 1 = 0.187141 loss)
I1106 17:24:20.507997 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0388324 (* 1 = 0.0388324 loss)
I1106 17:24:20.508002 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0900273 (* 1 = 0.0900273 loss)
I1106 17:24:20.508009 31365 sgd_solver.cpp:106] Iteration 4420, lr = 0.001
I1106 17:25:12.715917 31365 solver.cpp:229] Iteration 4440, loss = 0.379758
I1106 17:25:12.715989 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:25:12.716002 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.264076 (* 1 = 0.264076 loss)
I1106 17:25:12.716011 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.151861 (* 1 = 0.151861 loss)
I1106 17:25:12.716015 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0086163 (* 1 = 0.0086163 loss)
I1106 17:25:12.716020 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0242942 (* 1 = 0.0242942 loss)
I1106 17:25:12.716027 31365 sgd_solver.cpp:106] Iteration 4440, lr = 0.001
I1106 17:26:04.929899 31365 solver.cpp:229] Iteration 4460, loss = 0.558266
I1106 17:26:04.929972 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:26:04.929986 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.283161 (* 1 = 0.283161 loss)
I1106 17:26:04.929994 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.177149 (* 1 = 0.177149 loss)
I1106 17:26:04.929999 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.010517 (* 1 = 0.010517 loss)
I1106 17:26:04.930004 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0212151 (* 1 = 0.0212151 loss)
I1106 17:26:04.930011 31365 sgd_solver.cpp:106] Iteration 4460, lr = 0.001
I1106 17:26:57.139065 31365 solver.cpp:229] Iteration 4480, loss = 0.36406
I1106 17:26:57.139137 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:26:57.139148 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.149949 (* 1 = 0.149949 loss)
I1106 17:26:57.139158 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.174106 (* 1 = 0.174106 loss)
I1106 17:26:57.139163 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0302737 (* 1 = 0.0302737 loss)
I1106 17:26:57.139168 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0180624 (* 1 = 0.0180624 loss)
I1106 17:26:57.139174 31365 sgd_solver.cpp:106] Iteration 4480, lr = 0.001
I1106 17:27:49.334771 31365 solver.cpp:229] Iteration 4500, loss = 0.336895
I1106 17:27:49.334844 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:27:49.334857 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.244887 (* 1 = 0.244887 loss)
I1106 17:27:49.334867 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0551193 (* 1 = 0.0551193 loss)
I1106 17:27:49.334870 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0218749 (* 1 = 0.0218749 loss)
I1106 17:27:49.334877 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0646642 (* 1 = 0.0646642 loss)
I1106 17:27:49.334882 31365 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I1106 17:28:41.528311 31365 solver.cpp:229] Iteration 4520, loss = 0.330219
I1106 17:28:41.528380 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:28:41.528393 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.219793 (* 1 = 0.219793 loss)
I1106 17:28:41.528399 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0737782 (* 1 = 0.0737782 loss)
I1106 17:28:41.528405 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0191305 (* 1 = 0.0191305 loss)
I1106 17:28:41.528409 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0425809 (* 1 = 0.0425809 loss)
I1106 17:28:41.528416 31365 sgd_solver.cpp:106] Iteration 4520, lr = 0.001
I1106 17:29:33.720305 31365 solver.cpp:229] Iteration 4540, loss = 0.367442
I1106 17:29:33.720379 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:29:33.720391 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.286364 (* 1 = 0.286364 loss)
I1106 17:29:33.720401 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0900372 (* 1 = 0.0900372 loss)
I1106 17:29:33.720405 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0489424 (* 1 = 0.0489424 loss)
I1106 17:29:33.720410 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0275028 (* 1 = 0.0275028 loss)
I1106 17:29:33.720417 31365 sgd_solver.cpp:106] Iteration 4540, lr = 0.001
I1106 17:30:25.930738 31365 solver.cpp:229] Iteration 4560, loss = 0.457986
I1106 17:30:25.930812 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:30:25.930824 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.308176 (* 1 = 0.308176 loss)
I1106 17:30:25.930831 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0579179 (* 1 = 0.0579179 loss)
I1106 17:30:25.930836 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0296819 (* 1 = 0.0296819 loss)
I1106 17:30:25.930841 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.128203 (* 1 = 0.128203 loss)
I1106 17:30:25.930848 31365 sgd_solver.cpp:106] Iteration 4560, lr = 0.001
I1106 17:31:18.071938 31365 solver.cpp:229] Iteration 4580, loss = 0.462515
I1106 17:31:18.072013 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:31:18.072026 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.218597 (* 1 = 0.218597 loss)
I1106 17:31:18.072033 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.258234 (* 1 = 0.258234 loss)
I1106 17:31:18.072039 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0327084 (* 1 = 0.0327084 loss)
I1106 17:31:18.072044 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0395355 (* 1 = 0.0395355 loss)
I1106 17:31:18.072052 31365 sgd_solver.cpp:106] Iteration 4580, lr = 0.001
I1106 17:32:10.255720 31365 solver.cpp:229] Iteration 4600, loss = 0.535721
I1106 17:32:10.255794 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:32:10.255806 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.224293 (* 1 = 0.224293 loss)
I1106 17:32:10.255815 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.213084 (* 1 = 0.213084 loss)
I1106 17:32:10.255820 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0554722 (* 1 = 0.0554722 loss)
I1106 17:32:10.255825 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.117986 (* 1 = 0.117986 loss)
I1106 17:32:10.255832 31365 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I1106 17:33:02.419176 31365 solver.cpp:229] Iteration 4620, loss = 0.39664
I1106 17:33:02.419247 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:33:02.419261 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.300452 (* 1 = 0.300452 loss)
I1106 17:33:02.419270 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.177929 (* 1 = 0.177929 loss)
I1106 17:33:02.419275 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00911098 (* 1 = 0.00911098 loss)
I1106 17:33:02.419281 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0238022 (* 1 = 0.0238022 loss)
I1106 17:33:02.419287 31365 sgd_solver.cpp:106] Iteration 4620, lr = 0.001
I1106 17:33:54.573869 31365 solver.cpp:229] Iteration 4640, loss = 0.361182
I1106 17:33:54.573943 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:33:54.573956 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.187739 (* 1 = 0.187739 loss)
I1106 17:33:54.573964 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.163667 (* 1 = 0.163667 loss)
I1106 17:33:54.573971 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00466125 (* 1 = 0.00466125 loss)
I1106 17:33:54.573976 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0122764 (* 1 = 0.0122764 loss)
I1106 17:33:54.573982 31365 sgd_solver.cpp:106] Iteration 4640, lr = 0.001
I1106 17:34:46.764003 31365 solver.cpp:229] Iteration 4660, loss = 0.351769
I1106 17:34:46.764075 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:34:46.764086 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.254523 (* 1 = 0.254523 loss)
I1106 17:34:46.764092 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.121001 (* 1 = 0.121001 loss)
I1106 17:34:46.764097 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100398 (* 1 = 0.0100398 loss)
I1106 17:34:46.764102 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0336127 (* 1 = 0.0336127 loss)
I1106 17:34:46.764109 31365 sgd_solver.cpp:106] Iteration 4660, lr = 0.001
I1106 17:35:38.942174 31365 solver.cpp:229] Iteration 4680, loss = 0.408175
I1106 17:35:38.942247 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:35:38.942260 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.216373 (* 1 = 0.216373 loss)
I1106 17:35:38.942267 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.172586 (* 1 = 0.172586 loss)
I1106 17:35:38.942273 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0127614 (* 1 = 0.0127614 loss)
I1106 17:35:38.942278 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0464152 (* 1 = 0.0464152 loss)
I1106 17:35:38.942284 31365 sgd_solver.cpp:106] Iteration 4680, lr = 0.001
I1106 17:36:31.099818 31365 solver.cpp:229] Iteration 4700, loss = 0.328985
I1106 17:36:31.099889 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:36:31.099902 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.161294 (* 1 = 0.161294 loss)
I1106 17:36:31.099908 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0793884 (* 1 = 0.0793884 loss)
I1106 17:36:31.099913 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00293057 (* 1 = 0.00293057 loss)
I1106 17:36:31.099918 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0225221 (* 1 = 0.0225221 loss)
I1106 17:36:31.099925 31365 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I1106 17:37:23.257994 31365 solver.cpp:229] Iteration 4720, loss = 0.576596
I1106 17:37:23.258069 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 17:37:23.258080 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.47415 (* 1 = 0.47415 loss)
I1106 17:37:23.258086 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.230173 (* 1 = 0.230173 loss)
I1106 17:37:23.258091 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00431208 (* 1 = 0.00431208 loss)
I1106 17:37:23.258096 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.025665 (* 1 = 0.025665 loss)
I1106 17:37:23.258103 31365 sgd_solver.cpp:106] Iteration 4720, lr = 0.001
I1106 17:38:15.447363 31365 solver.cpp:229] Iteration 4740, loss = 0.435496
I1106 17:38:15.447437 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:38:15.447448 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.196397 (* 1 = 0.196397 loss)
I1106 17:38:15.447458 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.209071 (* 1 = 0.209071 loss)
I1106 17:38:15.447463 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00222833 (* 1 = 0.00222833 loss)
I1106 17:38:15.447468 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0057615 (* 1 = 0.0057615 loss)
I1106 17:38:15.447475 31365 sgd_solver.cpp:106] Iteration 4740, lr = 0.001
I1106 17:39:07.629470 31365 solver.cpp:229] Iteration 4760, loss = 0.397641
I1106 17:39:07.629544 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 17:39:07.629557 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.188168 (* 1 = 0.188168 loss)
I1106 17:39:07.629564 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0856769 (* 1 = 0.0856769 loss)
I1106 17:39:07.629568 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00667588 (* 1 = 0.00667588 loss)
I1106 17:39:07.629575 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0101496 (* 1 = 0.0101496 loss)
I1106 17:39:07.629581 31365 sgd_solver.cpp:106] Iteration 4760, lr = 0.001
I1106 17:39:59.780290 31365 solver.cpp:229] Iteration 4780, loss = 0.445723
I1106 17:39:59.780364 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 17:39:59.780378 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.167622 (* 1 = 0.167622 loss)
I1106 17:39:59.780385 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.181859 (* 1 = 0.181859 loss)
I1106 17:39:59.780390 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00758973 (* 1 = 0.00758973 loss)
I1106 17:39:59.780395 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0904023 (* 1 = 0.0904023 loss)
I1106 17:39:59.780402 31365 sgd_solver.cpp:106] Iteration 4780, lr = 0.001
I1106 17:40:51.943142 31365 solver.cpp:229] Iteration 4800, loss = 0.494801
I1106 17:40:51.943197 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:40:51.943213 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.229221 (* 1 = 0.229221 loss)
I1106 17:40:51.943222 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.155259 (* 1 = 0.155259 loss)
I1106 17:40:51.943229 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0443636 (* 1 = 0.0443636 loss)
I1106 17:40:51.943238 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0213107 (* 1 = 0.0213107 loss)
I1106 17:40:51.943246 31365 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I1106 17:41:44.041388 31365 solver.cpp:229] Iteration 4820, loss = 0.3782
I1106 17:41:44.041455 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:41:44.041466 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.281115 (* 1 = 0.281115 loss)
I1106 17:41:44.041473 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0866616 (* 1 = 0.0866616 loss)
I1106 17:41:44.041478 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0299755 (* 1 = 0.0299755 loss)
I1106 17:41:44.041483 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0479946 (* 1 = 0.0479946 loss)
I1106 17:41:44.041489 31365 sgd_solver.cpp:106] Iteration 4820, lr = 0.001
I1106 17:42:36.226043 31365 solver.cpp:229] Iteration 4840, loss = 0.420504
I1106 17:42:36.226111 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:42:36.226136 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.212636 (* 1 = 0.212636 loss)
I1106 17:42:36.226145 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.180558 (* 1 = 0.180558 loss)
I1106 17:42:36.226150 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0287171 (* 1 = 0.0287171 loss)
I1106 17:42:36.226155 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0343758 (* 1 = 0.0343758 loss)
I1106 17:42:36.226162 31365 sgd_solver.cpp:106] Iteration 4840, lr = 0.001
I1106 17:43:28.348930 31365 solver.cpp:229] Iteration 4860, loss = 0.44844
I1106 17:43:28.348997 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:43:28.349010 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.272001 (* 1 = 0.272001 loss)
I1106 17:43:28.349019 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.220035 (* 1 = 0.220035 loss)
I1106 17:43:28.349023 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0150007 (* 1 = 0.0150007 loss)
I1106 17:43:28.349028 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0355947 (* 1 = 0.0355947 loss)
I1106 17:43:28.349035 31365 sgd_solver.cpp:106] Iteration 4860, lr = 0.001
I1106 17:44:20.505570 31365 solver.cpp:229] Iteration 4880, loss = 0.41416
I1106 17:44:20.505638 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 17:44:20.505650 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.262114 (* 1 = 0.262114 loss)
I1106 17:44:20.505656 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0477821 (* 1 = 0.0477821 loss)
I1106 17:44:20.505661 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0106972 (* 1 = 0.0106972 loss)
I1106 17:44:20.505666 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00536838 (* 1 = 0.00536838 loss)
I1106 17:44:20.505673 31365 sgd_solver.cpp:106] Iteration 4880, lr = 0.001
I1106 17:45:12.617543 31365 solver.cpp:229] Iteration 4900, loss = 0.49993
I1106 17:45:12.617614 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 17:45:12.617625 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.388445 (* 1 = 0.388445 loss)
I1106 17:45:12.617635 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.224169 (* 1 = 0.224169 loss)
I1106 17:45:12.617640 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0265427 (* 1 = 0.0265427 loss)
I1106 17:45:12.617645 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0178811 (* 1 = 0.0178811 loss)
I1106 17:45:12.617651 31365 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I1106 17:46:04.793087 31365 solver.cpp:229] Iteration 4920, loss = 0.399838
I1106 17:46:04.793161 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 17:46:04.793175 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.236179 (* 1 = 0.236179 loss)
I1106 17:46:04.793182 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.141588 (* 1 = 0.141588 loss)
I1106 17:46:04.793187 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0829543 (* 1 = 0.0829543 loss)
I1106 17:46:04.793193 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0604206 (* 1 = 0.0604206 loss)
I1106 17:46:04.793200 31365 sgd_solver.cpp:106] Iteration 4920, lr = 0.001
I1106 17:46:56.946776 31365 solver.cpp:229] Iteration 4940, loss = 0.535249
I1106 17:46:56.946851 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:46:56.946866 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.374795 (* 1 = 0.374795 loss)
I1106 17:46:56.946871 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.245837 (* 1 = 0.245837 loss)
I1106 17:46:56.946877 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0479789 (* 1 = 0.0479789 loss)
I1106 17:46:56.946882 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0709747 (* 1 = 0.0709747 loss)
I1106 17:46:56.946888 31365 sgd_solver.cpp:106] Iteration 4940, lr = 0.001
I1106 17:47:49.138144 31365 solver.cpp:229] Iteration 4960, loss = 0.271421
I1106 17:47:49.138219 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 17:47:49.138232 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.132203 (* 1 = 0.132203 loss)
I1106 17:47:49.138238 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0831912 (* 1 = 0.0831912 loss)
I1106 17:47:49.138244 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00719362 (* 1 = 0.00719362 loss)
I1106 17:47:49.138249 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0140544 (* 1 = 0.0140544 loss)
I1106 17:47:49.138257 31365 sgd_solver.cpp:106] Iteration 4960, lr = 0.001
I1106 17:48:41.278883 31365 solver.cpp:229] Iteration 4980, loss = 0.382876
I1106 17:48:41.278954 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:48:41.278969 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.211228 (* 1 = 0.211228 loss)
I1106 17:48:41.278975 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.112483 (* 1 = 0.112483 loss)
I1106 17:48:41.278980 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0197022 (* 1 = 0.0197022 loss)
I1106 17:48:41.278985 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0292091 (* 1 = 0.0292091 loss)
I1106 17:48:41.278992 31365 sgd_solver.cpp:106] Iteration 4980, lr = 0.001
I1106 17:49:36.804126 31365 solver.cpp:229] Iteration 5000, loss = 0.377268
I1106 17:49:36.804178 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 17:49:36.804193 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.2704 (* 1 = 0.2704 loss)
I1106 17:49:36.804203 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.223406 (* 1 = 0.223406 loss)
I1106 17:49:36.804210 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0282272 (* 1 = 0.0282272 loss)
I1106 17:49:36.804221 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0474456 (* 1 = 0.0474456 loss)
I1106 17:49:36.804229 31365 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1106 17:50:28.975389 31365 solver.cpp:229] Iteration 5020, loss = 0.326741
I1106 17:50:28.975461 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 17:50:28.975472 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.155627 (* 1 = 0.155627 loss)
I1106 17:50:28.975479 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.147811 (* 1 = 0.147811 loss)
I1106 17:50:28.975484 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.024048 (* 1 = 0.024048 loss)
I1106 17:50:28.975489 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0164039 (* 1 = 0.0164039 loss)
I1106 17:50:28.975497 31365 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I1106 17:51:21.160089 31365 solver.cpp:229] Iteration 5040, loss = 0.46288
I1106 17:51:21.160161 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:51:21.160173 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.147788 (* 1 = 0.147788 loss)
I1106 17:51:21.160181 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.063208 (* 1 = 0.063208 loss)
I1106 17:51:21.160187 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0193085 (* 1 = 0.0193085 loss)
I1106 17:51:21.160192 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0130404 (* 1 = 0.0130404 loss)
I1106 17:51:21.160198 31365 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I1106 17:52:13.343504 31365 solver.cpp:229] Iteration 5060, loss = 0.29766
I1106 17:52:13.343577 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:52:13.343590 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.2013 (* 1 = 0.2013 loss)
I1106 17:52:13.343597 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.117911 (* 1 = 0.117911 loss)
I1106 17:52:13.343603 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00735775 (* 1 = 0.00735775 loss)
I1106 17:52:13.343608 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0248524 (* 1 = 0.0248524 loss)
I1106 17:52:13.343616 31365 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I1106 17:53:05.526914 31365 solver.cpp:229] Iteration 5080, loss = 0.375826
I1106 17:53:05.526988 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:53:05.527000 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.151934 (* 1 = 0.151934 loss)
I1106 17:53:05.527005 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.192424 (* 1 = 0.192424 loss)
I1106 17:53:05.527011 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0218303 (* 1 = 0.0218303 loss)
I1106 17:53:05.527016 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0237293 (* 1 = 0.0237293 loss)
I1106 17:53:05.527024 31365 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I1106 17:53:57.722355 31365 solver.cpp:229] Iteration 5100, loss = 0.319085
I1106 17:53:57.722439 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:53:57.722451 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.136887 (* 1 = 0.136887 loss)
I1106 17:53:57.722458 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.171894 (* 1 = 0.171894 loss)
I1106 17:53:57.722463 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100916 (* 1 = 0.0100916 loss)
I1106 17:53:57.722468 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00817668 (* 1 = 0.00817668 loss)
I1106 17:53:57.722476 31365 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1106 17:54:49.871834 31365 solver.cpp:229] Iteration 5120, loss = 0.494956
I1106 17:54:49.871902 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 17:54:49.871915 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.294205 (* 1 = 0.294205 loss)
I1106 17:54:49.871923 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.386159 (* 1 = 0.386159 loss)
I1106 17:54:49.871929 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0320757 (* 1 = 0.0320757 loss)
I1106 17:54:49.871934 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0245726 (* 1 = 0.0245726 loss)
I1106 17:54:49.871942 31365 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I1106 17:55:42.007289 31365 solver.cpp:229] Iteration 5140, loss = 0.330167
I1106 17:55:42.007355 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 17:55:42.007380 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.201904 (* 1 = 0.201904 loss)
I1106 17:55:42.007390 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0529818 (* 1 = 0.0529818 loss)
I1106 17:55:42.007395 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0142182 (* 1 = 0.0142182 loss)
I1106 17:55:42.007400 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00700831 (* 1 = 0.00700831 loss)
I1106 17:55:42.007406 31365 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I1106 17:56:34.164269 31365 solver.cpp:229] Iteration 5160, loss = 0.363638
I1106 17:56:34.164338 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 17:56:34.164351 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.104339 (* 1 = 0.104339 loss)
I1106 17:56:34.164360 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.109158 (* 1 = 0.109158 loss)
I1106 17:56:34.164364 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.158479 (* 1 = 0.158479 loss)
I1106 17:56:34.164369 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0139788 (* 1 = 0.0139788 loss)
I1106 17:56:34.164376 31365 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I1106 17:57:26.380983 31365 solver.cpp:229] Iteration 5180, loss = 0.384822
I1106 17:57:26.381053 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 17:57:26.381065 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.217033 (* 1 = 0.217033 loss)
I1106 17:57:26.381074 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0961571 (* 1 = 0.0961571 loss)
I1106 17:57:26.381079 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00870227 (* 1 = 0.00870227 loss)
I1106 17:57:26.381084 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0252525 (* 1 = 0.0252525 loss)
I1106 17:57:26.381090 31365 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I1106 17:58:18.504519 31365 solver.cpp:229] Iteration 5200, loss = 0.200227
I1106 17:58:18.504585 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 17:58:18.504598 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.126382 (* 1 = 0.126382 loss)
I1106 17:58:18.504603 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0548234 (* 1 = 0.0548234 loss)
I1106 17:58:18.504608 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0105003 (* 1 = 0.0105003 loss)
I1106 17:58:18.504614 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00844243 (* 1 = 0.00844243 loss)
I1106 17:58:18.504621 31365 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1106 17:59:10.678771 31365 solver.cpp:229] Iteration 5220, loss = 0.345734
I1106 17:59:10.678845 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 17:59:10.678860 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.181743 (* 1 = 0.181743 loss)
I1106 17:59:10.678866 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0986502 (* 1 = 0.0986502 loss)
I1106 17:59:10.678871 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0264892 (* 1 = 0.0264892 loss)
I1106 17:59:10.678876 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0283345 (* 1 = 0.0283345 loss)
I1106 17:59:10.678884 31365 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I1106 18:00:02.822749 31365 solver.cpp:229] Iteration 5240, loss = 0.418638
I1106 18:00:02.822824 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:00:02.822837 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178083 (* 1 = 0.178083 loss)
I1106 18:00:02.822844 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.075969 (* 1 = 0.075969 loss)
I1106 18:00:02.822849 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0263606 (* 1 = 0.0263606 loss)
I1106 18:00:02.822854 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0532444 (* 1 = 0.0532444 loss)
I1106 18:00:02.822860 31365 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I1106 18:00:54.958606 31365 solver.cpp:229] Iteration 5260, loss = 0.345464
I1106 18:00:54.958673 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:00:54.958686 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.228333 (* 1 = 0.228333 loss)
I1106 18:00:54.958693 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.132285 (* 1 = 0.132285 loss)
I1106 18:00:54.958699 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0368781 (* 1 = 0.0368781 loss)
I1106 18:00:54.958704 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0476726 (* 1 = 0.0476726 loss)
I1106 18:00:54.958710 31365 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I1106 18:01:47.092918 31365 solver.cpp:229] Iteration 5280, loss = 0.446337
I1106 18:01:47.092988 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:01:47.093014 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.201573 (* 1 = 0.201573 loss)
I1106 18:01:47.093022 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116292 (* 1 = 0.116292 loss)
I1106 18:01:47.093027 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00629221 (* 1 = 0.00629221 loss)
I1106 18:01:47.093034 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0160603 (* 1 = 0.0160603 loss)
I1106 18:01:47.093039 31365 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I1106 18:02:39.242957 31365 solver.cpp:229] Iteration 5300, loss = 0.492508
I1106 18:02:39.243026 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 18:02:39.243039 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.184221 (* 1 = 0.184221 loss)
I1106 18:02:39.243047 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.178239 (* 1 = 0.178239 loss)
I1106 18:02:39.243052 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0176441 (* 1 = 0.0176441 loss)
I1106 18:02:39.243057 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0198334 (* 1 = 0.0198334 loss)
I1106 18:02:39.243064 31365 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1106 18:03:31.368232 31365 solver.cpp:229] Iteration 5320, loss = 0.388498
I1106 18:03:31.368299 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:03:31.368324 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.17466 (* 1 = 0.17466 loss)
I1106 18:03:31.368330 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0706696 (* 1 = 0.0706696 loss)
I1106 18:03:31.368335 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0262838 (* 1 = 0.0262838 loss)
I1106 18:03:31.368340 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00899206 (* 1 = 0.00899206 loss)
I1106 18:03:31.368347 31365 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I1106 18:04:23.527928 31365 solver.cpp:229] Iteration 5340, loss = 0.326741
I1106 18:04:23.527998 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:04:23.528012 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.180827 (* 1 = 0.180827 loss)
I1106 18:04:23.528017 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.126952 (* 1 = 0.126952 loss)
I1106 18:04:23.528023 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0200787 (* 1 = 0.0200787 loss)
I1106 18:04:23.528026 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0207225 (* 1 = 0.0207225 loss)
I1106 18:04:23.528033 31365 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I1106 18:05:15.734688 31365 solver.cpp:229] Iteration 5360, loss = 0.522656
I1106 18:05:15.734763 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:05:15.734776 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.215969 (* 1 = 0.215969 loss)
I1106 18:05:15.734782 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.13277 (* 1 = 0.13277 loss)
I1106 18:05:15.734787 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0103042 (* 1 = 0.0103042 loss)
I1106 18:05:15.734792 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0325333 (* 1 = 0.0325333 loss)
I1106 18:05:15.734799 31365 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I1106 18:06:07.919289 31365 solver.cpp:229] Iteration 5380, loss = 0.446499
I1106 18:06:07.919361 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 18:06:07.919375 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.386407 (* 1 = 0.386407 loss)
I1106 18:06:07.919380 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.140443 (* 1 = 0.140443 loss)
I1106 18:06:07.919386 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00453194 (* 1 = 0.00453194 loss)
I1106 18:06:07.919391 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00598955 (* 1 = 0.00598955 loss)
I1106 18:06:07.919399 31365 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I1106 18:07:00.093217 31365 solver.cpp:229] Iteration 5400, loss = 0.325843
I1106 18:07:00.093293 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:07:00.093307 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.166526 (* 1 = 0.166526 loss)
I1106 18:07:00.093313 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.140851 (* 1 = 0.140851 loss)
I1106 18:07:00.093319 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0449671 (* 1 = 0.0449671 loss)
I1106 18:07:00.093324 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0053712 (* 1 = 0.0053712 loss)
I1106 18:07:00.093331 31365 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1106 18:07:52.252276 31365 solver.cpp:229] Iteration 5420, loss = 0.471456
I1106 18:07:52.252346 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 18:07:52.252357 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.250743 (* 1 = 0.250743 loss)
I1106 18:07:52.252365 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.23328 (* 1 = 0.23328 loss)
I1106 18:07:52.252372 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0356341 (* 1 = 0.0356341 loss)
I1106 18:07:52.252377 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0209243 (* 1 = 0.0209243 loss)
I1106 18:07:52.252382 31365 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I1106 18:08:44.403360 31365 solver.cpp:229] Iteration 5440, loss = 0.459041
I1106 18:08:44.403429 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:08:44.403442 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.286825 (* 1 = 0.286825 loss)
I1106 18:08:44.403447 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.192602 (* 1 = 0.192602 loss)
I1106 18:08:44.403452 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0576615 (* 1 = 0.0576615 loss)
I1106 18:08:44.403457 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0445026 (* 1 = 0.0445026 loss)
I1106 18:08:44.403465 31365 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I1106 18:09:36.563724 31365 solver.cpp:229] Iteration 5460, loss = 0.529982
I1106 18:09:36.563774 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 18:09:36.563791 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.244183 (* 1 = 0.244183 loss)
I1106 18:09:36.563799 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.155122 (* 1 = 0.155122 loss)
I1106 18:09:36.563807 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.023882 (* 1 = 0.023882 loss)
I1106 18:09:36.563818 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0119934 (* 1 = 0.0119934 loss)
I1106 18:09:36.563827 31365 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I1106 18:10:28.660894 31365 solver.cpp:229] Iteration 5480, loss = 0.35163
I1106 18:10:28.660959 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:10:28.660971 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.162492 (* 1 = 0.162492 loss)
I1106 18:10:28.660976 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.128431 (* 1 = 0.128431 loss)
I1106 18:10:28.660982 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00730622 (* 1 = 0.00730622 loss)
I1106 18:10:28.660987 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0136377 (* 1 = 0.0136377 loss)
I1106 18:10:28.660993 31365 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I1106 18:11:20.758023 31365 solver.cpp:229] Iteration 5500, loss = 0.298717
I1106 18:11:20.758092 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:11:20.758105 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.170373 (* 1 = 0.170373 loss)
I1106 18:11:20.758111 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0872215 (* 1 = 0.0872215 loss)
I1106 18:11:20.758116 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00873943 (* 1 = 0.00873943 loss)
I1106 18:11:20.758121 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0166259 (* 1 = 0.0166259 loss)
I1106 18:11:20.758126 31365 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1106 18:12:12.845314 31365 solver.cpp:229] Iteration 5520, loss = 0.407489
I1106 18:12:12.845381 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 18:12:12.845393 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.189422 (* 1 = 0.189422 loss)
I1106 18:12:12.845399 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.290248 (* 1 = 0.290248 loss)
I1106 18:12:12.845404 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0174733 (* 1 = 0.0174733 loss)
I1106 18:12:12.845409 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0269814 (* 1 = 0.0269814 loss)
I1106 18:12:12.845415 31365 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I1106 18:13:05.043354 31365 solver.cpp:229] Iteration 5540, loss = 0.315599
I1106 18:13:05.043427 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 18:13:05.043439 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.148122 (* 1 = 0.148122 loss)
I1106 18:13:05.043447 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0603638 (* 1 = 0.0603638 loss)
I1106 18:13:05.043452 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00683102 (* 1 = 0.00683102 loss)
I1106 18:13:05.043457 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.037195 (* 1 = 0.037195 loss)
I1106 18:13:05.043464 31365 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I1106 18:13:57.213889 31365 solver.cpp:229] Iteration 5560, loss = 0.372164
I1106 18:13:57.213958 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:13:57.213970 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.149462 (* 1 = 0.149462 loss)
I1106 18:13:57.213976 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0787635 (* 1 = 0.0787635 loss)
I1106 18:13:57.213981 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00349017 (* 1 = 0.00349017 loss)
I1106 18:13:57.213986 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0385493 (* 1 = 0.0385493 loss)
I1106 18:13:57.213992 31365 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I1106 18:14:49.318423 31365 solver.cpp:229] Iteration 5580, loss = 0.428987
I1106 18:14:49.318493 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:14:49.318506 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.197175 (* 1 = 0.197175 loss)
I1106 18:14:49.318513 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.190547 (* 1 = 0.190547 loss)
I1106 18:14:49.318518 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0183795 (* 1 = 0.0183795 loss)
I1106 18:14:49.318523 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0399166 (* 1 = 0.0399166 loss)
I1106 18:14:49.318531 31365 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I1106 18:15:41.408535 31365 solver.cpp:229] Iteration 5600, loss = 0.634812
I1106 18:15:41.408601 31365 solver.cpp:245]     Train net output #0: accuracy = 0.875
I1106 18:15:41.408614 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.41412 (* 1 = 0.41412 loss)
I1106 18:15:41.408619 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.205299 (* 1 = 0.205299 loss)
I1106 18:15:41.408624 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.014917 (* 1 = 0.014917 loss)
I1106 18:15:41.408629 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0211841 (* 1 = 0.0211841 loss)
I1106 18:15:41.408637 31365 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1106 18:16:33.526818 31365 solver.cpp:229] Iteration 5620, loss = 0.398543
I1106 18:16:33.526887 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:16:33.526899 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.206561 (* 1 = 0.206561 loss)
I1106 18:16:33.526904 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.161396 (* 1 = 0.161396 loss)
I1106 18:16:33.526911 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0112287 (* 1 = 0.0112287 loss)
I1106 18:16:33.526916 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.086196 (* 1 = 0.086196 loss)
I1106 18:16:33.526921 31365 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
I1106 18:17:25.635506 31365 solver.cpp:229] Iteration 5640, loss = 0.492261
I1106 18:17:25.635576 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:17:25.635589 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.324278 (* 1 = 0.324278 loss)
I1106 18:17:25.635598 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.114527 (* 1 = 0.114527 loss)
I1106 18:17:25.635603 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0873479 (* 1 = 0.0873479 loss)
I1106 18:17:25.635608 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0495339 (* 1 = 0.0495339 loss)
I1106 18:17:25.635614 31365 sgd_solver.cpp:106] Iteration 5640, lr = 0.001
I1106 18:18:17.777384 31365 solver.cpp:229] Iteration 5660, loss = 0.657056
I1106 18:18:17.777453 31365 solver.cpp:245]     Train net output #0: accuracy = 0.859375
I1106 18:18:17.777465 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.508183 (* 1 = 0.508183 loss)
I1106 18:18:17.777473 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.418611 (* 1 = 0.418611 loss)
I1106 18:18:17.777478 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0451211 (* 1 = 0.0451211 loss)
I1106 18:18:17.777483 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0145463 (* 1 = 0.0145463 loss)
I1106 18:18:17.777490 31365 sgd_solver.cpp:106] Iteration 5660, lr = 0.001
I1106 18:19:09.938297 31365 solver.cpp:229] Iteration 5680, loss = 0.431038
I1106 18:19:09.938369 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:19:09.938390 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.264388 (* 1 = 0.264388 loss)
I1106 18:19:09.938410 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.178702 (* 1 = 0.178702 loss)
I1106 18:19:09.938422 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0172962 (* 1 = 0.0172962 loss)
I1106 18:19:09.938427 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0495983 (* 1 = 0.0495983 loss)
I1106 18:19:09.938434 31365 sgd_solver.cpp:106] Iteration 5680, lr = 0.001
I1106 18:20:02.160624 31365 solver.cpp:229] Iteration 5700, loss = 0.409149
I1106 18:20:02.160697 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:20:02.160709 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.24389 (* 1 = 0.24389 loss)
I1106 18:20:02.160718 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0759053 (* 1 = 0.0759053 loss)
I1106 18:20:02.160723 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0216424 (* 1 = 0.0216424 loss)
I1106 18:20:02.160728 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0490042 (* 1 = 0.0490042 loss)
I1106 18:20:02.160734 31365 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1106 18:20:54.274715 31365 solver.cpp:229] Iteration 5720, loss = 0.404555
I1106 18:20:54.274783 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:20:54.274796 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.1995 (* 1 = 0.1995 loss)
I1106 18:20:54.274802 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0827922 (* 1 = 0.0827922 loss)
I1106 18:20:54.274807 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0119017 (* 1 = 0.0119017 loss)
I1106 18:20:54.274812 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0197626 (* 1 = 0.0197626 loss)
I1106 18:20:54.274819 31365 sgd_solver.cpp:106] Iteration 5720, lr = 0.001
I1106 18:21:46.373390 31365 solver.cpp:229] Iteration 5740, loss = 0.467622
I1106 18:21:46.373458 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 18:21:46.373471 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.225785 (* 1 = 0.225785 loss)
I1106 18:21:46.373476 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.255871 (* 1 = 0.255871 loss)
I1106 18:21:46.373481 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.030977 (* 1 = 0.030977 loss)
I1106 18:21:46.373486 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.074684 (* 1 = 0.074684 loss)
I1106 18:21:46.373492 31365 sgd_solver.cpp:106] Iteration 5740, lr = 0.001
I1106 18:22:38.477938 31365 solver.cpp:229] Iteration 5760, loss = 0.39131
I1106 18:22:38.478006 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 18:22:38.478018 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.250501 (* 1 = 0.250501 loss)
I1106 18:22:38.478024 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0462242 (* 1 = 0.0462242 loss)
I1106 18:22:38.478029 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0113768 (* 1 = 0.0113768 loss)
I1106 18:22:38.478034 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0433164 (* 1 = 0.0433164 loss)
I1106 18:22:38.478041 31365 sgd_solver.cpp:106] Iteration 5760, lr = 0.001
I1106 18:23:30.598001 31365 solver.cpp:229] Iteration 5780, loss = 0.285729
I1106 18:23:30.598070 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:23:30.598083 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.154536 (* 1 = 0.154536 loss)
I1106 18:23:30.598091 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.138672 (* 1 = 0.138672 loss)
I1106 18:23:30.598096 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00637729 (* 1 = 0.00637729 loss)
I1106 18:23:30.598101 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0265616 (* 1 = 0.0265616 loss)
I1106 18:23:30.598109 31365 sgd_solver.cpp:106] Iteration 5780, lr = 0.001
I1106 18:24:22.667732 31365 solver.cpp:229] Iteration 5800, loss = 0.421313
I1106 18:24:22.667800 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:24:22.667812 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.206629 (* 1 = 0.206629 loss)
I1106 18:24:22.667820 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.158656 (* 1 = 0.158656 loss)
I1106 18:24:22.667826 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0140306 (* 1 = 0.0140306 loss)
I1106 18:24:22.667831 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0631047 (* 1 = 0.0631047 loss)
I1106 18:24:22.667837 31365 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1106 18:25:14.823868 31365 solver.cpp:229] Iteration 5820, loss = 0.334338
I1106 18:25:14.823943 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:25:14.823957 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.20243 (* 1 = 0.20243 loss)
I1106 18:25:14.823963 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0957082 (* 1 = 0.0957082 loss)
I1106 18:25:14.823968 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0258515 (* 1 = 0.0258515 loss)
I1106 18:25:14.823973 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0480202 (* 1 = 0.0480202 loss)
I1106 18:25:14.823981 31365 sgd_solver.cpp:106] Iteration 5820, lr = 0.001
I1106 18:26:07.013486 31365 solver.cpp:229] Iteration 5840, loss = 0.393039
I1106 18:26:07.013558 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:26:07.013572 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.161031 (* 1 = 0.161031 loss)
I1106 18:26:07.013577 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.158633 (* 1 = 0.158633 loss)
I1106 18:26:07.013583 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00865022 (* 1 = 0.00865022 loss)
I1106 18:26:07.013588 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0185027 (* 1 = 0.0185027 loss)
I1106 18:26:07.013595 31365 sgd_solver.cpp:106] Iteration 5840, lr = 0.001
I1106 18:26:59.183853 31365 solver.cpp:229] Iteration 5860, loss = 0.35142
I1106 18:26:59.183926 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:26:59.183939 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.211058 (* 1 = 0.211058 loss)
I1106 18:26:59.183946 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.158082 (* 1 = 0.158082 loss)
I1106 18:26:59.183951 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0202264 (* 1 = 0.0202264 loss)
I1106 18:26:59.183957 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0647529 (* 1 = 0.0647529 loss)
I1106 18:26:59.183964 31365 sgd_solver.cpp:106] Iteration 5860, lr = 0.001
I1106 18:27:51.347450 31365 solver.cpp:229] Iteration 5880, loss = 0.315715
I1106 18:27:51.347520 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:27:51.347534 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.244595 (* 1 = 0.244595 loss)
I1106 18:27:51.347543 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0903725 (* 1 = 0.0903725 loss)
I1106 18:27:51.347548 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0126435 (* 1 = 0.0126435 loss)
I1106 18:27:51.347553 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0175142 (* 1 = 0.0175142 loss)
I1106 18:27:51.347560 31365 sgd_solver.cpp:106] Iteration 5880, lr = 0.001
I1106 18:28:43.570255 31365 solver.cpp:229] Iteration 5900, loss = 0.381722
I1106 18:28:43.570323 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:28:43.570335 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.33989 (* 1 = 0.33989 loss)
I1106 18:28:43.570341 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0882827 (* 1 = 0.0882827 loss)
I1106 18:28:43.570346 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0231833 (* 1 = 0.0231833 loss)
I1106 18:28:43.570353 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0303743 (* 1 = 0.0303743 loss)
I1106 18:28:43.570358 31365 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1106 18:29:35.721551 31365 solver.cpp:229] Iteration 5920, loss = 0.425959
I1106 18:29:35.721617 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:29:35.721629 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.295885 (* 1 = 0.295885 loss)
I1106 18:29:35.721638 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.117195 (* 1 = 0.117195 loss)
I1106 18:29:35.721643 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0290658 (* 1 = 0.0290658 loss)
I1106 18:29:35.721648 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0227618 (* 1 = 0.0227618 loss)
I1106 18:29:35.721654 31365 sgd_solver.cpp:106] Iteration 5920, lr = 0.001
I1106 18:30:27.854300 31365 solver.cpp:229] Iteration 5940, loss = 0.319441
I1106 18:30:27.854367 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 18:30:27.854387 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.228629 (* 1 = 0.228629 loss)
I1106 18:30:27.854393 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0351838 (* 1 = 0.0351838 loss)
I1106 18:30:27.854398 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0124886 (* 1 = 0.0124886 loss)
I1106 18:30:27.854403 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.015667 (* 1 = 0.015667 loss)
I1106 18:30:27.854410 31365 sgd_solver.cpp:106] Iteration 5940, lr = 0.001
I1106 18:31:19.966136 31365 solver.cpp:229] Iteration 5960, loss = 0.327274
I1106 18:31:19.966204 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:31:19.966217 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.203128 (* 1 = 0.203128 loss)
I1106 18:31:19.966223 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.115019 (* 1 = 0.115019 loss)
I1106 18:31:19.966228 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00554738 (* 1 = 0.00554738 loss)
I1106 18:31:19.966233 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0197083 (* 1 = 0.0197083 loss)
I1106 18:31:19.966238 31365 sgd_solver.cpp:106] Iteration 5960, lr = 0.001
I1106 18:32:12.072439 31365 solver.cpp:229] Iteration 5980, loss = 0.34655
I1106 18:32:12.072507 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:32:12.072520 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.218183 (* 1 = 0.218183 loss)
I1106 18:32:12.072525 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0965604 (* 1 = 0.0965604 loss)
I1106 18:32:12.072530 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00380193 (* 1 = 0.00380193 loss)
I1106 18:32:12.072535 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0134898 (* 1 = 0.0134898 loss)
I1106 18:32:12.072541 31365 sgd_solver.cpp:106] Iteration 5980, lr = 0.001
I1106 18:33:04.207352 31365 solver.cpp:229] Iteration 6000, loss = 0.366263
I1106 18:33:04.207420 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 18:33:04.207433 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.166254 (* 1 = 0.166254 loss)
I1106 18:33:04.207439 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0518668 (* 1 = 0.0518668 loss)
I1106 18:33:04.207444 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0140751 (* 1 = 0.0140751 loss)
I1106 18:33:04.207449 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00944484 (* 1 = 0.00944484 loss)
I1106 18:33:04.207455 31365 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1106 18:33:56.320086 31365 solver.cpp:229] Iteration 6020, loss = 0.352251
I1106 18:33:56.320159 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:33:56.320173 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.188727 (* 1 = 0.188727 loss)
I1106 18:33:56.320180 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.115152 (* 1 = 0.115152 loss)
I1106 18:33:56.320185 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00277126 (* 1 = 0.00277126 loss)
I1106 18:33:56.320190 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0185758 (* 1 = 0.0185758 loss)
I1106 18:33:56.320197 31365 sgd_solver.cpp:106] Iteration 6020, lr = 0.001
I1106 18:34:48.479328 31365 solver.cpp:229] Iteration 6040, loss = 0.395314
I1106 18:34:48.479400 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:34:48.479413 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.153868 (* 1 = 0.153868 loss)
I1106 18:34:48.479421 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116719 (* 1 = 0.116719 loss)
I1106 18:34:48.479426 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.010542 (* 1 = 0.010542 loss)
I1106 18:34:48.479432 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0239011 (* 1 = 0.0239011 loss)
I1106 18:34:48.479439 31365 sgd_solver.cpp:106] Iteration 6040, lr = 0.001
I1106 18:35:40.693647 31365 solver.cpp:229] Iteration 6060, loss = 0.511513
I1106 18:35:40.693717 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:35:40.693728 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.309455 (* 1 = 0.309455 loss)
I1106 18:35:40.693737 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.102688 (* 1 = 0.102688 loss)
I1106 18:35:40.693742 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00531383 (* 1 = 0.00531383 loss)
I1106 18:35:40.693747 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0261908 (* 1 = 0.0261908 loss)
I1106 18:35:40.693753 31365 sgd_solver.cpp:106] Iteration 6060, lr = 0.001
I1106 18:36:32.853577 31365 solver.cpp:229] Iteration 6080, loss = 0.398577
I1106 18:36:32.853652 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 18:36:32.853665 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.16937 (* 1 = 0.16937 loss)
I1106 18:36:32.853672 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.265177 (* 1 = 0.265177 loss)
I1106 18:36:32.853677 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0168575 (* 1 = 0.0168575 loss)
I1106 18:36:32.853682 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0367586 (* 1 = 0.0367586 loss)
I1106 18:36:32.853689 31365 sgd_solver.cpp:106] Iteration 6080, lr = 0.001
I1106 18:37:25.001097 31365 solver.cpp:229] Iteration 6100, loss = 0.331126
I1106 18:37:25.001166 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:37:25.001178 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.225044 (* 1 = 0.225044 loss)
I1106 18:37:25.001188 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0976287 (* 1 = 0.0976287 loss)
I1106 18:37:25.001193 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00764759 (* 1 = 0.00764759 loss)
I1106 18:37:25.001197 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0231832 (* 1 = 0.0231832 loss)
I1106 18:37:25.001204 31365 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1106 18:38:17.115213 31365 solver.cpp:229] Iteration 6120, loss = 0.371617
I1106 18:38:17.115283 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 18:38:17.115295 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.218587 (* 1 = 0.218587 loss)
I1106 18:38:17.115301 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.175425 (* 1 = 0.175425 loss)
I1106 18:38:17.115306 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0163204 (* 1 = 0.0163204 loss)
I1106 18:38:17.115311 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0367599 (* 1 = 0.0367599 loss)
I1106 18:38:17.115319 31365 sgd_solver.cpp:106] Iteration 6120, lr = 0.001
I1106 18:39:09.249083 31365 solver.cpp:229] Iteration 6140, loss = 0.332122
I1106 18:39:09.249153 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:39:09.249166 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.192038 (* 1 = 0.192038 loss)
I1106 18:39:09.249171 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.097636 (* 1 = 0.097636 loss)
I1106 18:39:09.249176 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00525189 (* 1 = 0.00525189 loss)
I1106 18:39:09.249181 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.019682 (* 1 = 0.019682 loss)
I1106 18:39:09.249187 31365 sgd_solver.cpp:106] Iteration 6140, lr = 0.001
I1106 18:40:01.389513 31365 solver.cpp:229] Iteration 6160, loss = 0.323946
I1106 18:40:01.389582 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:40:01.389595 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.194356 (* 1 = 0.194356 loss)
I1106 18:40:01.389603 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0902829 (* 1 = 0.0902829 loss)
I1106 18:40:01.389608 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0288704 (* 1 = 0.0288704 loss)
I1106 18:40:01.389613 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0783962 (* 1 = 0.0783962 loss)
I1106 18:40:01.389621 31365 sgd_solver.cpp:106] Iteration 6160, lr = 0.001
I1106 18:40:53.567174 31365 solver.cpp:229] Iteration 6180, loss = 0.377054
I1106 18:40:53.567247 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 18:40:53.567260 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.162369 (* 1 = 0.162369 loss)
I1106 18:40:53.567265 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0378508 (* 1 = 0.0378508 loss)
I1106 18:40:53.567271 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0092562 (* 1 = 0.0092562 loss)
I1106 18:40:53.567276 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00868015 (* 1 = 0.00868015 loss)
I1106 18:40:53.567282 31365 sgd_solver.cpp:106] Iteration 6180, lr = 0.001
I1106 18:41:45.750824 31365 solver.cpp:229] Iteration 6200, loss = 0.335125
I1106 18:41:45.750897 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:41:45.750910 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.275124 (* 1 = 0.275124 loss)
I1106 18:41:45.750917 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.106413 (* 1 = 0.106413 loss)
I1106 18:41:45.750922 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00450322 (* 1 = 0.00450322 loss)
I1106 18:41:45.750928 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0231585 (* 1 = 0.0231585 loss)
I1106 18:41:45.750936 31365 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1106 18:42:37.933459 31365 solver.cpp:229] Iteration 6220, loss = 0.285342
I1106 18:42:37.933531 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:42:37.933542 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.180861 (* 1 = 0.180861 loss)
I1106 18:42:37.933548 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.115198 (* 1 = 0.115198 loss)
I1106 18:42:37.933553 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00878264 (* 1 = 0.00878264 loss)
I1106 18:42:37.933558 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0140636 (* 1 = 0.0140636 loss)
I1106 18:42:37.933565 31365 sgd_solver.cpp:106] Iteration 6220, lr = 0.001
I1106 18:43:30.070556 31365 solver.cpp:229] Iteration 6240, loss = 0.285791
I1106 18:43:30.070628 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:43:30.070641 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.165429 (* 1 = 0.165429 loss)
I1106 18:43:30.070649 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0822265 (* 1 = 0.0822265 loss)
I1106 18:43:30.070654 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0035569 (* 1 = 0.0035569 loss)
I1106 18:43:30.070659 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00929289 (* 1 = 0.00929289 loss)
I1106 18:43:30.070665 31365 sgd_solver.cpp:106] Iteration 6240, lr = 0.001
I1106 18:44:22.224336 31365 solver.cpp:229] Iteration 6260, loss = 0.363241
I1106 18:44:22.224409 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:44:22.224423 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.148769 (* 1 = 0.148769 loss)
I1106 18:44:22.224431 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.159952 (* 1 = 0.159952 loss)
I1106 18:44:22.224436 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00366243 (* 1 = 0.00366243 loss)
I1106 18:44:22.224442 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0177335 (* 1 = 0.0177335 loss)
I1106 18:44:22.224448 31365 sgd_solver.cpp:106] Iteration 6260, lr = 0.001
I1106 18:45:14.405937 31365 solver.cpp:229] Iteration 6280, loss = 0.409277
I1106 18:45:14.406013 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:45:14.406028 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.165946 (* 1 = 0.165946 loss)
I1106 18:45:14.406034 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.107997 (* 1 = 0.107997 loss)
I1106 18:45:14.406039 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00682108 (* 1 = 0.00682108 loss)
I1106 18:45:14.406045 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0216104 (* 1 = 0.0216104 loss)
I1106 18:45:14.406054 31365 sgd_solver.cpp:106] Iteration 6280, lr = 0.001
I1106 18:46:06.575721 31365 solver.cpp:229] Iteration 6300, loss = 0.346113
I1106 18:46:06.575795 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:46:06.575809 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.183622 (* 1 = 0.183622 loss)
I1106 18:46:06.575814 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.142365 (* 1 = 0.142365 loss)
I1106 18:46:06.575820 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00700394 (* 1 = 0.00700394 loss)
I1106 18:46:06.575825 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0297812 (* 1 = 0.0297812 loss)
I1106 18:46:06.575832 31365 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1106 18:46:58.722461 31365 solver.cpp:229] Iteration 6320, loss = 0.49758
I1106 18:46:58.722532 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 18:46:58.722544 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.211057 (* 1 = 0.211057 loss)
I1106 18:46:58.722553 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.203619 (* 1 = 0.203619 loss)
I1106 18:46:58.722558 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.047112 (* 1 = 0.047112 loss)
I1106 18:46:58.722564 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0916099 (* 1 = 0.0916099 loss)
I1106 18:46:58.722570 31365 sgd_solver.cpp:106] Iteration 6320, lr = 0.001
I1106 18:47:50.858392 31365 solver.cpp:229] Iteration 6340, loss = 0.331104
I1106 18:47:50.858461 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:47:50.858474 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157554 (* 1 = 0.157554 loss)
I1106 18:47:50.858482 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0699468 (* 1 = 0.0699468 loss)
I1106 18:47:50.858487 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0130034 (* 1 = 0.0130034 loss)
I1106 18:47:50.858492 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0120355 (* 1 = 0.0120355 loss)
I1106 18:47:50.858500 31365 sgd_solver.cpp:106] Iteration 6340, lr = 0.001
I1106 18:48:43.012403 31365 solver.cpp:229] Iteration 6360, loss = 0.318965
I1106 18:48:43.012472 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:48:43.012498 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.1554 (* 1 = 0.1554 loss)
I1106 18:48:43.012506 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118712 (* 1 = 0.118712 loss)
I1106 18:48:43.012511 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0115853 (* 1 = 0.0115853 loss)
I1106 18:48:43.012516 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.067812 (* 1 = 0.067812 loss)
I1106 18:48:43.012522 31365 sgd_solver.cpp:106] Iteration 6360, lr = 0.001
I1106 18:49:35.138206 31365 solver.cpp:229] Iteration 6380, loss = 0.299591
I1106 18:49:35.138273 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:49:35.138285 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.13036 (* 1 = 0.13036 loss)
I1106 18:49:35.138291 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0856305 (* 1 = 0.0856305 loss)
I1106 18:49:35.138296 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00445794 (* 1 = 0.00445794 loss)
I1106 18:49:35.138301 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00902949 (* 1 = 0.00902949 loss)
I1106 18:49:35.138309 31365 sgd_solver.cpp:106] Iteration 6380, lr = 0.001
I1106 18:50:27.293601 31365 solver.cpp:229] Iteration 6400, loss = 0.485838
I1106 18:50:27.293671 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:50:27.293684 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.304461 (* 1 = 0.304461 loss)
I1106 18:50:27.293692 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.167178 (* 1 = 0.167178 loss)
I1106 18:50:27.293699 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0295969 (* 1 = 0.0295969 loss)
I1106 18:50:27.293702 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0309523 (* 1 = 0.0309523 loss)
I1106 18:50:27.293709 31365 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1106 18:51:19.470715 31365 solver.cpp:229] Iteration 6420, loss = 0.400862
I1106 18:51:19.470818 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 18:51:19.470830 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.176154 (* 1 = 0.176154 loss)
I1106 18:51:19.470837 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.282417 (* 1 = 0.282417 loss)
I1106 18:51:19.470844 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0194506 (* 1 = 0.0194506 loss)
I1106 18:51:19.470849 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0187217 (* 1 = 0.0187217 loss)
I1106 18:51:19.470855 31365 sgd_solver.cpp:106] Iteration 6420, lr = 0.001
I1106 18:52:11.593019 31365 solver.cpp:229] Iteration 6440, loss = 0.336127
I1106 18:52:11.593092 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:52:11.593106 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.127601 (* 1 = 0.127601 loss)
I1106 18:52:11.593113 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0806906 (* 1 = 0.0806906 loss)
I1106 18:52:11.593119 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00895843 (* 1 = 0.00895843 loss)
I1106 18:52:11.593124 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00082289 (* 1 = 0.00082289 loss)
I1106 18:52:11.593132 31365 sgd_solver.cpp:106] Iteration 6440, lr = 0.001
I1106 18:53:03.770655 31365 solver.cpp:229] Iteration 6460, loss = 0.360496
I1106 18:53:03.770730 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:53:03.770745 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.15331 (* 1 = 0.15331 loss)
I1106 18:53:03.770753 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0698125 (* 1 = 0.0698125 loss)
I1106 18:53:03.770759 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00244717 (* 1 = 0.00244717 loss)
I1106 18:53:03.770764 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.014659 (* 1 = 0.014659 loss)
I1106 18:53:03.770771 31365 sgd_solver.cpp:106] Iteration 6460, lr = 0.001
I1106 18:53:55.978250 31365 solver.cpp:229] Iteration 6480, loss = 0.364813
I1106 18:53:55.978320 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 18:53:55.978332 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.1267 (* 1 = 0.1267 loss)
I1106 18:53:55.978341 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0861105 (* 1 = 0.0861105 loss)
I1106 18:53:55.978348 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00790995 (* 1 = 0.00790995 loss)
I1106 18:53:55.978353 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0260724 (* 1 = 0.0260724 loss)
I1106 18:53:55.978358 31365 sgd_solver.cpp:106] Iteration 6480, lr = 0.001
I1106 18:54:48.118324 31365 solver.cpp:229] Iteration 6500, loss = 0.401905
I1106 18:54:48.118402 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:54:48.118415 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.23573 (* 1 = 0.23573 loss)
I1106 18:54:48.118422 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0870746 (* 1 = 0.0870746 loss)
I1106 18:54:48.118427 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0234618 (* 1 = 0.0234618 loss)
I1106 18:54:48.118432 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0589207 (* 1 = 0.0589207 loss)
I1106 18:54:48.118450 31365 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1106 18:55:40.268085 31365 solver.cpp:229] Iteration 6520, loss = 0.527369
I1106 18:55:40.268157 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 18:55:40.268168 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.513127 (* 1 = 0.513127 loss)
I1106 18:55:40.268174 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.159331 (* 1 = 0.159331 loss)
I1106 18:55:40.268179 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.020179 (* 1 = 0.020179 loss)
I1106 18:55:40.268184 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0296844 (* 1 = 0.0296844 loss)
I1106 18:55:40.268191 31365 sgd_solver.cpp:106] Iteration 6520, lr = 0.001
I1106 18:56:32.395835 31365 solver.cpp:229] Iteration 6540, loss = 0.395024
I1106 18:56:32.395905 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:56:32.395916 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.201741 (* 1 = 0.201741 loss)
I1106 18:56:32.395923 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.113344 (* 1 = 0.113344 loss)
I1106 18:56:32.395928 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0334852 (* 1 = 0.0334852 loss)
I1106 18:56:32.395933 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0170763 (* 1 = 0.0170763 loss)
I1106 18:56:32.395939 31365 sgd_solver.cpp:106] Iteration 6540, lr = 0.001
I1106 18:57:24.499207 31365 solver.cpp:229] Iteration 6560, loss = 0.333171
I1106 18:57:24.499279 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 18:57:24.499290 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.167159 (* 1 = 0.167159 loss)
I1106 18:57:24.499299 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0763053 (* 1 = 0.0763053 loss)
I1106 18:57:24.499303 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0295807 (* 1 = 0.0295807 loss)
I1106 18:57:24.499308 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0184084 (* 1 = 0.0184084 loss)
I1106 18:57:24.499315 31365 sgd_solver.cpp:106] Iteration 6560, lr = 0.001
I1106 18:58:16.607537 31365 solver.cpp:229] Iteration 6580, loss = 0.353839
I1106 18:58:16.607607 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 18:58:16.607620 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.145485 (* 1 = 0.145485 loss)
I1106 18:58:16.607626 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.203459 (* 1 = 0.203459 loss)
I1106 18:58:16.607631 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0225949 (* 1 = 0.0225949 loss)
I1106 18:58:16.607636 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0147183 (* 1 = 0.0147183 loss)
I1106 18:58:16.607642 31365 sgd_solver.cpp:106] Iteration 6580, lr = 0.001
I1106 18:59:08.736500 31365 solver.cpp:229] Iteration 6600, loss = 0.496414
I1106 18:59:08.736569 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 18:59:08.736582 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.291431 (* 1 = 0.291431 loss)
I1106 18:59:08.736590 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.253559 (* 1 = 0.253559 loss)
I1106 18:59:08.736595 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0459026 (* 1 = 0.0459026 loss)
I1106 18:59:08.736600 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0539633 (* 1 = 0.0539633 loss)
I1106 18:59:08.736606 31365 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1106 19:00:00.907584 31365 solver.cpp:229] Iteration 6620, loss = 0.37366
I1106 19:00:00.907660 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 19:00:00.907673 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157556 (* 1 = 0.157556 loss)
I1106 19:00:00.907680 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.133764 (* 1 = 0.133764 loss)
I1106 19:00:00.907685 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100385 (* 1 = 0.0100385 loss)
I1106 19:00:00.907691 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0399964 (* 1 = 0.0399964 loss)
I1106 19:00:00.907697 31365 sgd_solver.cpp:106] Iteration 6620, lr = 0.001
I1106 19:00:53.053339 31365 solver.cpp:229] Iteration 6640, loss = 0.390559
I1106 19:00:53.053539 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:00:53.053555 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.290244 (* 1 = 0.290244 loss)
I1106 19:00:53.053562 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127763 (* 1 = 0.127763 loss)
I1106 19:00:53.053567 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0130232 (* 1 = 0.0130232 loss)
I1106 19:00:53.053572 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.021763 (* 1 = 0.021763 loss)
I1106 19:00:53.053580 31365 sgd_solver.cpp:106] Iteration 6640, lr = 0.001
I1106 19:01:45.226272 31365 solver.cpp:229] Iteration 6660, loss = 0.401932
I1106 19:01:45.226346 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:01:45.226358 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.279994 (* 1 = 0.279994 loss)
I1106 19:01:45.226366 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0924698 (* 1 = 0.0924698 loss)
I1106 19:01:45.226372 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0222147 (* 1 = 0.0222147 loss)
I1106 19:01:45.226377 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0685282 (* 1 = 0.0685282 loss)
I1106 19:01:45.226393 31365 sgd_solver.cpp:106] Iteration 6660, lr = 0.001
I1106 19:02:37.419967 31365 solver.cpp:229] Iteration 6680, loss = 0.274013
I1106 19:02:37.420042 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:02:37.420054 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.172648 (* 1 = 0.172648 loss)
I1106 19:02:37.420063 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.100026 (* 1 = 0.100026 loss)
I1106 19:02:37.420068 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00420253 (* 1 = 0.00420253 loss)
I1106 19:02:37.420073 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0108979 (* 1 = 0.0108979 loss)
I1106 19:02:37.420079 31365 sgd_solver.cpp:106] Iteration 6680, lr = 0.001
I1106 19:03:29.627676 31365 solver.cpp:229] Iteration 6700, loss = 0.507138
I1106 19:03:29.627749 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:03:29.627763 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.260121 (* 1 = 0.260121 loss)
I1106 19:03:29.627770 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.244969 (* 1 = 0.244969 loss)
I1106 19:03:29.627775 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0104097 (* 1 = 0.0104097 loss)
I1106 19:03:29.627780 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0471969 (* 1 = 0.0471969 loss)
I1106 19:03:29.627787 31365 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1106 19:04:21.848672 31365 solver.cpp:229] Iteration 6720, loss = 0.394997
I1106 19:04:21.848744 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:04:21.848757 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.300877 (* 1 = 0.300877 loss)
I1106 19:04:21.848763 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0676762 (* 1 = 0.0676762 loss)
I1106 19:04:21.848768 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00432473 (* 1 = 0.00432473 loss)
I1106 19:04:21.848773 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0747622 (* 1 = 0.0747622 loss)
I1106 19:04:21.848793 31365 sgd_solver.cpp:106] Iteration 6720, lr = 0.001
I1106 19:05:14.011039 31365 solver.cpp:229] Iteration 6740, loss = 0.276688
I1106 19:05:14.011112 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:05:14.011126 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.154713 (* 1 = 0.154713 loss)
I1106 19:05:14.011132 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0691152 (* 1 = 0.0691152 loss)
I1106 19:05:14.011137 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00588402 (* 1 = 0.00588402 loss)
I1106 19:05:14.011142 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0103385 (* 1 = 0.0103385 loss)
I1106 19:05:14.011149 31365 sgd_solver.cpp:106] Iteration 6740, lr = 0.001
I1106 19:06:06.228834 31365 solver.cpp:229] Iteration 6760, loss = 0.270042
I1106 19:06:06.228901 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:06:06.228915 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.217917 (* 1 = 0.217917 loss)
I1106 19:06:06.228924 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0624605 (* 1 = 0.0624605 loss)
I1106 19:06:06.228929 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00857915 (* 1 = 0.00857915 loss)
I1106 19:06:06.228935 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0300473 (* 1 = 0.0300473 loss)
I1106 19:06:06.228940 31365 sgd_solver.cpp:106] Iteration 6760, lr = 0.001
I1106 19:06:58.393451 31365 solver.cpp:229] Iteration 6780, loss = 0.394338
I1106 19:06:58.393522 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 19:06:58.393534 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.241267 (* 1 = 0.241267 loss)
I1106 19:06:58.393543 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.17831 (* 1 = 0.17831 loss)
I1106 19:06:58.393548 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0299092 (* 1 = 0.0299092 loss)
I1106 19:06:58.393553 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.039974 (* 1 = 0.039974 loss)
I1106 19:06:58.393559 31365 sgd_solver.cpp:106] Iteration 6780, lr = 0.001
I1106 19:07:50.539688 31365 solver.cpp:229] Iteration 6800, loss = 0.351677
I1106 19:07:50.539759 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:07:50.539772 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.252623 (* 1 = 0.252623 loss)
I1106 19:07:50.539777 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.111611 (* 1 = 0.111611 loss)
I1106 19:07:50.539783 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00758531 (* 1 = 0.00758531 loss)
I1106 19:07:50.539788 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0451362 (* 1 = 0.0451362 loss)
I1106 19:07:50.539794 31365 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1106 19:08:42.647392 31365 solver.cpp:229] Iteration 6820, loss = 0.304691
I1106 19:08:42.647460 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:08:42.647472 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.163889 (* 1 = 0.163889 loss)
I1106 19:08:42.647478 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0451825 (* 1 = 0.0451825 loss)
I1106 19:08:42.647483 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0181417 (* 1 = 0.0181417 loss)
I1106 19:08:42.647488 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00422887 (* 1 = 0.00422887 loss)
I1106 19:08:42.647495 31365 sgd_solver.cpp:106] Iteration 6820, lr = 0.001
I1106 19:09:34.781417 31365 solver.cpp:229] Iteration 6840, loss = 0.375973
I1106 19:09:34.781493 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:09:34.781507 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.159906 (* 1 = 0.159906 loss)
I1106 19:09:34.781512 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.114461 (* 1 = 0.114461 loss)
I1106 19:09:34.781517 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0157382 (* 1 = 0.0157382 loss)
I1106 19:09:34.781523 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00968322 (* 1 = 0.00968322 loss)
I1106 19:09:34.781530 31365 sgd_solver.cpp:106] Iteration 6840, lr = 0.001
I1106 19:10:26.931645 31365 solver.cpp:229] Iteration 6860, loss = 0.385223
I1106 19:10:26.931715 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:10:26.931728 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.24695 (* 1 = 0.24695 loss)
I1106 19:10:26.931735 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.103632 (* 1 = 0.103632 loss)
I1106 19:10:26.931741 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00407893 (* 1 = 0.00407893 loss)
I1106 19:10:26.931746 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0128651 (* 1 = 0.0128651 loss)
I1106 19:10:26.931753 31365 sgd_solver.cpp:106] Iteration 6860, lr = 0.001
I1106 19:11:19.153666 31365 solver.cpp:229] Iteration 6880, loss = 0.311079
I1106 19:11:19.153739 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:11:19.153765 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.151406 (* 1 = 0.151406 loss)
I1106 19:11:19.153771 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.131931 (* 1 = 0.131931 loss)
I1106 19:11:19.153777 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0049024 (* 1 = 0.0049024 loss)
I1106 19:11:19.153782 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0166709 (* 1 = 0.0166709 loss)
I1106 19:11:19.153789 31365 sgd_solver.cpp:106] Iteration 6880, lr = 0.001
I1106 19:12:11.310052 31365 solver.cpp:229] Iteration 6900, loss = 0.333549
I1106 19:12:11.310117 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:12:11.310129 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.251592 (* 1 = 0.251592 loss)
I1106 19:12:11.310135 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.123382 (* 1 = 0.123382 loss)
I1106 19:12:11.310140 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0100016 (* 1 = 0.0100016 loss)
I1106 19:12:11.310145 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0274334 (* 1 = 0.0274334 loss)
I1106 19:12:11.310151 31365 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1106 19:13:03.447103 31365 solver.cpp:229] Iteration 6920, loss = 0.339417
I1106 19:13:03.447175 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:13:03.447188 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.168202 (* 1 = 0.168202 loss)
I1106 19:13:03.447197 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0450981 (* 1 = 0.0450981 loss)
I1106 19:13:03.447202 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00763815 (* 1 = 0.00763815 loss)
I1106 19:13:03.447207 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0433918 (* 1 = 0.0433918 loss)
I1106 19:13:03.447214 31365 sgd_solver.cpp:106] Iteration 6920, lr = 0.001
I1106 19:13:55.661154 31365 solver.cpp:229] Iteration 6940, loss = 0.33673
I1106 19:13:55.661231 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:13:55.661243 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.142057 (* 1 = 0.142057 loss)
I1106 19:13:55.661249 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.07291 (* 1 = 0.07291 loss)
I1106 19:13:55.661254 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0374899 (* 1 = 0.0374899 loss)
I1106 19:13:55.661259 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0242917 (* 1 = 0.0242917 loss)
I1106 19:13:55.661267 31365 sgd_solver.cpp:106] Iteration 6940, lr = 0.001
I1106 19:14:47.849210 31365 solver.cpp:229] Iteration 6960, loss = 0.324346
I1106 19:14:47.849280 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:14:47.849293 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.121527 (* 1 = 0.121527 loss)
I1106 19:14:47.849301 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0591368 (* 1 = 0.0591368 loss)
I1106 19:14:47.849308 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0379299 (* 1 = 0.0379299 loss)
I1106 19:14:47.849313 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0200476 (* 1 = 0.0200476 loss)
I1106 19:14:47.849318 31365 sgd_solver.cpp:106] Iteration 6960, lr = 0.001
I1106 19:15:40.048262 31365 solver.cpp:229] Iteration 6980, loss = 0.565613
I1106 19:15:40.048331 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 19:15:40.048343 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.221031 (* 1 = 0.221031 loss)
I1106 19:15:40.048352 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.258896 (* 1 = 0.258896 loss)
I1106 19:15:40.048357 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.033224 (* 1 = 0.033224 loss)
I1106 19:15:40.048362 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0687566 (* 1 = 0.0687566 loss)
I1106 19:15:40.048368 31365 sgd_solver.cpp:106] Iteration 6980, lr = 0.001
I1106 19:16:32.198251 31365 solver.cpp:229] Iteration 7000, loss = 0.339417
I1106 19:16:32.198318 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:16:32.198328 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.263741 (* 1 = 0.263741 loss)
I1106 19:16:32.198334 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0601204 (* 1 = 0.0601204 loss)
I1106 19:16:32.198339 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0312573 (* 1 = 0.0312573 loss)
I1106 19:16:32.198344 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0304444 (* 1 = 0.0304444 loss)
I1106 19:16:32.198350 31365 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1106 19:17:24.339088 31365 solver.cpp:229] Iteration 7020, loss = 0.390245
I1106 19:17:24.339157 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:17:24.339170 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.244309 (* 1 = 0.244309 loss)
I1106 19:17:24.339176 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0797713 (* 1 = 0.0797713 loss)
I1106 19:17:24.339181 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00381128 (* 1 = 0.00381128 loss)
I1106 19:17:24.339186 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0143863 (* 1 = 0.0143863 loss)
I1106 19:17:24.339192 31365 sgd_solver.cpp:106] Iteration 7020, lr = 0.001
I1106 19:18:16.473629 31365 solver.cpp:229] Iteration 7040, loss = 0.268554
I1106 19:18:16.473698 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:18:16.473711 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.0970254 (* 1 = 0.0970254 loss)
I1106 19:18:16.473716 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.13892 (* 1 = 0.13892 loss)
I1106 19:18:16.473721 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0117515 (* 1 = 0.0117515 loss)
I1106 19:18:16.473726 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0113778 (* 1 = 0.0113778 loss)
I1106 19:18:16.473732 31365 sgd_solver.cpp:106] Iteration 7040, lr = 0.001
I1106 19:19:08.626245 31365 solver.cpp:229] Iteration 7060, loss = 0.34354
I1106 19:19:08.626312 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 19:19:08.626325 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.215536 (* 1 = 0.215536 loss)
I1106 19:19:08.626332 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0353942 (* 1 = 0.0353942 loss)
I1106 19:19:08.626336 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00190178 (* 1 = 0.00190178 loss)
I1106 19:19:08.626343 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0108125 (* 1 = 0.0108125 loss)
I1106 19:19:08.626348 31365 sgd_solver.cpp:106] Iteration 7060, lr = 0.001
I1106 19:20:00.771153 31365 solver.cpp:229] Iteration 7080, loss = 0.291706
I1106 19:20:00.771224 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:20:00.771235 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.182725 (* 1 = 0.182725 loss)
I1106 19:20:00.771240 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.104536 (* 1 = 0.104536 loss)
I1106 19:20:00.771246 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00763752 (* 1 = 0.00763752 loss)
I1106 19:20:00.771251 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0374482 (* 1 = 0.0374482 loss)
I1106 19:20:00.771257 31365 sgd_solver.cpp:106] Iteration 7080, lr = 0.001
I1106 19:20:52.915329 31365 solver.cpp:229] Iteration 7100, loss = 0.450982
I1106 19:20:52.915403 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 19:20:52.915417 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.305255 (* 1 = 0.305255 loss)
I1106 19:20:52.915422 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.241644 (* 1 = 0.241644 loss)
I1106 19:20:52.915427 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00327005 (* 1 = 0.00327005 loss)
I1106 19:20:52.915433 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0268652 (* 1 = 0.0268652 loss)
I1106 19:20:52.915441 31365 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1106 19:21:45.068873 31365 solver.cpp:229] Iteration 7120, loss = 0.281782
I1106 19:21:45.068948 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:21:45.068963 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.123605 (* 1 = 0.123605 loss)
I1106 19:21:45.068969 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.084451 (* 1 = 0.084451 loss)
I1106 19:21:45.068974 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0405802 (* 1 = 0.0405802 loss)
I1106 19:21:45.068979 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00361986 (* 1 = 0.00361986 loss)
I1106 19:21:45.068986 31365 sgd_solver.cpp:106] Iteration 7120, lr = 0.001
I1106 19:22:37.181485 31365 solver.cpp:229] Iteration 7140, loss = 0.362739
I1106 19:22:37.181563 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:22:37.181577 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.391466 (* 1 = 0.391466 loss)
I1106 19:22:37.181583 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.123486 (* 1 = 0.123486 loss)
I1106 19:22:37.181588 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.009435 (* 1 = 0.009435 loss)
I1106 19:22:37.181593 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00972234 (* 1 = 0.00972234 loss)
I1106 19:22:37.181601 31365 sgd_solver.cpp:106] Iteration 7140, lr = 0.001
I1106 19:23:29.310925 31365 solver.cpp:229] Iteration 7160, loss = 0.292169
I1106 19:23:29.310999 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:23:29.311013 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.236098 (* 1 = 0.236098 loss)
I1106 19:23:29.311020 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0820458 (* 1 = 0.0820458 loss)
I1106 19:23:29.311025 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0280805 (* 1 = 0.0280805 loss)
I1106 19:23:29.311030 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0195975 (* 1 = 0.0195975 loss)
I1106 19:23:29.311038 31365 sgd_solver.cpp:106] Iteration 7160, lr = 0.001
I1106 19:24:21.433428 31365 solver.cpp:229] Iteration 7180, loss = 0.270266
I1106 19:24:21.433499 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:24:21.433513 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.140615 (* 1 = 0.140615 loss)
I1106 19:24:21.433521 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116829 (* 1 = 0.116829 loss)
I1106 19:24:21.433526 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00155288 (* 1 = 0.00155288 loss)
I1106 19:24:21.433531 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00382819 (* 1 = 0.00382819 loss)
I1106 19:24:21.433538 31365 sgd_solver.cpp:106] Iteration 7180, lr = 0.001
I1106 19:25:13.559317 31365 solver.cpp:229] Iteration 7200, loss = 0.486908
I1106 19:25:13.559392 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:25:13.559406 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.191017 (* 1 = 0.191017 loss)
I1106 19:25:13.559412 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0655371 (* 1 = 0.0655371 loss)
I1106 19:25:13.559417 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00820595 (* 1 = 0.00820595 loss)
I1106 19:25:13.559422 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0093739 (* 1 = 0.0093739 loss)
I1106 19:25:13.559429 31365 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1106 19:26:05.719360 31365 solver.cpp:229] Iteration 7220, loss = 0.264954
I1106 19:26:05.719435 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:26:05.719449 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.136822 (* 1 = 0.136822 loss)
I1106 19:26:05.719455 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0637708 (* 1 = 0.0637708 loss)
I1106 19:26:05.719460 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00534201 (* 1 = 0.00534201 loss)
I1106 19:26:05.719465 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0108684 (* 1 = 0.0108684 loss)
I1106 19:26:05.719472 31365 sgd_solver.cpp:106] Iteration 7220, lr = 0.001
I1106 19:26:57.871033 31365 solver.cpp:229] Iteration 7240, loss = 0.375867
I1106 19:26:57.871106 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:26:57.871119 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.170061 (* 1 = 0.170061 loss)
I1106 19:26:57.871126 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.145823 (* 1 = 0.145823 loss)
I1106 19:26:57.871132 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00717486 (* 1 = 0.00717486 loss)
I1106 19:26:57.871137 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0127842 (* 1 = 0.0127842 loss)
I1106 19:26:57.871145 31365 sgd_solver.cpp:106] Iteration 7240, lr = 0.001
I1106 19:27:49.993777 31365 solver.cpp:229] Iteration 7260, loss = 0.308595
I1106 19:27:49.993854 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 19:27:49.993867 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.194129 (* 1 = 0.194129 loss)
I1106 19:27:49.993873 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.02224 (* 1 = 0.02224 loss)
I1106 19:27:49.993878 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0202432 (* 1 = 0.0202432 loss)
I1106 19:27:49.993883 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0266778 (* 1 = 0.0266778 loss)
I1106 19:27:49.993891 31365 sgd_solver.cpp:106] Iteration 7260, lr = 0.001
I1106 19:28:42.159205 31365 solver.cpp:229] Iteration 7280, loss = 0.282081
I1106 19:28:42.159283 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:28:42.159297 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.151541 (* 1 = 0.151541 loss)
I1106 19:28:42.159303 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0880423 (* 1 = 0.0880423 loss)
I1106 19:28:42.159308 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00365428 (* 1 = 0.00365428 loss)
I1106 19:28:42.159313 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0194122 (* 1 = 0.0194122 loss)
I1106 19:28:42.159320 31365 sgd_solver.cpp:106] Iteration 7280, lr = 0.001
I1106 19:29:34.365156 31365 solver.cpp:229] Iteration 7300, loss = 0.332372
I1106 19:29:34.365231 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:29:34.365243 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.208806 (* 1 = 0.208806 loss)
I1106 19:29:34.365250 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.112163 (* 1 = 0.112163 loss)
I1106 19:29:34.365257 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00209012 (* 1 = 0.00209012 loss)
I1106 19:29:34.365262 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0377664 (* 1 = 0.0377664 loss)
I1106 19:29:34.365268 31365 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1106 19:30:26.578894 31365 solver.cpp:229] Iteration 7320, loss = 0.342121
I1106 19:30:26.578969 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:30:26.578981 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.180596 (* 1 = 0.180596 loss)
I1106 19:30:26.578989 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.106455 (* 1 = 0.106455 loss)
I1106 19:30:26.578994 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0222759 (* 1 = 0.0222759 loss)
I1106 19:30:26.578999 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.035713 (* 1 = 0.035713 loss)
I1106 19:30:26.579005 31365 sgd_solver.cpp:106] Iteration 7320, lr = 0.001
I1106 19:31:18.804831 31365 solver.cpp:229] Iteration 7340, loss = 0.307236
I1106 19:31:18.804905 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:31:18.804919 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.192904 (* 1 = 0.192904 loss)
I1106 19:31:18.804926 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0417745 (* 1 = 0.0417745 loss)
I1106 19:31:18.804931 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0363877 (* 1 = 0.0363877 loss)
I1106 19:31:18.804936 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0207861 (* 1 = 0.0207861 loss)
I1106 19:31:18.804944 31365 sgd_solver.cpp:106] Iteration 7340, lr = 0.001
I1106 19:32:10.958873 31365 solver.cpp:229] Iteration 7360, loss = 0.257988
I1106 19:32:10.958945 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:32:10.958958 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.189934 (* 1 = 0.189934 loss)
I1106 19:32:10.958966 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0742983 (* 1 = 0.0742983 loss)
I1106 19:32:10.958971 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00300176 (* 1 = 0.00300176 loss)
I1106 19:32:10.958976 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0137168 (* 1 = 0.0137168 loss)
I1106 19:32:10.958983 31365 sgd_solver.cpp:106] Iteration 7360, lr = 0.001
I1106 19:33:03.143533 31365 solver.cpp:229] Iteration 7380, loss = 0.309238
I1106 19:33:03.143606 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:33:03.143620 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.256531 (* 1 = 0.256531 loss)
I1106 19:33:03.143628 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0970943 (* 1 = 0.0970943 loss)
I1106 19:33:03.143633 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0108628 (* 1 = 0.0108628 loss)
I1106 19:33:03.143638 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0143844 (* 1 = 0.0143844 loss)
I1106 19:33:03.143646 31365 sgd_solver.cpp:106] Iteration 7380, lr = 0.001
I1106 19:33:55.346472 31365 solver.cpp:229] Iteration 7400, loss = 0.337431
I1106 19:33:55.346546 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 19:33:55.346560 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.250239 (* 1 = 0.250239 loss)
I1106 19:33:55.346566 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.14525 (* 1 = 0.14525 loss)
I1106 19:33:55.346572 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00848968 (* 1 = 0.00848968 loss)
I1106 19:33:55.346577 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00757155 (* 1 = 0.00757155 loss)
I1106 19:33:55.346585 31365 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1106 19:34:47.560482 31365 solver.cpp:229] Iteration 7420, loss = 0.388217
I1106 19:34:47.560554 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:34:47.560567 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.241392 (* 1 = 0.241392 loss)
I1106 19:34:47.560575 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.130937 (* 1 = 0.130937 loss)
I1106 19:34:47.560581 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00523889 (* 1 = 0.00523889 loss)
I1106 19:34:47.560586 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0472275 (* 1 = 0.0472275 loss)
I1106 19:34:47.560592 31365 sgd_solver.cpp:106] Iteration 7420, lr = 0.001
I1106 19:35:39.711159 31365 solver.cpp:229] Iteration 7440, loss = 0.462762
I1106 19:35:39.711230 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:35:39.711241 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.201951 (* 1 = 0.201951 loss)
I1106 19:35:39.711247 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0665057 (* 1 = 0.0665057 loss)
I1106 19:35:39.711253 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00458006 (* 1 = 0.00458006 loss)
I1106 19:35:39.711257 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00889964 (* 1 = 0.00889964 loss)
I1106 19:35:39.711264 31365 sgd_solver.cpp:106] Iteration 7440, lr = 0.001
I1106 19:36:31.844632 31365 solver.cpp:229] Iteration 7460, loss = 0.325186
I1106 19:36:31.844698 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:36:31.844712 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178051 (* 1 = 0.178051 loss)
I1106 19:36:31.844720 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.098465 (* 1 = 0.098465 loss)
I1106 19:36:31.844725 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0313305 (* 1 = 0.0313305 loss)
I1106 19:36:31.844730 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0164614 (* 1 = 0.0164614 loss)
I1106 19:36:31.844736 31365 sgd_solver.cpp:106] Iteration 7460, lr = 0.001
I1106 19:37:23.951565 31365 solver.cpp:229] Iteration 7480, loss = 0.316604
I1106 19:37:23.951633 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:37:23.951645 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.185167 (* 1 = 0.185167 loss)
I1106 19:37:23.951654 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0710458 (* 1 = 0.0710458 loss)
I1106 19:37:23.951659 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00292592 (* 1 = 0.00292592 loss)
I1106 19:37:23.951664 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00720054 (* 1 = 0.00720054 loss)
I1106 19:37:23.951670 31365 sgd_solver.cpp:106] Iteration 7480, lr = 0.001
I1106 19:38:16.073098 31365 solver.cpp:229] Iteration 7500, loss = 0.261133
I1106 19:38:16.073165 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:38:16.073177 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.145587 (* 1 = 0.145587 loss)
I1106 19:38:16.073184 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0788291 (* 1 = 0.0788291 loss)
I1106 19:38:16.073189 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00615824 (* 1 = 0.00615824 loss)
I1106 19:38:16.073194 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0200313 (* 1 = 0.0200313 loss)
I1106 19:38:16.073199 31365 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1106 19:39:08.171777 31365 solver.cpp:229] Iteration 7520, loss = 0.542929
I1106 19:39:08.171842 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:39:08.171855 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.154758 (* 1 = 0.154758 loss)
I1106 19:39:08.171861 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0933766 (* 1 = 0.0933766 loss)
I1106 19:39:08.171866 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.011112 (* 1 = 0.011112 loss)
I1106 19:39:08.171870 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00873826 (* 1 = 0.00873826 loss)
I1106 19:39:08.171877 31365 sgd_solver.cpp:106] Iteration 7520, lr = 0.001
I1106 19:40:00.295991 31365 solver.cpp:229] Iteration 7540, loss = 0.344866
I1106 19:40:00.296058 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:40:00.296072 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.185661 (* 1 = 0.185661 loss)
I1106 19:40:00.296077 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0857956 (* 1 = 0.0857956 loss)
I1106 19:40:00.296082 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00550817 (* 1 = 0.00550817 loss)
I1106 19:40:00.296087 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0266387 (* 1 = 0.0266387 loss)
I1106 19:40:00.296093 31365 sgd_solver.cpp:106] Iteration 7540, lr = 0.001
I1106 19:40:52.388698 31365 solver.cpp:229] Iteration 7560, loss = 0.309277
I1106 19:40:52.388767 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:40:52.388779 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.0928915 (* 1 = 0.0928915 loss)
I1106 19:40:52.388788 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.145605 (* 1 = 0.145605 loss)
I1106 19:40:52.388793 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00455291 (* 1 = 0.00455291 loss)
I1106 19:40:52.388798 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00713825 (* 1 = 0.00713825 loss)
I1106 19:40:52.388804 31365 sgd_solver.cpp:106] Iteration 7560, lr = 0.001
I1106 19:41:44.493793 31365 solver.cpp:229] Iteration 7580, loss = 0.373372
I1106 19:41:44.493849 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 19:41:44.493863 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.122983 (* 1 = 0.122983 loss)
I1106 19:41:44.493871 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.121683 (* 1 = 0.121683 loss)
I1106 19:41:44.493878 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0654959 (* 1 = 0.0654959 loss)
I1106 19:41:44.493885 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0671641 (* 1 = 0.0671641 loss)
I1106 19:41:44.493893 31365 sgd_solver.cpp:106] Iteration 7580, lr = 0.001
I1106 19:42:36.652047 31365 solver.cpp:229] Iteration 7600, loss = 0.358875
I1106 19:42:36.652113 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:42:36.652138 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.26311 (* 1 = 0.26311 loss)
I1106 19:42:36.652144 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0982924 (* 1 = 0.0982924 loss)
I1106 19:42:36.652149 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0333005 (* 1 = 0.0333005 loss)
I1106 19:42:36.652154 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0159234 (* 1 = 0.0159234 loss)
I1106 19:42:36.652160 31365 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1106 19:43:28.792475 31365 solver.cpp:229] Iteration 7620, loss = 0.347479
I1106 19:43:28.792544 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:43:28.792557 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.107605 (* 1 = 0.107605 loss)
I1106 19:43:28.792562 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.111201 (* 1 = 0.111201 loss)
I1106 19:43:28.792567 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00432435 (* 1 = 0.00432435 loss)
I1106 19:43:28.792572 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0101379 (* 1 = 0.0101379 loss)
I1106 19:43:28.792578 31365 sgd_solver.cpp:106] Iteration 7620, lr = 0.001
I1106 19:44:20.937908 31365 solver.cpp:229] Iteration 7640, loss = 0.395912
I1106 19:44:20.937978 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 19:44:20.937990 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.240019 (* 1 = 0.240019 loss)
I1106 19:44:20.938012 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.226474 (* 1 = 0.226474 loss)
I1106 19:44:20.938017 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00472683 (* 1 = 0.00472683 loss)
I1106 19:44:20.938022 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0132598 (* 1 = 0.0132598 loss)
I1106 19:44:20.938030 31365 sgd_solver.cpp:106] Iteration 7640, lr = 0.001
I1106 19:45:13.121279 31365 solver.cpp:229] Iteration 7660, loss = 0.337671
I1106 19:45:13.121327 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 19:45:13.121340 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.171083 (* 1 = 0.171083 loss)
I1106 19:45:13.121347 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.171646 (* 1 = 0.171646 loss)
I1106 19:45:13.121354 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00518198 (* 1 = 0.00518198 loss)
I1106 19:45:13.121361 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0323182 (* 1 = 0.0323182 loss)
I1106 19:45:13.121369 31365 sgd_solver.cpp:106] Iteration 7660, lr = 0.001
I1106 19:46:05.190670 31365 solver.cpp:229] Iteration 7680, loss = 0.324955
I1106 19:46:05.190734 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:46:05.190747 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.141192 (* 1 = 0.141192 loss)
I1106 19:46:05.190755 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0506945 (* 1 = 0.0506945 loss)
I1106 19:46:05.190760 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00374559 (* 1 = 0.00374559 loss)
I1106 19:46:05.190765 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0113001 (* 1 = 0.0113001 loss)
I1106 19:46:05.190771 31365 sgd_solver.cpp:106] Iteration 7680, lr = 0.001
I1106 19:46:57.340268 31365 solver.cpp:229] Iteration 7700, loss = 0.339846
I1106 19:46:57.340343 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:46:57.340358 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.147001 (* 1 = 0.147001 loss)
I1106 19:46:57.340363 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.166149 (* 1 = 0.166149 loss)
I1106 19:46:57.340368 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0381774 (* 1 = 0.0381774 loss)
I1106 19:46:57.340373 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0182624 (* 1 = 0.0182624 loss)
I1106 19:46:57.340380 31365 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1106 19:47:49.494076 31365 solver.cpp:229] Iteration 7720, loss = 0.417705
I1106 19:47:49.494143 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:47:49.494154 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.337059 (* 1 = 0.337059 loss)
I1106 19:47:49.494160 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.113197 (* 1 = 0.113197 loss)
I1106 19:47:49.494166 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0310468 (* 1 = 0.0310468 loss)
I1106 19:47:49.494170 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0503752 (* 1 = 0.0503752 loss)
I1106 19:47:49.494177 31365 sgd_solver.cpp:106] Iteration 7720, lr = 0.001
I1106 19:48:41.645700 31365 solver.cpp:229] Iteration 7740, loss = 0.334639
I1106 19:48:41.645764 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:48:41.645783 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.174189 (* 1 = 0.174189 loss)
I1106 19:48:41.645797 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.10265 (* 1 = 0.10265 loss)
I1106 19:48:41.645807 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0328202 (* 1 = 0.0328202 loss)
I1106 19:48:41.645817 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0206148 (* 1 = 0.0206148 loss)
I1106 19:48:41.645828 31365 sgd_solver.cpp:106] Iteration 7740, lr = 0.001
I1106 19:49:33.835144 31365 solver.cpp:229] Iteration 7760, loss = 0.353235
I1106 19:49:33.835223 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:49:33.835237 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.160116 (* 1 = 0.160116 loss)
I1106 19:49:33.835243 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0948302 (* 1 = 0.0948302 loss)
I1106 19:49:33.835248 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0218161 (* 1 = 0.0218161 loss)
I1106 19:49:33.835253 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0794162 (* 1 = 0.0794162 loss)
I1106 19:49:33.835260 31365 sgd_solver.cpp:106] Iteration 7760, lr = 0.001
I1106 19:50:26.015230 31365 solver.cpp:229] Iteration 7780, loss = 0.319453
I1106 19:50:26.015306 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:50:26.015319 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.238478 (* 1 = 0.238478 loss)
I1106 19:50:26.015326 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0935141 (* 1 = 0.0935141 loss)
I1106 19:50:26.015331 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0133088 (* 1 = 0.0133088 loss)
I1106 19:50:26.015336 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0745948 (* 1 = 0.0745948 loss)
I1106 19:50:26.015343 31365 sgd_solver.cpp:106] Iteration 7780, lr = 0.001
I1106 19:51:18.189905 31365 solver.cpp:229] Iteration 7800, loss = 0.166476
I1106 19:51:18.189980 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 19:51:18.189993 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.123388 (* 1 = 0.123388 loss)
I1106 19:51:18.190001 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0227566 (* 1 = 0.0227566 loss)
I1106 19:51:18.190006 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00232746 (* 1 = 0.00232746 loss)
I1106 19:51:18.190011 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0155235 (* 1 = 0.0155235 loss)
I1106 19:51:18.190017 31365 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1106 19:52:10.325755 31365 solver.cpp:229] Iteration 7820, loss = 0.425762
I1106 19:52:10.325824 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:52:10.325836 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.266054 (* 1 = 0.266054 loss)
I1106 19:52:10.325842 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0857102 (* 1 = 0.0857102 loss)
I1106 19:52:10.325847 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0322508 (* 1 = 0.0322508 loss)
I1106 19:52:10.325852 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0509077 (* 1 = 0.0509077 loss)
I1106 19:52:10.325860 31365 sgd_solver.cpp:106] Iteration 7820, lr = 0.001
I1106 19:53:02.496156 31365 solver.cpp:229] Iteration 7840, loss = 0.473854
I1106 19:53:02.496230 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 19:53:02.496243 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.279702 (* 1 = 0.279702 loss)
I1106 19:53:02.496250 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.207456 (* 1 = 0.207456 loss)
I1106 19:53:02.496255 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.063357 (* 1 = 0.063357 loss)
I1106 19:53:02.496260 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0615209 (* 1 = 0.0615209 loss)
I1106 19:53:02.496268 31365 sgd_solver.cpp:106] Iteration 7840, lr = 0.001
I1106 19:53:54.717730 31365 solver.cpp:229] Iteration 7860, loss = 0.280755
I1106 19:53:54.717804 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:53:54.717819 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.101282 (* 1 = 0.101282 loss)
I1106 19:53:54.717824 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0568256 (* 1 = 0.0568256 loss)
I1106 19:53:54.717830 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0241495 (* 1 = 0.0241495 loss)
I1106 19:53:54.717835 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00675633 (* 1 = 0.00675633 loss)
I1106 19:53:54.717842 31365 sgd_solver.cpp:106] Iteration 7860, lr = 0.001
I1106 19:54:46.869201 31365 solver.cpp:229] Iteration 7880, loss = 0.23952
I1106 19:54:46.869269 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:54:46.869282 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.18202 (* 1 = 0.18202 loss)
I1106 19:54:46.869288 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0430158 (* 1 = 0.0430158 loss)
I1106 19:54:46.869293 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00245063 (* 1 = 0.00245063 loss)
I1106 19:54:46.869298 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00648022 (* 1 = 0.00648022 loss)
I1106 19:54:46.869305 31365 sgd_solver.cpp:106] Iteration 7880, lr = 0.001
I1106 19:55:39.018061 31365 solver.cpp:229] Iteration 7900, loss = 0.280831
I1106 19:55:39.018128 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 19:55:39.018141 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.139275 (* 1 = 0.139275 loss)
I1106 19:55:39.018148 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.090963 (* 1 = 0.090963 loss)
I1106 19:55:39.018153 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00968063 (* 1 = 0.00968063 loss)
I1106 19:55:39.018158 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00866319 (* 1 = 0.00866319 loss)
I1106 19:55:39.018165 31365 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1106 19:56:31.136878 31365 solver.cpp:229] Iteration 7920, loss = 0.294051
I1106 19:56:31.136942 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 19:56:31.136955 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.141383 (* 1 = 0.141383 loss)
I1106 19:56:31.136960 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118396 (* 1 = 0.118396 loss)
I1106 19:56:31.136965 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0175437 (* 1 = 0.0175437 loss)
I1106 19:56:31.136970 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.024233 (* 1 = 0.024233 loss)
I1106 19:56:31.136976 31365 sgd_solver.cpp:106] Iteration 7920, lr = 0.001
I1106 19:57:23.260527 31365 solver.cpp:229] Iteration 7940, loss = 0.322767
I1106 19:57:23.260596 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 19:57:23.260607 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.158185 (* 1 = 0.158185 loss)
I1106 19:57:23.260613 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0188041 (* 1 = 0.0188041 loss)
I1106 19:57:23.260619 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00908032 (* 1 = 0.00908032 loss)
I1106 19:57:23.260624 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00798856 (* 1 = 0.00798856 loss)
I1106 19:57:23.260630 31365 sgd_solver.cpp:106] Iteration 7940, lr = 0.001
I1106 19:58:15.389523 31365 solver.cpp:229] Iteration 7960, loss = 0.475011
I1106 19:58:15.389590 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 19:58:15.389614 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.179323 (* 1 = 0.179323 loss)
I1106 19:58:15.389623 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.264544 (* 1 = 0.264544 loss)
I1106 19:58:15.389628 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0135919 (* 1 = 0.0135919 loss)
I1106 19:58:15.389633 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0217103 (* 1 = 0.0217103 loss)
I1106 19:58:15.389641 31365 sgd_solver.cpp:106] Iteration 7960, lr = 0.001
I1106 19:59:07.530527 31365 solver.cpp:229] Iteration 7980, loss = 0.449026
I1106 19:59:07.530598 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 19:59:07.530611 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.305195 (* 1 = 0.305195 loss)
I1106 19:59:07.530617 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.172586 (* 1 = 0.172586 loss)
I1106 19:59:07.530623 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0716607 (* 1 = 0.0716607 loss)
I1106 19:59:07.530627 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0600944 (* 1 = 0.0600944 loss)
I1106 19:59:07.530634 31365 sgd_solver.cpp:106] Iteration 7980, lr = 0.001
I1106 19:59:59.696467 31365 solver.cpp:229] Iteration 8000, loss = 0.232691
I1106 19:59:59.696534 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 19:59:59.696559 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.186711 (* 1 = 0.186711 loss)
I1106 19:59:59.696568 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0334155 (* 1 = 0.0334155 loss)
I1106 19:59:59.696573 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0297576 (* 1 = 0.0297576 loss)
I1106 19:59:59.696578 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00786445 (* 1 = 0.00786445 loss)
I1106 19:59:59.696585 31365 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1106 20:00:51.845962 31365 solver.cpp:229] Iteration 8020, loss = 0.276062
I1106 20:00:51.846027 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:00:51.846040 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.182599 (* 1 = 0.182599 loss)
I1106 20:00:51.846045 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0945076 (* 1 = 0.0945076 loss)
I1106 20:00:51.846050 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00227877 (* 1 = 0.00227877 loss)
I1106 20:00:51.846055 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0076427 (* 1 = 0.0076427 loss)
I1106 20:00:51.846062 31365 sgd_solver.cpp:106] Iteration 8020, lr = 0.001
I1106 20:01:44.007758 31365 solver.cpp:229] Iteration 8040, loss = 0.292321
I1106 20:01:44.007827 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:01:44.007838 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.142433 (* 1 = 0.142433 loss)
I1106 20:01:44.007848 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.126033 (* 1 = 0.126033 loss)
I1106 20:01:44.007853 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00302043 (* 1 = 0.00302043 loss)
I1106 20:01:44.007858 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00839486 (* 1 = 0.00839486 loss)
I1106 20:01:44.007864 31365 sgd_solver.cpp:106] Iteration 8040, lr = 0.001
I1106 20:02:36.126809 31365 solver.cpp:229] Iteration 8060, loss = 0.283095
I1106 20:02:36.126876 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:02:36.126889 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.135506 (* 1 = 0.135506 loss)
I1106 20:02:36.126899 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0660006 (* 1 = 0.0660006 loss)
I1106 20:02:36.126904 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0247525 (* 1 = 0.0247525 loss)
I1106 20:02:36.126909 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0822834 (* 1 = 0.0822834 loss)
I1106 20:02:36.126915 31365 sgd_solver.cpp:106] Iteration 8060, lr = 0.001
I1106 20:03:28.267141 31365 solver.cpp:229] Iteration 8080, loss = 0.308478
I1106 20:03:28.267215 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:03:28.267228 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.213468 (* 1 = 0.213468 loss)
I1106 20:03:28.267235 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0589483 (* 1 = 0.0589483 loss)
I1106 20:03:28.267241 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.004592 (* 1 = 0.004592 loss)
I1106 20:03:28.267246 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0047546 (* 1 = 0.0047546 loss)
I1106 20:03:28.267252 31365 sgd_solver.cpp:106] Iteration 8080, lr = 0.001
I1106 20:04:20.441599 31365 solver.cpp:229] Iteration 8100, loss = 0.229464
I1106 20:04:20.441668 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:04:20.441692 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.138343 (* 1 = 0.138343 loss)
I1106 20:04:20.441699 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0330613 (* 1 = 0.0330613 loss)
I1106 20:04:20.441704 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00296886 (* 1 = 0.00296886 loss)
I1106 20:04:20.441709 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.010361 (* 1 = 0.010361 loss)
I1106 20:04:20.441715 31365 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1106 20:05:12.602851 31365 solver.cpp:229] Iteration 8120, loss = 0.493973
I1106 20:05:12.602921 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:05:12.602934 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.136108 (* 1 = 0.136108 loss)
I1106 20:05:12.602939 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.130104 (* 1 = 0.130104 loss)
I1106 20:05:12.602944 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.126162 (* 1 = 0.126162 loss)
I1106 20:05:12.602949 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0157871 (* 1 = 0.0157871 loss)
I1106 20:05:12.602957 31365 sgd_solver.cpp:106] Iteration 8120, lr = 0.001
I1106 20:06:04.723162 31365 solver.cpp:229] Iteration 8140, loss = 0.470502
I1106 20:06:04.723232 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 20:06:04.723243 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.386376 (* 1 = 0.386376 loss)
I1106 20:06:04.723249 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.205796 (* 1 = 0.205796 loss)
I1106 20:06:04.723254 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0044397 (* 1 = 0.0044397 loss)
I1106 20:06:04.723259 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0757948 (* 1 = 0.0757948 loss)
I1106 20:06:04.723266 31365 sgd_solver.cpp:106] Iteration 8140, lr = 0.001
I1106 20:06:56.871690 31365 solver.cpp:229] Iteration 8160, loss = 0.350845
I1106 20:06:56.871773 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:06:56.871800 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.165849 (* 1 = 0.165849 loss)
I1106 20:06:56.871806 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.166991 (* 1 = 0.166991 loss)
I1106 20:06:56.871811 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.018078 (* 1 = 0.018078 loss)
I1106 20:06:56.871817 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0233064 (* 1 = 0.0233064 loss)
I1106 20:06:56.871824 31365 sgd_solver.cpp:106] Iteration 8160, lr = 0.001
I1106 20:07:49.018821 31365 solver.cpp:229] Iteration 8180, loss = 0.402764
I1106 20:07:49.018888 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:07:49.018914 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.192227 (* 1 = 0.192227 loss)
I1106 20:07:49.018923 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.106284 (* 1 = 0.106284 loss)
I1106 20:07:49.018927 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0344829 (* 1 = 0.0344829 loss)
I1106 20:07:49.018932 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0226409 (* 1 = 0.0226409 loss)
I1106 20:07:49.018939 31365 sgd_solver.cpp:106] Iteration 8180, lr = 0.001
I1106 20:08:41.180866 31365 solver.cpp:229] Iteration 8200, loss = 0.294417
I1106 20:08:41.180932 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:08:41.180943 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.196663 (* 1 = 0.196663 loss)
I1106 20:08:41.180949 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0620963 (* 1 = 0.0620963 loss)
I1106 20:08:41.180955 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00566153 (* 1 = 0.00566153 loss)
I1106 20:08:41.180959 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00812332 (* 1 = 0.00812332 loss)
I1106 20:08:41.180966 31365 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1106 20:09:33.376128 31365 solver.cpp:229] Iteration 8220, loss = 0.349761
I1106 20:09:33.376196 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:09:33.376209 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.214248 (* 1 = 0.214248 loss)
I1106 20:09:33.376217 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.086536 (* 1 = 0.086536 loss)
I1106 20:09:33.376224 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0225545 (* 1 = 0.0225545 loss)
I1106 20:09:33.376229 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.046518 (* 1 = 0.046518 loss)
I1106 20:09:33.376235 31365 sgd_solver.cpp:106] Iteration 8220, lr = 0.001
I1106 20:10:25.500798 31365 solver.cpp:229] Iteration 8240, loss = 0.235269
I1106 20:10:25.500869 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:10:25.500895 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.132309 (* 1 = 0.132309 loss)
I1106 20:10:25.500902 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0848263 (* 1 = 0.0848263 loss)
I1106 20:10:25.500907 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0296148 (* 1 = 0.0296148 loss)
I1106 20:10:25.500912 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0100907 (* 1 = 0.0100907 loss)
I1106 20:10:25.500919 31365 sgd_solver.cpp:106] Iteration 8240, lr = 0.001
I1106 20:11:17.628108 31365 solver.cpp:229] Iteration 8260, loss = 0.361224
I1106 20:11:17.628182 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:11:17.628196 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.211303 (* 1 = 0.211303 loss)
I1106 20:11:17.628201 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0771217 (* 1 = 0.0771217 loss)
I1106 20:11:17.628206 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0425038 (* 1 = 0.0425038 loss)
I1106 20:11:17.628211 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0565283 (* 1 = 0.0565283 loss)
I1106 20:11:17.628218 31365 sgd_solver.cpp:106] Iteration 8260, lr = 0.001
I1106 20:12:09.820164 31365 solver.cpp:229] Iteration 8280, loss = 0.458243
I1106 20:12:09.820240 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:12:09.820252 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.290367 (* 1 = 0.290367 loss)
I1106 20:12:09.820258 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0966226 (* 1 = 0.0966226 loss)
I1106 20:12:09.820264 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00812642 (* 1 = 0.00812642 loss)
I1106 20:12:09.820269 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.015301 (* 1 = 0.015301 loss)
I1106 20:12:09.820276 31365 sgd_solver.cpp:106] Iteration 8280, lr = 0.001
I1106 20:13:01.985620 31365 solver.cpp:229] Iteration 8300, loss = 0.310484
I1106 20:13:01.985692 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:13:01.985705 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.161167 (* 1 = 0.161167 loss)
I1106 20:13:01.985711 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116558 (* 1 = 0.116558 loss)
I1106 20:13:01.985718 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00599928 (* 1 = 0.00599928 loss)
I1106 20:13:01.985723 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00730756 (* 1 = 0.00730756 loss)
I1106 20:13:01.985729 31365 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1106 20:13:54.133412 31365 solver.cpp:229] Iteration 8320, loss = 0.232253
I1106 20:13:54.133481 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:13:54.133493 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.143034 (* 1 = 0.143034 loss)
I1106 20:13:54.133499 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0829533 (* 1 = 0.0829533 loss)
I1106 20:13:54.133504 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0045774 (* 1 = 0.0045774 loss)
I1106 20:13:54.133509 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00870403 (* 1 = 0.00870403 loss)
I1106 20:13:54.133517 31365 sgd_solver.cpp:106] Iteration 8320, lr = 0.001
I1106 20:14:46.219256 31365 solver.cpp:229] Iteration 8340, loss = 0.410836
I1106 20:14:46.219326 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:14:46.219338 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.191728 (* 1 = 0.191728 loss)
I1106 20:14:46.219346 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.137855 (* 1 = 0.137855 loss)
I1106 20:14:46.219350 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0402848 (* 1 = 0.0402848 loss)
I1106 20:14:46.219355 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0550253 (* 1 = 0.0550253 loss)
I1106 20:14:46.219362 31365 sgd_solver.cpp:106] Iteration 8340, lr = 0.001
I1106 20:15:38.297471 31365 solver.cpp:229] Iteration 8360, loss = 0.31472
I1106 20:15:38.297539 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:15:38.297551 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.150219 (* 1 = 0.150219 loss)
I1106 20:15:38.297560 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118112 (* 1 = 0.118112 loss)
I1106 20:15:38.297565 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0189145 (* 1 = 0.0189145 loss)
I1106 20:15:38.297571 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00667838 (* 1 = 0.00667838 loss)
I1106 20:15:38.297577 31365 sgd_solver.cpp:106] Iteration 8360, lr = 0.001
I1106 20:16:30.467538 31365 solver.cpp:229] Iteration 8380, loss = 0.230873
I1106 20:16:30.467612 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 20:16:30.467640 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.146844 (* 1 = 0.146844 loss)
I1106 20:16:30.467646 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.042541 (* 1 = 0.042541 loss)
I1106 20:16:30.467653 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00149033 (* 1 = 0.00149033 loss)
I1106 20:16:30.467658 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0258679 (* 1 = 0.0258679 loss)
I1106 20:16:30.467664 31365 sgd_solver.cpp:106] Iteration 8380, lr = 0.001
I1106 20:17:22.634356 31365 solver.cpp:229] Iteration 8400, loss = 0.560853
I1106 20:17:22.634441 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:17:22.634455 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.161159 (* 1 = 0.161159 loss)
I1106 20:17:22.634460 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.180707 (* 1 = 0.180707 loss)
I1106 20:17:22.634465 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.297095 (* 1 = 0.297095 loss)
I1106 20:17:22.634471 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0812823 (* 1 = 0.0812823 loss)
I1106 20:17:22.634479 31365 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1106 20:18:14.805127 31365 solver.cpp:229] Iteration 8420, loss = 0.293184
I1106 20:18:14.805203 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:18:14.805217 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.198005 (* 1 = 0.198005 loss)
I1106 20:18:14.805223 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.102785 (* 1 = 0.102785 loss)
I1106 20:18:14.805229 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00580942 (* 1 = 0.00580942 loss)
I1106 20:18:14.805234 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0778353 (* 1 = 0.0778353 loss)
I1106 20:18:14.805243 31365 sgd_solver.cpp:106] Iteration 8420, lr = 0.001
I1106 20:19:06.973206 31365 solver.cpp:229] Iteration 8440, loss = 0.422229
I1106 20:19:06.973281 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:19:06.973294 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.226658 (* 1 = 0.226658 loss)
I1106 20:19:06.973301 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0440317 (* 1 = 0.0440317 loss)
I1106 20:19:06.973306 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00822087 (* 1 = 0.00822087 loss)
I1106 20:19:06.973311 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0306352 (* 1 = 0.0306352 loss)
I1106 20:19:06.973318 31365 sgd_solver.cpp:106] Iteration 8440, lr = 0.001
I1106 20:19:59.176391 31365 solver.cpp:229] Iteration 8460, loss = 0.368573
I1106 20:19:59.176460 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:19:59.176486 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.122868 (* 1 = 0.122868 loss)
I1106 20:19:59.176494 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0835042 (* 1 = 0.0835042 loss)
I1106 20:19:59.176499 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0022415 (* 1 = 0.0022415 loss)
I1106 20:19:59.176504 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.005813 (* 1 = 0.005813 loss)
I1106 20:19:59.176512 31365 sgd_solver.cpp:106] Iteration 8460, lr = 0.001
I1106 20:20:51.321620 31365 solver.cpp:229] Iteration 8480, loss = 0.394473
I1106 20:20:51.321687 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:20:51.321701 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.194306 (* 1 = 0.194306 loss)
I1106 20:20:51.321705 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.153705 (* 1 = 0.153705 loss)
I1106 20:20:51.321712 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0295618 (* 1 = 0.0295618 loss)
I1106 20:20:51.321715 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0297676 (* 1 = 0.0297676 loss)
I1106 20:20:51.321722 31365 sgd_solver.cpp:106] Iteration 8480, lr = 0.001
I1106 20:21:43.435407 31365 solver.cpp:229] Iteration 8500, loss = 0.356214
I1106 20:21:43.435475 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:21:43.435488 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.134379 (* 1 = 0.134379 loss)
I1106 20:21:43.435494 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.194005 (* 1 = 0.194005 loss)
I1106 20:21:43.435499 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0010433 (* 1 = 0.0010433 loss)
I1106 20:21:43.435504 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0124561 (* 1 = 0.0124561 loss)
I1106 20:21:43.435511 31365 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1106 20:22:35.566685 31365 solver.cpp:229] Iteration 8520, loss = 0.248379
I1106 20:22:35.566753 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:22:35.566766 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.159631 (* 1 = 0.159631 loss)
I1106 20:22:35.566773 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116996 (* 1 = 0.116996 loss)
I1106 20:22:35.566779 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00549631 (* 1 = 0.00549631 loss)
I1106 20:22:35.566784 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0155184 (* 1 = 0.0155184 loss)
I1106 20:22:35.566790 31365 sgd_solver.cpp:106] Iteration 8520, lr = 0.001
I1106 20:23:27.722261 31365 solver.cpp:229] Iteration 8540, loss = 0.265957
I1106 20:23:27.722331 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:23:27.722342 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.145238 (* 1 = 0.145238 loss)
I1106 20:23:27.722350 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0694059 (* 1 = 0.0694059 loss)
I1106 20:23:27.722357 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00474829 (* 1 = 0.00474829 loss)
I1106 20:23:27.722360 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00229363 (* 1 = 0.00229363 loss)
I1106 20:23:27.722368 31365 sgd_solver.cpp:106] Iteration 8540, lr = 0.001
I1106 20:24:19.851531 31365 solver.cpp:229] Iteration 8560, loss = 0.431419
I1106 20:24:19.851598 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:24:19.851611 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.263561 (* 1 = 0.263561 loss)
I1106 20:24:19.851619 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.178029 (* 1 = 0.178029 loss)
I1106 20:24:19.851624 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0143291 (* 1 = 0.0143291 loss)
I1106 20:24:19.851629 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0733963 (* 1 = 0.0733963 loss)
I1106 20:24:19.851635 31365 sgd_solver.cpp:106] Iteration 8560, lr = 0.001
I1106 20:25:11.974901 31365 solver.cpp:229] Iteration 8580, loss = 0.347903
I1106 20:25:11.974969 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 20:25:11.974982 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.19338 (* 1 = 0.19338 loss)
I1106 20:25:11.974992 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0858075 (* 1 = 0.0858075 loss)
I1106 20:25:11.974997 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00353706 (* 1 = 0.00353706 loss)
I1106 20:25:11.975002 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0180272 (* 1 = 0.0180272 loss)
I1106 20:25:11.975008 31365 sgd_solver.cpp:106] Iteration 8580, lr = 0.001
I1106 20:26:04.073663 31365 solver.cpp:229] Iteration 8600, loss = 0.416572
I1106 20:26:04.073732 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 20:26:04.073757 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.234523 (* 1 = 0.234523 loss)
I1106 20:26:04.073763 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.209331 (* 1 = 0.209331 loss)
I1106 20:26:04.073768 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0146263 (* 1 = 0.0146263 loss)
I1106 20:26:04.073773 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0411348 (* 1 = 0.0411348 loss)
I1106 20:26:04.073781 31365 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1106 20:26:56.235158 31365 solver.cpp:229] Iteration 8620, loss = 0.279733
I1106 20:26:56.235229 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:26:56.235240 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.137701 (* 1 = 0.137701 loss)
I1106 20:26:56.235249 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0843021 (* 1 = 0.0843021 loss)
I1106 20:26:56.235255 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00340044 (* 1 = 0.00340044 loss)
I1106 20:26:56.235260 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0174587 (* 1 = 0.0174587 loss)
I1106 20:26:56.235265 31365 sgd_solver.cpp:106] Iteration 8620, lr = 0.001
I1106 20:27:48.395938 31365 solver.cpp:229] Iteration 8640, loss = 0.280716
I1106 20:27:48.396013 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:27:48.396028 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.136807 (* 1 = 0.136807 loss)
I1106 20:27:48.396034 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0616064 (* 1 = 0.0616064 loss)
I1106 20:27:48.396039 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00825301 (* 1 = 0.00825301 loss)
I1106 20:27:48.396044 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0182537 (* 1 = 0.0182537 loss)
I1106 20:27:48.396050 31365 sgd_solver.cpp:106] Iteration 8640, lr = 0.001
I1106 20:28:40.558133 31365 solver.cpp:229] Iteration 8660, loss = 0.350399
I1106 20:28:40.558208 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:28:40.558223 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.114076 (* 1 = 0.114076 loss)
I1106 20:28:40.558228 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0709268 (* 1 = 0.0709268 loss)
I1106 20:28:40.558233 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0167881 (* 1 = 0.0167881 loss)
I1106 20:28:40.558238 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0032864 (* 1 = 0.0032864 loss)
I1106 20:28:40.558245 31365 sgd_solver.cpp:106] Iteration 8660, lr = 0.001
I1106 20:29:32.710759 31365 solver.cpp:229] Iteration 8680, loss = 0.315093
I1106 20:29:32.710826 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 20:29:32.710840 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.136092 (* 1 = 0.136092 loss)
I1106 20:29:32.710849 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.124265 (* 1 = 0.124265 loss)
I1106 20:29:32.710855 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00436705 (* 1 = 0.00436705 loss)
I1106 20:29:32.710860 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00955234 (* 1 = 0.00955234 loss)
I1106 20:29:32.710865 31365 sgd_solver.cpp:106] Iteration 8680, lr = 0.001
I1106 20:30:24.839215 31365 solver.cpp:229] Iteration 8700, loss = 0.262205
I1106 20:30:24.839290 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:30:24.839304 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.122012 (* 1 = 0.122012 loss)
I1106 20:30:24.839310 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0684418 (* 1 = 0.0684418 loss)
I1106 20:30:24.839315 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0168668 (* 1 = 0.0168668 loss)
I1106 20:30:24.839320 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00444166 (* 1 = 0.00444166 loss)
I1106 20:30:24.839328 31365 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1106 20:31:16.999822 31365 solver.cpp:229] Iteration 8720, loss = 0.372618
I1106 20:31:16.999897 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:31:16.999910 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178792 (* 1 = 0.178792 loss)
I1106 20:31:16.999917 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.152683 (* 1 = 0.152683 loss)
I1106 20:31:16.999922 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00122559 (* 1 = 0.00122559 loss)
I1106 20:31:16.999927 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0188505 (* 1 = 0.0188505 loss)
I1106 20:31:16.999934 31365 sgd_solver.cpp:106] Iteration 8720, lr = 0.001
I1106 20:32:09.171659 31365 solver.cpp:229] Iteration 8740, loss = 0.34671
I1106 20:32:09.171726 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:32:09.171738 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.197188 (* 1 = 0.197188 loss)
I1106 20:32:09.171743 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.154171 (* 1 = 0.154171 loss)
I1106 20:32:09.171749 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00911177 (* 1 = 0.00911177 loss)
I1106 20:32:09.171754 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0178875 (* 1 = 0.0178875 loss)
I1106 20:32:09.171761 31365 sgd_solver.cpp:106] Iteration 8740, lr = 0.001
I1106 20:33:01.270041 31365 solver.cpp:229] Iteration 8760, loss = 0.319989
I1106 20:33:01.270112 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:33:01.270123 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.241049 (* 1 = 0.241049 loss)
I1106 20:33:01.270129 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.081697 (* 1 = 0.081697 loss)
I1106 20:33:01.270135 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0225343 (* 1 = 0.0225343 loss)
I1106 20:33:01.270139 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0211541 (* 1 = 0.0211541 loss)
I1106 20:33:01.270146 31365 sgd_solver.cpp:106] Iteration 8760, lr = 0.001
I1106 20:33:53.359359 31365 solver.cpp:229] Iteration 8780, loss = 0.312168
I1106 20:33:53.359428 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:33:53.359441 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.1798 (* 1 = 0.1798 loss)
I1106 20:33:53.359447 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.116182 (* 1 = 0.116182 loss)
I1106 20:33:53.359452 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0123493 (* 1 = 0.0123493 loss)
I1106 20:33:53.359457 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0659078 (* 1 = 0.0659078 loss)
I1106 20:33:53.359464 31365 sgd_solver.cpp:106] Iteration 8780, lr = 0.001
I1106 20:34:45.541858 31365 solver.cpp:229] Iteration 8800, loss = 0.404488
I1106 20:34:45.541934 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:34:45.541947 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.102826 (* 1 = 0.102826 loss)
I1106 20:34:45.541954 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.146699 (* 1 = 0.146699 loss)
I1106 20:34:45.541959 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0372523 (* 1 = 0.0372523 loss)
I1106 20:34:45.541963 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0414213 (* 1 = 0.0414213 loss)
I1106 20:34:45.541970 31365 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1106 20:35:37.682440 31365 solver.cpp:229] Iteration 8820, loss = 0.334452
I1106 20:35:37.682512 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:35:37.682526 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.205049 (* 1 = 0.205049 loss)
I1106 20:35:37.682533 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.189894 (* 1 = 0.189894 loss)
I1106 20:35:37.682538 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0106037 (* 1 = 0.0106037 loss)
I1106 20:35:37.682543 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0129388 (* 1 = 0.0129388 loss)
I1106 20:35:37.682549 31365 sgd_solver.cpp:106] Iteration 8820, lr = 0.001
I1106 20:36:29.875932 31365 solver.cpp:229] Iteration 8840, loss = 0.406386
I1106 20:36:29.876006 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:36:29.876020 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.275242 (* 1 = 0.275242 loss)
I1106 20:36:29.876025 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.102986 (* 1 = 0.102986 loss)
I1106 20:36:29.876031 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00885701 (* 1 = 0.00885701 loss)
I1106 20:36:29.876036 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.069138 (* 1 = 0.069138 loss)
I1106 20:36:29.876044 31365 sgd_solver.cpp:106] Iteration 8840, lr = 0.001
I1106 20:37:22.097605 31365 solver.cpp:229] Iteration 8860, loss = 0.399376
I1106 20:37:22.097679 31365 solver.cpp:245]     Train net output #0: accuracy = 0.890625
I1106 20:37:22.097703 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.209779 (* 1 = 0.209779 loss)
I1106 20:37:22.097708 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.273707 (* 1 = 0.273707 loss)
I1106 20:37:22.097713 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00500906 (* 1 = 0.00500906 loss)
I1106 20:37:22.097719 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0122173 (* 1 = 0.0122173 loss)
I1106 20:37:22.097725 31365 sgd_solver.cpp:106] Iteration 8860, lr = 0.001
I1106 20:38:14.251704 31365 solver.cpp:229] Iteration 8880, loss = 0.327306
I1106 20:38:14.251772 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:38:14.251785 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.196052 (* 1 = 0.196052 loss)
I1106 20:38:14.251793 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0648793 (* 1 = 0.0648793 loss)
I1106 20:38:14.251798 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0165938 (* 1 = 0.0165938 loss)
I1106 20:38:14.251803 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0203841 (* 1 = 0.0203841 loss)
I1106 20:38:14.251809 31365 sgd_solver.cpp:106] Iteration 8880, lr = 0.001
I1106 20:39:06.391163 31365 solver.cpp:229] Iteration 8900, loss = 0.393488
I1106 20:39:06.391232 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:39:06.391257 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.156634 (* 1 = 0.156634 loss)
I1106 20:39:06.391265 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0838191 (* 1 = 0.0838191 loss)
I1106 20:39:06.391270 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0276307 (* 1 = 0.0276307 loss)
I1106 20:39:06.391275 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0396243 (* 1 = 0.0396243 loss)
I1106 20:39:06.391283 31365 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1106 20:39:58.560290 31365 solver.cpp:229] Iteration 8920, loss = 0.309601
I1106 20:39:58.560361 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 20:39:58.560374 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.220532 (* 1 = 0.220532 loss)
I1106 20:39:58.560382 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0602713 (* 1 = 0.0602713 loss)
I1106 20:39:58.560387 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00942517 (* 1 = 0.00942517 loss)
I1106 20:39:58.560392 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0253495 (* 1 = 0.0253495 loss)
I1106 20:39:58.560400 31365 sgd_solver.cpp:106] Iteration 8920, lr = 0.001
I1106 20:40:50.688483 31365 solver.cpp:229] Iteration 8940, loss = 0.319061
I1106 20:40:50.688551 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:40:50.688565 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.19128 (* 1 = 0.19128 loss)
I1106 20:40:50.688573 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.149297 (* 1 = 0.149297 loss)
I1106 20:40:50.688578 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0069935 (* 1 = 0.0069935 loss)
I1106 20:40:50.688583 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0191522 (* 1 = 0.0191522 loss)
I1106 20:40:50.688591 31365 sgd_solver.cpp:106] Iteration 8940, lr = 0.001
I1106 20:41:42.824865 31365 solver.cpp:229] Iteration 8960, loss = 0.268895
I1106 20:41:42.824940 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:41:42.824954 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.170985 (* 1 = 0.170985 loss)
I1106 20:41:42.824959 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0924774 (* 1 = 0.0924774 loss)
I1106 20:41:42.824965 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0138446 (* 1 = 0.0138446 loss)
I1106 20:41:42.824970 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0327792 (* 1 = 0.0327792 loss)
I1106 20:41:42.824977 31365 sgd_solver.cpp:106] Iteration 8960, lr = 0.001
I1106 20:42:34.989534 31365 solver.cpp:229] Iteration 8980, loss = 0.313282
I1106 20:42:34.989608 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:42:34.989620 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.167507 (* 1 = 0.167507 loss)
I1106 20:42:34.989627 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0943805 (* 1 = 0.0943805 loss)
I1106 20:42:34.989632 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00194931 (* 1 = 0.00194931 loss)
I1106 20:42:34.989637 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00711772 (* 1 = 0.00711772 loss)
I1106 20:42:34.989645 31365 sgd_solver.cpp:106] Iteration 8980, lr = 0.001
I1106 20:43:27.180403 31365 solver.cpp:229] Iteration 9000, loss = 0.377776
I1106 20:43:27.180479 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:43:27.180492 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.147128 (* 1 = 0.147128 loss)
I1106 20:43:27.180498 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.152079 (* 1 = 0.152079 loss)
I1106 20:43:27.180503 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0113044 (* 1 = 0.0113044 loss)
I1106 20:43:27.180508 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0133491 (* 1 = 0.0133491 loss)
I1106 20:43:27.180516 31365 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1106 20:44:19.320086 31365 solver.cpp:229] Iteration 9020, loss = 0.200527
I1106 20:44:19.320153 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:44:19.320178 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.130094 (* 1 = 0.130094 loss)
I1106 20:44:19.320186 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0947975 (* 1 = 0.0947975 loss)
I1106 20:44:19.320192 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0016608 (* 1 = 0.0016608 loss)
I1106 20:44:19.320197 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00249098 (* 1 = 0.00249098 loss)
I1106 20:44:19.320204 31365 sgd_solver.cpp:106] Iteration 9020, lr = 0.001
I1106 20:45:11.452376 31365 solver.cpp:229] Iteration 9040, loss = 0.237538
I1106 20:45:11.452445 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:45:11.452457 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.12932 (* 1 = 0.12932 loss)
I1106 20:45:11.452462 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0512052 (* 1 = 0.0512052 loss)
I1106 20:45:11.452468 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00467162 (* 1 = 0.00467162 loss)
I1106 20:45:11.452472 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0347108 (* 1 = 0.0347108 loss)
I1106 20:45:11.452479 31365 sgd_solver.cpp:106] Iteration 9040, lr = 0.001
I1106 20:46:03.563752 31365 solver.cpp:229] Iteration 9060, loss = 0.373099
I1106 20:46:03.563820 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:46:03.563833 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.118852 (* 1 = 0.118852 loss)
I1106 20:46:03.563843 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0919252 (* 1 = 0.0919252 loss)
I1106 20:46:03.563848 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00520532 (* 1 = 0.00520532 loss)
I1106 20:46:03.563853 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0161159 (* 1 = 0.0161159 loss)
I1106 20:46:03.563859 31365 sgd_solver.cpp:106] Iteration 9060, lr = 0.001
I1106 20:46:55.692103 31365 solver.cpp:229] Iteration 9080, loss = 0.217399
I1106 20:46:55.692173 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 20:46:55.692184 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.184268 (* 1 = 0.184268 loss)
I1106 20:46:55.692190 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0347584 (* 1 = 0.0347584 loss)
I1106 20:46:55.692195 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.000675456 (* 1 = 0.000675456 loss)
I1106 20:46:55.692200 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00428546 (* 1 = 0.00428546 loss)
I1106 20:46:55.692207 31365 sgd_solver.cpp:106] Iteration 9080, lr = 0.001
I1106 20:47:47.875520 31365 solver.cpp:229] Iteration 9100, loss = 0.23803
I1106 20:47:47.875592 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:47:47.875604 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157036 (* 1 = 0.157036 loss)
I1106 20:47:47.875614 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0797677 (* 1 = 0.0797677 loss)
I1106 20:47:47.875619 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00306704 (* 1 = 0.00306704 loss)
I1106 20:47:47.875624 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00561077 (* 1 = 0.00561077 loss)
I1106 20:47:47.875630 31365 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1106 20:48:40.020853 31365 solver.cpp:229] Iteration 9120, loss = 0.376338
I1106 20:48:40.020920 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:48:40.020932 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.315937 (* 1 = 0.315937 loss)
I1106 20:48:40.020941 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0694269 (* 1 = 0.0694269 loss)
I1106 20:48:40.020946 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00812345 (* 1 = 0.00812345 loss)
I1106 20:48:40.020951 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0112648 (* 1 = 0.0112648 loss)
I1106 20:48:40.020958 31365 sgd_solver.cpp:106] Iteration 9120, lr = 0.001
I1106 20:49:32.155275 31365 solver.cpp:229] Iteration 9140, loss = 0.240377
I1106 20:49:32.155344 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:49:32.155354 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.125781 (* 1 = 0.125781 loss)
I1106 20:49:32.155364 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0844579 (* 1 = 0.0844579 loss)
I1106 20:49:32.155369 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00436947 (* 1 = 0.00436947 loss)
I1106 20:49:32.155374 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0164743 (* 1 = 0.0164743 loss)
I1106 20:49:32.155380 31365 sgd_solver.cpp:106] Iteration 9140, lr = 0.001
I1106 20:50:24.256245 31365 solver.cpp:229] Iteration 9160, loss = 0.387985
I1106 20:50:24.256314 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:50:24.256326 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.145178 (* 1 = 0.145178 loss)
I1106 20:50:24.256335 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.120905 (* 1 = 0.120905 loss)
I1106 20:50:24.256340 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0564572 (* 1 = 0.0564572 loss)
I1106 20:50:24.256345 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0114504 (* 1 = 0.0114504 loss)
I1106 20:50:24.256351 31365 sgd_solver.cpp:106] Iteration 9160, lr = 0.001
I1106 20:51:16.341022 31365 solver.cpp:229] Iteration 9180, loss = 0.366403
I1106 20:51:16.341090 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:51:16.341102 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.173845 (* 1 = 0.173845 loss)
I1106 20:51:16.341111 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.111449 (* 1 = 0.111449 loss)
I1106 20:51:16.341116 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.004519 (* 1 = 0.004519 loss)
I1106 20:51:16.341121 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0105425 (* 1 = 0.0105425 loss)
I1106 20:51:16.341127 31365 sgd_solver.cpp:106] Iteration 9180, lr = 0.001
I1106 20:52:08.463461 31365 solver.cpp:229] Iteration 9200, loss = 0.356017
I1106 20:52:08.463529 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:52:08.463542 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.122463 (* 1 = 0.122463 loss)
I1106 20:52:08.463549 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.11154 (* 1 = 0.11154 loss)
I1106 20:52:08.463555 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00333316 (* 1 = 0.00333316 loss)
I1106 20:52:08.463560 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.028478 (* 1 = 0.028478 loss)
I1106 20:52:08.463567 31365 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1106 20:53:00.625576 31365 solver.cpp:229] Iteration 9220, loss = 0.280852
I1106 20:53:00.625654 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:53:00.625669 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.164886 (* 1 = 0.164886 loss)
I1106 20:53:00.625674 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.12883 (* 1 = 0.12883 loss)
I1106 20:53:00.625679 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00195702 (* 1 = 0.00195702 loss)
I1106 20:53:00.625684 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0215533 (* 1 = 0.0215533 loss)
I1106 20:53:00.625691 31365 sgd_solver.cpp:106] Iteration 9220, lr = 0.001
I1106 20:53:52.733232 31365 solver.cpp:229] Iteration 9240, loss = 0.346497
I1106 20:53:52.733310 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 20:53:52.733325 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.173949 (* 1 = 0.173949 loss)
I1106 20:53:52.733330 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0617384 (* 1 = 0.0617384 loss)
I1106 20:53:52.733335 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00583144 (* 1 = 0.00583144 loss)
I1106 20:53:52.733340 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0189929 (* 1 = 0.0189929 loss)
I1106 20:53:52.733347 31365 sgd_solver.cpp:106] Iteration 9240, lr = 0.001
I1106 20:54:44.854302 31365 solver.cpp:229] Iteration 9260, loss = 0.28608
I1106 20:54:44.854387 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:54:44.854403 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.120639 (* 1 = 0.120639 loss)
I1106 20:54:44.854408 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.170403 (* 1 = 0.170403 loss)
I1106 20:54:44.854413 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00837718 (* 1 = 0.00837718 loss)
I1106 20:54:44.854418 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0126069 (* 1 = 0.0126069 loss)
I1106 20:54:44.854439 31365 sgd_solver.cpp:106] Iteration 9260, lr = 0.001
I1106 20:55:37.024443 31365 solver.cpp:229] Iteration 9280, loss = 0.309318
I1106 20:55:37.024518 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:55:37.024533 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.152255 (* 1 = 0.152255 loss)
I1106 20:55:37.024538 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0761067 (* 1 = 0.0761067 loss)
I1106 20:55:37.024544 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0253035 (* 1 = 0.0253035 loss)
I1106 20:55:37.024549 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0179904 (* 1 = 0.0179904 loss)
I1106 20:55:37.024555 31365 sgd_solver.cpp:106] Iteration 9280, lr = 0.001
I1106 20:56:29.201457 31365 solver.cpp:229] Iteration 9300, loss = 0.28391
I1106 20:56:29.201531 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 20:56:29.201544 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.140797 (* 1 = 0.140797 loss)
I1106 20:56:29.201551 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0669124 (* 1 = 0.0669124 loss)
I1106 20:56:29.201556 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00290911 (* 1 = 0.00290911 loss)
I1106 20:56:29.201561 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0112229 (* 1 = 0.0112229 loss)
I1106 20:56:29.201568 31365 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1106 20:57:21.420967 31365 solver.cpp:229] Iteration 9320, loss = 0.352902
I1106 20:57:21.421036 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 20:57:21.421047 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.237307 (* 1 = 0.237307 loss)
I1106 20:57:21.421056 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118659 (* 1 = 0.118659 loss)
I1106 20:57:21.421061 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00430688 (* 1 = 0.00430688 loss)
I1106 20:57:21.421066 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0880591 (* 1 = 0.0880591 loss)
I1106 20:57:21.421073 31365 sgd_solver.cpp:106] Iteration 9320, lr = 0.001
I1106 20:58:13.546489 31365 solver.cpp:229] Iteration 9340, loss = 0.327032
I1106 20:58:13.546558 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:58:13.546571 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.163488 (* 1 = 0.163488 loss)
I1106 20:58:13.546576 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.119806 (* 1 = 0.119806 loss)
I1106 20:58:13.546581 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00540361 (* 1 = 0.00540361 loss)
I1106 20:58:13.546586 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0193505 (* 1 = 0.0193505 loss)
I1106 20:58:13.546593 31365 sgd_solver.cpp:106] Iteration 9340, lr = 0.001
I1106 20:59:05.636422 31365 solver.cpp:229] Iteration 9360, loss = 0.528559
I1106 20:59:05.636492 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 20:59:05.636503 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.184732 (* 1 = 0.184732 loss)
I1106 20:59:05.636512 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.117126 (* 1 = 0.117126 loss)
I1106 20:59:05.636518 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0530855 (* 1 = 0.0530855 loss)
I1106 20:59:05.636523 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0216163 (* 1 = 0.0216163 loss)
I1106 20:59:05.636528 31365 sgd_solver.cpp:106] Iteration 9360, lr = 0.001
I1106 20:59:57.715158 31365 solver.cpp:229] Iteration 9380, loss = 0.247637
I1106 20:59:57.715227 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 20:59:57.715240 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.187487 (* 1 = 0.187487 loss)
I1106 20:59:57.715248 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0429176 (* 1 = 0.0429176 loss)
I1106 20:59:57.715253 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00362823 (* 1 = 0.00362823 loss)
I1106 20:59:57.715258 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00434327 (* 1 = 0.00434327 loss)
I1106 20:59:57.715265 31365 sgd_solver.cpp:106] Iteration 9380, lr = 0.001
I1106 21:00:49.818564 31365 solver.cpp:229] Iteration 9400, loss = 0.472563
I1106 21:00:49.818632 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 21:00:49.818645 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.190111 (* 1 = 0.190111 loss)
I1106 21:00:49.818652 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0731492 (* 1 = 0.0731492 loss)
I1106 21:00:49.818657 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0239405 (* 1 = 0.0239405 loss)
I1106 21:00:49.818663 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0585717 (* 1 = 0.0585717 loss)
I1106 21:00:49.818670 31365 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1106 21:01:41.965739 31365 solver.cpp:229] Iteration 9420, loss = 0.316254
I1106 21:01:41.965804 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 21:01:41.965816 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.153834 (* 1 = 0.153834 loss)
I1106 21:01:41.965822 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0151106 (* 1 = 0.0151106 loss)
I1106 21:01:41.965827 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00508214 (* 1 = 0.00508214 loss)
I1106 21:01:41.965832 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0331798 (* 1 = 0.0331798 loss)
I1106 21:01:41.965839 31365 sgd_solver.cpp:106] Iteration 9420, lr = 0.001
I1106 21:02:34.059886 31365 solver.cpp:229] Iteration 9440, loss = 0.332093
I1106 21:02:34.059955 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:02:34.059967 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.199668 (* 1 = 0.199668 loss)
I1106 21:02:34.059975 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.125812 (* 1 = 0.125812 loss)
I1106 21:02:34.059980 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0182311 (* 1 = 0.0182311 loss)
I1106 21:02:34.059985 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0230597 (* 1 = 0.0230597 loss)
I1106 21:02:34.059993 31365 sgd_solver.cpp:106] Iteration 9440, lr = 0.001
I1106 21:03:26.158829 31365 solver.cpp:229] Iteration 9460, loss = 0.296302
I1106 21:03:26.158928 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 21:03:26.158941 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.123213 (* 1 = 0.123213 loss)
I1106 21:03:26.158947 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.131133 (* 1 = 0.131133 loss)
I1106 21:03:26.158952 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00435552 (* 1 = 0.00435552 loss)
I1106 21:03:26.158958 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00819629 (* 1 = 0.00819629 loss)
I1106 21:03:26.158964 31365 sgd_solver.cpp:106] Iteration 9460, lr = 0.001
I1106 21:04:18.285087 31365 solver.cpp:229] Iteration 9480, loss = 0.324608
I1106 21:04:18.285158 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:04:18.285171 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.105263 (* 1 = 0.105263 loss)
I1106 21:04:18.285177 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.161901 (* 1 = 0.161901 loss)
I1106 21:04:18.285182 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0180033 (* 1 = 0.0180033 loss)
I1106 21:04:18.285187 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0124099 (* 1 = 0.0124099 loss)
I1106 21:04:18.285194 31365 sgd_solver.cpp:106] Iteration 9480, lr = 0.001
I1106 21:05:10.379202 31365 solver.cpp:229] Iteration 9500, loss = 0.230051
I1106 21:05:10.379271 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 21:05:10.379283 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.224275 (* 1 = 0.224275 loss)
I1106 21:05:10.379292 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0227947 (* 1 = 0.0227947 loss)
I1106 21:05:10.379297 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00269729 (* 1 = 0.00269729 loss)
I1106 21:05:10.379302 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0322711 (* 1 = 0.0322711 loss)
I1106 21:05:10.379308 31365 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1106 21:06:02.503053 31365 solver.cpp:229] Iteration 9520, loss = 0.367591
I1106 21:06:02.503120 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:06:02.503134 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.19125 (* 1 = 0.19125 loss)
I1106 21:06:02.503142 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0975223 (* 1 = 0.0975223 loss)
I1106 21:06:02.503147 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0249058 (* 1 = 0.0249058 loss)
I1106 21:06:02.503152 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0637897 (* 1 = 0.0637897 loss)
I1106 21:06:02.503159 31365 sgd_solver.cpp:106] Iteration 9520, lr = 0.001
I1106 21:06:54.593204 31365 solver.cpp:229] Iteration 9540, loss = 0.267346
I1106 21:06:54.593273 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:06:54.593286 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.221703 (* 1 = 0.221703 loss)
I1106 21:06:54.593291 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.107371 (* 1 = 0.107371 loss)
I1106 21:06:54.593297 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0211021 (* 1 = 0.0211021 loss)
I1106 21:06:54.593302 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0187429 (* 1 = 0.0187429 loss)
I1106 21:06:54.593308 31365 sgd_solver.cpp:106] Iteration 9540, lr = 0.001
I1106 21:07:46.692782 31365 solver.cpp:229] Iteration 9560, loss = 0.366078
I1106 21:07:46.692852 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 21:07:46.692878 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.178847 (* 1 = 0.178847 loss)
I1106 21:07:46.692885 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.233301 (* 1 = 0.233301 loss)
I1106 21:07:46.692890 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0215145 (* 1 = 0.0215145 loss)
I1106 21:07:46.692895 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0234869 (* 1 = 0.0234869 loss)
I1106 21:07:46.692903 31365 sgd_solver.cpp:106] Iteration 9560, lr = 0.001
I1106 21:08:38.815650 31365 solver.cpp:229] Iteration 9580, loss = 0.303393
I1106 21:08:38.815719 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 21:08:38.815732 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.156993 (* 1 = 0.156993 loss)
I1106 21:08:38.815740 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0285389 (* 1 = 0.0285389 loss)
I1106 21:08:38.815745 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00150814 (* 1 = 0.00150814 loss)
I1106 21:08:38.815750 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00432266 (* 1 = 0.00432266 loss)
I1106 21:08:38.815757 31365 sgd_solver.cpp:106] Iteration 9580, lr = 0.001
I1106 21:09:30.893178 31365 solver.cpp:229] Iteration 9600, loss = 0.390391
I1106 21:09:30.893261 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:09:30.893275 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.198607 (* 1 = 0.198607 loss)
I1106 21:09:30.893283 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0958015 (* 1 = 0.0958015 loss)
I1106 21:09:30.893290 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0375089 (* 1 = 0.0375089 loss)
I1106 21:09:30.893295 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0999624 (* 1 = 0.0999624 loss)
I1106 21:09:30.893301 31365 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1106 21:10:22.997738 31365 solver.cpp:229] Iteration 9620, loss = 0.275794
I1106 21:10:22.997807 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:10:22.997819 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.167035 (* 1 = 0.167035 loss)
I1106 21:10:22.997826 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.138946 (* 1 = 0.138946 loss)
I1106 21:10:22.997831 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0262162 (* 1 = 0.0262162 loss)
I1106 21:10:22.997836 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0301803 (* 1 = 0.0301803 loss)
I1106 21:10:22.997843 31365 sgd_solver.cpp:106] Iteration 9620, lr = 0.001
I1106 21:11:15.081523 31365 solver.cpp:229] Iteration 9640, loss = 0.2611
I1106 21:11:15.081593 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 21:11:15.081605 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.110872 (* 1 = 0.110872 loss)
I1106 21:11:15.081612 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0505733 (* 1 = 0.0505733 loss)
I1106 21:11:15.081619 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00194565 (* 1 = 0.00194565 loss)
I1106 21:11:15.081624 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.000595758 (* 1 = 0.000595758 loss)
I1106 21:11:15.081629 31365 sgd_solver.cpp:106] Iteration 9640, lr = 0.001
I1106 21:12:07.172693 31365 solver.cpp:229] Iteration 9660, loss = 0.45716
I1106 21:12:07.172760 31365 solver.cpp:245]     Train net output #0: accuracy = 0.90625
I1106 21:12:07.172772 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.182862 (* 1 = 0.182862 loss)
I1106 21:12:07.172778 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.252882 (* 1 = 0.252882 loss)
I1106 21:12:07.172783 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00764854 (* 1 = 0.00764854 loss)
I1106 21:12:07.172788 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0248023 (* 1 = 0.0248023 loss)
I1106 21:12:07.172796 31365 sgd_solver.cpp:106] Iteration 9660, lr = 0.001
I1106 21:12:59.248746 31365 solver.cpp:229] Iteration 9680, loss = 0.325696
I1106 21:12:59.248816 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:12:59.248827 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.17003 (* 1 = 0.17003 loss)
I1106 21:12:59.248836 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0997655 (* 1 = 0.0997655 loss)
I1106 21:12:59.248842 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.013861 (* 1 = 0.013861 loss)
I1106 21:12:59.248847 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0406177 (* 1 = 0.0406177 loss)
I1106 21:12:59.248853 31365 sgd_solver.cpp:106] Iteration 9680, lr = 0.001
I1106 21:13:51.468811 31365 solver.cpp:229] Iteration 9700, loss = 0.35188
I1106 21:13:51.468883 31365 solver.cpp:245]     Train net output #0: accuracy = 0.9375
I1106 21:13:51.468895 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.143986 (* 1 = 0.143986 loss)
I1106 21:13:51.468902 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.143628 (* 1 = 0.143628 loss)
I1106 21:13:51.468909 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00576331 (* 1 = 0.00576331 loss)
I1106 21:13:51.468914 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0136346 (* 1 = 0.0136346 loss)
I1106 21:13:51.468919 31365 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1106 21:14:43.658318 31365 solver.cpp:229] Iteration 9720, loss = 0.468407
I1106 21:14:43.658398 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:14:43.658413 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.247597 (* 1 = 0.247597 loss)
I1106 21:14:43.658419 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127436 (* 1 = 0.127436 loss)
I1106 21:14:43.658424 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0948206 (* 1 = 0.0948206 loss)
I1106 21:14:43.658429 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0618078 (* 1 = 0.0618078 loss)
I1106 21:14:43.658449 31365 sgd_solver.cpp:106] Iteration 9720, lr = 0.001
I1106 21:15:35.861603 31365 solver.cpp:229] Iteration 9740, loss = 0.363872
I1106 21:15:35.861675 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 21:15:35.861687 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.26492 (* 1 = 0.26492 loss)
I1106 21:15:35.861696 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.121415 (* 1 = 0.121415 loss)
I1106 21:15:35.861701 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.018768 (* 1 = 0.018768 loss)
I1106 21:15:35.861706 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0160186 (* 1 = 0.0160186 loss)
I1106 21:15:35.861712 31365 sgd_solver.cpp:106] Iteration 9740, lr = 0.001
I1106 21:16:28.055583 31365 solver.cpp:229] Iteration 9760, loss = 0.318811
I1106 21:16:28.055655 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:16:28.055668 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.101834 (* 1 = 0.101834 loss)
I1106 21:16:28.055675 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0849672 (* 1 = 0.0849672 loss)
I1106 21:16:28.055680 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00826213 (* 1 = 0.00826213 loss)
I1106 21:16:28.055685 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0246967 (* 1 = 0.0246967 loss)
I1106 21:16:28.055691 31365 sgd_solver.cpp:106] Iteration 9760, lr = 0.001
I1106 21:17:20.231503 31365 solver.cpp:229] Iteration 9780, loss = 0.373207
I1106 21:17:20.231577 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:17:20.231606 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.260744 (* 1 = 0.260744 loss)
I1106 21:17:20.231611 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.118485 (* 1 = 0.118485 loss)
I1106 21:17:20.231616 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0282296 (* 1 = 0.0282296 loss)
I1106 21:17:20.231621 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0338589 (* 1 = 0.0338589 loss)
I1106 21:17:20.231629 31365 sgd_solver.cpp:106] Iteration 9780, lr = 0.001
I1106 21:18:12.415925 31365 solver.cpp:229] Iteration 9800, loss = 0.290482
I1106 21:18:12.415999 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 21:18:12.416028 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.141294 (* 1 = 0.141294 loss)
I1106 21:18:12.416033 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.133017 (* 1 = 0.133017 loss)
I1106 21:18:12.416039 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00343598 (* 1 = 0.00343598 loss)
I1106 21:18:12.416044 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00591771 (* 1 = 0.00591771 loss)
I1106 21:18:12.416051 31365 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1106 21:19:04.591612 31365 solver.cpp:229] Iteration 9820, loss = 0.259218
I1106 21:19:04.591688 31365 solver.cpp:245]     Train net output #0: accuracy = 1
I1106 21:19:04.591701 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.236328 (* 1 = 0.236328 loss)
I1106 21:19:04.591707 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0173698 (* 1 = 0.0173698 loss)
I1106 21:19:04.591713 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00278365 (* 1 = 0.00278365 loss)
I1106 21:19:04.591718 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0589882 (* 1 = 0.0589882 loss)
I1106 21:19:04.591725 31365 sgd_solver.cpp:106] Iteration 9820, lr = 0.001
I1106 21:19:56.771533 31365 solver.cpp:229] Iteration 9840, loss = 0.433391
I1106 21:19:56.771606 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:19:56.771620 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.243138 (* 1 = 0.243138 loss)
I1106 21:19:56.771627 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0968779 (* 1 = 0.0968779 loss)
I1106 21:19:56.771633 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00836807 (* 1 = 0.00836807 loss)
I1106 21:19:56.771638 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0467695 (* 1 = 0.0467695 loss)
I1106 21:19:56.771644 31365 sgd_solver.cpp:106] Iteration 9840, lr = 0.001
I1106 21:20:48.892005 31365 solver.cpp:229] Iteration 9860, loss = 0.418736
I1106 21:20:48.892084 31365 solver.cpp:245]     Train net output #0: accuracy = 0.921875
I1106 21:20:48.892112 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.325411 (* 1 = 0.325411 loss)
I1106 21:20:48.892118 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.159015 (* 1 = 0.159015 loss)
I1106 21:20:48.892123 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00955116 (* 1 = 0.00955116 loss)
I1106 21:20:48.892129 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0725405 (* 1 = 0.0725405 loss)
I1106 21:20:48.892135 31365 sgd_solver.cpp:106] Iteration 9860, lr = 0.001
I1106 21:21:40.977401 31365 solver.cpp:229] Iteration 9880, loss = 0.264636
I1106 21:21:40.977476 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:21:40.977490 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.116807 (* 1 = 0.116807 loss)
I1106 21:21:40.977496 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0599205 (* 1 = 0.0599205 loss)
I1106 21:21:40.977502 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00254223 (* 1 = 0.00254223 loss)
I1106 21:21:40.977507 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.00476795 (* 1 = 0.00476795 loss)
I1106 21:21:40.977514 31365 sgd_solver.cpp:106] Iteration 9880, lr = 0.001
I1106 21:22:33.118547 31365 solver.cpp:229] Iteration 9900, loss = 0.280492
I1106 21:22:33.118623 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 21:22:33.118634 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.144698 (* 1 = 0.144698 loss)
I1106 21:22:33.118640 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0729065 (* 1 = 0.0729065 loss)
I1106 21:22:33.118646 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00892199 (* 1 = 0.00892199 loss)
I1106 21:22:33.118651 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0213188 (* 1 = 0.0213188 loss)
I1106 21:22:33.118659 31365 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1106 21:23:25.237862 31365 solver.cpp:229] Iteration 9920, loss = 0.310276
I1106 21:23:25.237931 31365 solver.cpp:245]     Train net output #0: accuracy = 0.953125
I1106 21:23:25.237942 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.157679 (* 1 = 0.157679 loss)
I1106 21:23:25.237949 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.109712 (* 1 = 0.109712 loss)
I1106 21:23:25.237954 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00437877 (* 1 = 0.00437877 loss)
I1106 21:23:25.237959 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.017375 (* 1 = 0.017375 loss)
I1106 21:23:25.237967 31365 sgd_solver.cpp:106] Iteration 9920, lr = 0.001
I1106 21:24:17.333488 31365 solver.cpp:229] Iteration 9940, loss = 0.332992
I1106 21:24:17.333570 31365 solver.cpp:245]     Train net output #0: accuracy = 0.984375
I1106 21:24:17.333582 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.155593 (* 1 = 0.155593 loss)
I1106 21:24:17.333590 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0624587 (* 1 = 0.0624587 loss)
I1106 21:24:17.333595 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0147921 (* 1 = 0.0147921 loss)
I1106 21:24:17.333601 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.016586 (* 1 = 0.016586 loss)
I1106 21:24:17.333607 31365 sgd_solver.cpp:106] Iteration 9940, lr = 0.001
I1106 21:25:09.436676 31365 solver.cpp:229] Iteration 9960, loss = 0.418196
I1106 21:25:09.436745 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:25:09.436758 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.179792 (* 1 = 0.179792 loss)
I1106 21:25:09.436764 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.16489 (* 1 = 0.16489 loss)
I1106 21:25:09.436769 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00323094 (* 1 = 0.00323094 loss)
I1106 21:25:09.436774 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.058226 (* 1 = 0.058226 loss)
I1106 21:25:09.436781 31365 sgd_solver.cpp:106] Iteration 9960, lr = 0.001
I1106 21:26:01.610137 31365 solver.cpp:229] Iteration 9980, loss = 0.312981
I1106 21:26:01.610219 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:26:01.610231 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.15363 (* 1 = 0.15363 loss)
I1106 21:26:01.610237 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.127235 (* 1 = 0.127235 loss)
I1106 21:26:01.610244 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.00731887 (* 1 = 0.00731887 loss)
I1106 21:26:01.610249 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0158891 (* 1 = 0.0158891 loss)
I1106 21:26:01.610255 31365 sgd_solver.cpp:106] Iteration 9980, lr = 0.001
I1106 21:26:57.119298 31365 solver.cpp:229] Iteration 10000, loss = 0.254638
I1106 21:26:57.119370 31365 solver.cpp:245]     Train net output #0: accuracy = 0.96875
I1106 21:26:57.119397 31365 solver.cpp:245]     Train net output #1: loss_bbox = 0.158766 (* 1 = 0.158766 loss)
I1106 21:26:57.119405 31365 solver.cpp:245]     Train net output #2: loss_cls = 0.0622396 (* 1 = 0.0622396 loss)
I1106 21:26:57.119410 31365 solver.cpp:245]     Train net output #3: rpn_cls_loss = 0.0235715 (* 1 = 0.0235715 loss)
I1106 21:26:57.119415 31365 solver.cpp:245]     Train net output #4: rpn_loss_bbox = 0.0599693 (* 1 = 0.0599693 loss)
I1106 21:26:57.119421 31365 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
.  1.  1.  1.  1.  1.  1.
  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
[ 1.  1.  1.  1.  1.  1.  1.  1.]
Filtered 0 roidb entries: 4134 -> 4134
Computing bounding-box regression targets...
bbox target means:
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]
[ 0.  0.  0.  0.]
bbox target stdevs:
[[ 0.1  0.1  0.2  0.2]
 [ 0.1  0.1  0.2  0.2]]
[ 0.1  0.1  0.2  0.2]
Normalizing targets
done
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
Loading pretrained model weights from ./data/imagenet_models/VGG16.v2.caffemodel
Solving...
speed: 2.403s / iter
speed: 2.503s / iter
speed: 2.537s / iter
speed: 2.555s / iter
speed: 2.565s / iter
speed: 2.572s / iter
speed: 2.577s / iter
speed: 2.581s / iter
speed: 2.584s / iter
speed: 2.586s / iter
speed: 2.588s / iter
speed: 2.589s / iter
speed: 2.591s / iter
speed: 2.592s / iter
speed: 2.593s / iter
speed: 2.594s / iter
speed: 2.594s / iter
speed: 2.595s / iter
speed: 2.596s / iter
speed: 2.596s / iter
speed: 2.597s / iter
speed: 2.597s / iter
speed: 2.598s / iter
speed: 2.598s / iter
speed: 2.599s / iter
Wrote snapshot to: /media/ray/maps_project/faster-rcnn/output/faster_rcnn_end2end/train/vgg16_faster_rcnn_map_iter_5000.caffemodel
speed: 2.599s / iter
speed: 2.599s / iter
speed: 2.600s / iter
speed: 2.600s / iter
speed: 2.600s / iter
speed: 2.600s / iter
speed: 2.600s / iter
speed: 2.601s / iter
speed: 2.601s / iter
speed: 2.601s / iter
speed: 2.601s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.602s / iter
speed: 2.603s / iter
speed: 2.603s / iter
speed: 2.603s / iter
speed: 2.603s / iter
speed: 2.603s / iter
speed: 2.603s / iter
Wrote snapshot to: /media/ray/maps_project/faster-rcnn/output/faster_rcnn_end2end/train/vgg16_faster_rcnn_map_iter_10000.caffemodel
Wrote snapshot to: /media/ray/maps_project/faster-rcnn/output/faster_rcnn_end2end/train/vgg16_faster_rcnn_map_iter_10010.caffemodel
done solving
